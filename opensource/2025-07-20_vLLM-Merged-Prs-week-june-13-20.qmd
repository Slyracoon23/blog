---
aliases:
- /vllm-merged-prs-week-june-13-20/
categories:
- Open Source
- Python
- FastAPI
- Performance
date: '2025-07-20'
image: /images/opensource/vllm_merged_prs_week_june_13_20.png
title: "vLLM Merged PRs week (June 13-20)"
subtitle: "A look at the PRs merged to vLLM this week"
format: html
---
# vLLM Merged PRs - Week (June 13-20, 2025)

**Total PRs: 114**

## Summary by Category

- **Bug Fixes**: 28 PRs (24.6%)
- **Miscellaneous & Cleanup**: 21 PRs (18.4%)
- **Performance & Optimization**: 16 PRs (14.0%)
- **Kernel & Hardware**: 12 PRs (10.5%)
- **V1 Engine**: 8 PRs (7.0%)
- **Model Support**: 8 PRs (7.0%)
- **CI/Build**: 7 PRs (6.1%)
- **Frontend & API**: 5 PRs (4.4%)
- **Documentation**: 5 PRs (4.4%)
- **TPU**: 3 PRs (2.6%)
- **Other categories**: 1 PR each

## Key Highlights

1. **Major Focus on Bug Fixes**: Nearly 25% of PRs were bug fixes, showing active maintenance
2. **Performance Improvements**: Significant work on FP4 MOE kernels and CUTLASS optimizations
3. **V1 Engine Development**: Continued development of the new V1 engine architecture
4. **Speculative Decoding Fixes**: Multiple fixes for speculative decoding test stability
5. **Hardware Support**: Updates for TPU, AMD ROCm, and NVIDIA hardware
6. **Code Quality**: Many cleanup and maintenance PRs to improve code quality

--- 

::: {.panel-tabset}

## üêõ Bug Fixes (28 PRs)

::: {.callout-warning collapse="true" title="üêõ Bug Fix - PR #19624: Fix DP Coordinator incorrect debug log message"}
## [#19624](https://github.com/vllm-project/vllm/pull/19624) - Fix DP Coordinator incorrect debug log message

#### Overview
This PR fixes a logging bug in the Data Parallel (DP) Coordinator where debug messages were showing incorrect wave numbers. The coordinator was logging `"Moving DP wave from N to N"` instead of the correct `"Moving DP wave from N to (N + 1)"`. This affects the V1 engine's data parallel processing wave management system.

**Motivation**: Accurate debug logging is crucial for troubleshooting distributed processing coordination issues in vLLM's V1 engine architecture.

#### Code Changes (Verified)

**File**: `vllm/v1/engine/coordinator.py`

```diff
@@ -183,11 +183,12 @@ def process_input_socket(self, front_publish_address: str,
                     # engines are paused, so that we can wake the other
                     # engines.
                     engine_to_exclude, wave = msgspec.msgpack.decode(buffer)
-                    if wave < self.current_wave:
-                        # If the wave number is stale, ensure the message is
-                        # handled by all the engines.
-                        engine_to_exclude = None
                     if not self.engines_running:
+                        if wave < self.current_wave:
+                            # If the wave number is stale, ensure the message
+                            # is handled by all the engines.
+                            engine_to_exclude = None
+
                         self.engines_running = True
                         self.stats_changed = True
                         self._send_start_wave(publish_back, self.current_wave,
@@ -217,8 +218,10 @@ def process_input_socket(self, front_publish_address: str,
                         # (engines_running==False).
                         if self.current_wave <= wave:
+                            new_wave = wave + 1
                             logger.debug("Moving DP wave from %d to %d.",
-                                         self.current_wave, wave)
-                            self.current_wave = wave + 1
+                                         self.current_wave, new_wave)
+                            self.current_wave = new_wave
                             self.engines_running = False
                             self.stats_changed = True
```

**Explanation**: 

- **Main Fix**: Introduced a `new_wave` variable to compute `wave + 1` before logging, ensuring the debug message shows the correct transition from current wave to next wave

- **Code Refactoring**: Moved the stale wave check inside the `if not self.engines_running` block for better logical grouping (noted as "unrelated simplification" by author)

- **Variable Extraction**: Extracted `scheduler_stats` to a local variable to avoid repeated attribute access, improving code readability

#### PR Discussion & Comments

> **Development Team Discussion:**
> 
> **@njhill** commented on code organization improvement:  
> *"This is unrelated simplification, this check only applies to the case inside the if."*
> 
> **@njhill** identified the main bug:  
> *"This is the actual bug fix"*

#### Key Takeaways
- **Bug Impact**: Incorrect debug logging can mislead developers during troubleshooting of distributed processing coordination
- **Simple but Important**: The fix demonstrates how even small logging errors should be corrected for proper debugging experience
- **Code Quality**: The PR also includes minor refactoring improvements alongside the main fix
- **V1 Engine Context**: This affects the newer V1 engine architecture's data parallel processing system

#### Further Reading
- [vLLM V1 Engine Documentation](https://docs.vllm.ai)
- [Original PR #19624](https://github.com/vllm-project/vllm/pull/19624)

:::


::: {.callout-warning collapse="true" title="üêõ Bug Fix - PR #19316: Fix auto dtype casting for BatchFeature"}
## [#19316](https://github.com/vllm-project/vllm/pull/19316) - Fix auto dtype casting for BatchFeature

#### Overview
This PR fixes a critical dtype casting issue for `BatchFeature` objects in vision-language models, particularly affecting DeepSeek VL2. The bug occurred because `BatchFeature` is a `UserDict`, and `isinstance(value, dict)` returns `False`, causing data type casting to fail in the `json_map_leaves` function.

**Motivation**: Proper dtype casting is essential for vision-language models to ensure tensor operations are performed with consistent data types, preventing runtime errors like `"Input type (float) and bias type (c10::BFloat16) should be the same"`.

#### Code Changes (Verified)

**File**: `vllm/inputs/registry.py`

```diff
@@ -168,10 +168,12 @@ def maybe_cast_dtype(x):
         try:
             output = hf_processor(**data, **merged_kwargs, return_tensors="pt")
             # this emulates output.to(dtype=self.model_config.dtype)
-            cast_output = json_map_leaves(maybe_cast_dtype, output)
             if isinstance(output, BatchFeature):
+                cast_output = json_map_leaves(maybe_cast_dtype, output.data)
                 return BatchFeature(cast_output)

+            cast_output = json_map_leaves(maybe_cast_dtype, output)
+
             logger.warning_once(
                 f"{type(hf_processor).__name__} did not return `BatchFeature`. "
                 "Make sure to match the behaviour of `ProcessorMixin` when "
```

**File**: `vllm/model_executor/models/qwen2_vl.py` (and `qwen2_5_vl.py`)

```diff
@@ -1208,9 +1208,9 @@ def _process_image_input(
         assert grid_thw.ndim == 2

         if image_input["type"] == "image_embeds":
-            image_embeds = image_input["image_embeds"].type(self.visual.dtype)
+            image_embeds = image_input["image_embeds"]
         else:
-            pixel_values = image_input["pixel_values"].type(self.visual.dtype)
+            pixel_values = image_input["pixel_values"]
             image_embeds = self.visual(pixel_values, grid_thw=grid_thw)
```

**File**: `vllm/utils.py`

```diff
@@ -190,6 +190,16 @@
     torch.int64: np.int64,
 }

+
+@contextlib.contextmanager
+def set_default_torch_num_threads(num_threads: int):
+    """Sets the default number of threads for PyTorch to the given value."""
+    old_num_threads = torch.get_num_threads()
+    torch.set_num_threads(num_threads)
+    yield
+    torch.set_num_threads(old_num_threads)
+
+
 P = ParamSpec('P')
 T = TypeVar("T")
 U = TypeVar("U")
```

**Explanation**: 

- **Main Fix**: Modified `json_map_leaves` to work with `BatchFeature.data` instead of the `BatchFeature` object directly, since `BatchFeature` inherits from `UserDict`

- **Model Updates**: Removed explicit `.type()` casting in Qwen2 VL models since the casting is now handled correctly at the input processing level

- **Test Infrastructure**: Added `set_default_torch_num_threads` utility to prevent OpenMP deadlocks during test execution

- **Circular Import Fix**: Moved utility functions to `vllm.utils` to avoid circular import issues

#### PR Discussion & Comments

> **Development Team Discussion:**
> 
> **@Isotr0py** explained the circular import resolution:  
> *"I move this to vllm.utils to avoid circular import: ImportError: cannot import name 'LoadConfig' from partially initialized module 'vllm.config'"*
> 
> **@Isotr0py** addressed deadlock prevention:  
> *"Seems that disable openmp by setting torch_num_threads=1 during engine forking can fix the deadlock issue locally"*

#### Key Takeaways
- **Vision-Language Models**: `BatchFeature` objects require special handling since they inherit from `UserDict`, not `dict`
- **Type Safety**: Proper dtype casting prevents runtime errors in mixed-precision scenarios common in modern LLM inference
- **Test Stability**: OpenMP thread management is crucial for preventing deadlocks in multi-process test environments
- **Code Organization**: Moving utility functions to appropriate modules prevents circular dependencies

#### Further Reading
- [vLLM Vision-Language Models Documentation](https://docs.vllm.ai)
- [Original Issue #19219](https://github.com/vllm-project/vllm/issues/19219)
- [Original PR #19316](https://github.com/vllm-project/vllm/pull/19316)

:::

::: {.callout-warning collapse="true" title="üêõ Bug Fix - PR #19561: Don't attempt to use triton if no driver is active"}
## [#19561](https://github.com/vllm-project/vllm/pull/19561) - Don't attempt to use triton if no driver is active

#### Overview
This PR fixes a critical issue where vLLM would attempt to use Triton kernels even when no active GPU driver is available, causing runtime crashes. The problem occurs when Triton is installed by external packages (like xgrammar) on non-GPU platforms, leading to conflicts during MLA (Multi-Layer Attention) backend imports.

**Motivation**: Prevent runtime crashes on platforms where Triton is installed but incompatible due to missing or inactive GPU drivers, ensuring graceful fallback behavior.

#### Code Changes (Verified)

**File**: `vllm/triton_utils/importing.py`

```diff
@@ -12,6 +12,36 @@
     find_spec("triton") is not None
     or find_spec("pytorch-triton-xpu") is not None  # Not compatible
 )
+if HAS_TRITON:
+    try:
+        from triton.backends import backends
+
+        # It's generally expected that x.driver exists and has
+        # an is_active method.
+        # The `x.driver and` check adds a small layer of safety.
+        active_drivers = [
+            x.driver for x in backends.values()
+            if x.driver and x.driver.is_active()
+        ]
+        if len(active_drivers) != 1:
+            logger.info(
+                "Triton is installed but %d active driver(s) found "
+                "(expected 1). Disabling Triton to prevent runtime errors.",
+                len(active_drivers))
+            HAS_TRITON = False
+    except ImportError:
+        # This can occur if Triton is partially installed or triton.backends
+        # is missing.
+        logger.warning(
+            "Triton is installed, but `triton.backends` could not be imported. "
+            "Disabling Triton.")
+        HAS_TRITON = False
+    except Exception as e:
+        # Catch any other unexpected errors during the check.
+        logger.warning(
+            "An unexpected error occurred while checking Triton active drivers:"
+            " %s. Disabling Triton.", e)
+        HAS_TRITON = False
 
 if not HAS_TRITON:
     logger.info("Triton not installed or not compatible; certain GPU-related"
```

**Explanation**: 

- **Driver Validation**: Added comprehensive check for active Triton backends by examining `triton.backends.backends.values()` and filtering for drivers where `x.driver.is_active()` returns `True`

- **Expected Driver Count**: Enforces that exactly one active driver should exist; zero drivers indicates no GPU support, while multiple drivers suggests an ambiguous configuration

- **Graceful Fallback**: When driver validation fails, `HAS_TRITON` is set to `False`, allowing vLLM to fall back to non-Triton alternatives

- **Robust Error Handling**: Three-tier exception handling covers ImportError (missing backends), generic exceptions (corrupted installations), and ensures vLLM never crashes during import due to Triton issues

- **Informative Logging**: Specific log messages help users understand why Triton was disabled, distinguishing between driver count issues, import failures, and unexpected errors

#### PR Discussion & Comments

> **Development Team Discussion:**
> 
> **@gemini-code-assist[bot] ‚Üí @kzawora-intel** ‚Äî Enhanced error handling and logging needed for robustness  
> *"The added check for active Triton drivers is a good approach to prevent runtime errors when Triton is installed but not usable."*
> 
> **@kzawora-intel** confirmed implementation of suggestions:  
> *"done"*

#### Key Takeaways
- **Cross-Platform Compatibility**: External packages can install Triton on incompatible platforms, requiring runtime validation rather than just package detection
- **Error Prevention**: Proactive driver validation prevents cryptic runtime crashes like `"RuntimeError: 0 active drivers ([]). There should only be one."`
- **Graceful Degradation**: vLLM should always have fallback paths when hardware-specific optimizations are unavailable
- **Import-Time Validation**: Critical infrastructure checks should occur at import time to fail fast and provide clear error messages

#### Further Reading
- [vLLM Triton Integration Documentation](https://docs.vllm.ai)
- [Triton GPU Programming Guide](https://triton-lang.org/)
- [Original PR #19561](https://github.com/vllm-project/vllm/pull/19561)

:::

::: {.callout-warning collapse="true" title="üêõ Bug Fix - PR #19644: Fix speculative decoding CI - Fix test_ngram_e2e_greedy_correctness"}
## [#19644](https://github.com/vllm-project/vllm/pull/19644) - Fix speculative decoding CI - Fix test_ngram_e2e_greedy_correctness

#### Overview
This PR fixes numerical instability issues in speculative decoding tests by explicitly setting test models to use `float32` precision instead of the default `float16`. The JackFram/llama models used in these tests were originally trained in fp32, and using fp16 caused numerical instability leading to test failures.

**Motivation**: Ensure test stability by using the correct precision for test models, preventing CI failures due to floating-point precision mismatches between baseline and speculative decoding results.

#### Code Changes (Verified)

**File**: `tests/spec_decode/e2e/test_integration.py`

```diff
@@ -14,10 +14,13 @@
 @pytest.mark.parametrize(
     "common_llm_kwargs",
     [{
+        "model_name": "JackFram/llama-68m",
 
         # Verify equality when cuda graphs allowed.
         "enforce_eager": False,
-        "model_name": "JackFram/llama-68m",
+
+        # The original model is float32, keep it for numerical stability.
+        "dtype": "float32",
     }])
```

**File**: `tests/spec_decode/e2e/test_logprobs.py`

```diff
@@ -17,7 +17,10 @@
         "model_name": "JackFram/llama-160m",
 
         # Skip cuda graph recording for fast test.
-        "enforce_eager": True
+        "enforce_eager": True,
+
+        # The original model is float32, keep it for numerical stability.
+        "dtype": "float32",
     }])
```

**File**: `tests/spec_decode/e2e/test_multistep_correctness.py`

```diff
@@ -57,6 +57,9 @@
 
         # Skip cuda graph recording for fast test.
         "enforce_eager": True,
+
+        # The original model is float32, keep it for numerical stability.
+        "dtype": "float32",
     }])
```

**File**: `tests/spec_decode/e2e/test_ngram_correctness.py`

```diff
@@ -40,6 +40,9 @@
 
         # Print spec metrics.
         "disable_log_stats": False,
+
+        # The original model is float32, keep it for numerical stability.
+        "dtype": "float32",
     }])
```

**File**: `tests/spec_decode/e2e/test_mlp_correctness.py`

```diff
@@ -494,6 +494,9 @@ def test_mlp_disable_queue(vllm_runner, common_llm_kwargs,
 
         # Skip cuda graph recording for fast test.
         "enforce_eager": True,
+
+        # Precision
+        "dtype": PRECISION,
     }])
```

**File**: `vllm/model_executor/models/eagle.py`

```diff
@@ -74,6 +74,7 @@ class EAGLE(nn.Module):
     def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
         super().__init__()
         config = vllm_config.model_config.hf_config
+        self.dtype = vllm_config.model_config.dtype
         self.config = config
```

```diff
@@ -250,7 +251,7 @@ def load_weights(self, weights: Iterable[tuple[str, torch.Tensor]]):
             lm_head_weight = torch.zeros(
                 self.lm_head.org_vocab_size,
                 self.lm_head.embedding_dim,
-                dtype=self.config.torch_dtype,
+                dtype=self.dtype,
             )
```

**Explanation**: 

- **Test Configuration Updates**: Added explicit `"dtype": "float32"` to all speculative decoding test configurations to match the original model precision

- **Numerical Stability**: JackFram/llama models were trained in fp32; using fp16 causes precision loss that manifests as differences between baseline and speculative decoding outputs

- **EAGLE Model Fix**: Updated EAGLE model to use `vllm_config.model_config.dtype` instead of `self.config.torch_dtype` for consistent dtype handling

- **Comprehensive Coverage**: Applied the fix across all speculative decoding test files to ensure consistent behavior

- **Code Consistency**: Some locations used `PRECISION` constant for dtype specification, maintaining existing patterns

#### PR Discussion & Comments

> **Development Team Discussion:**
> 
> **@gemini-code-assist[bot]** suggested formatting improvements:  
> *"For consistency with other additions in this PR, consider adding a blank line before this comment block."*

#### Key Takeaways
- **Precision Matching**: Test models should use the same precision they were trained with to avoid numerical instability
- **Speculative Decoding Sensitivity**: Speculative decoding correctness tests are particularly sensitive to floating-point precision differences
- **CI Stability**: Systematic dtype specification across test suites prevents flaky CI failures due to precision mismatches
- **Model Consistency**: All components (base model, speculative model, tests) should use consistent dtypes for reliable comparisons

#### Further Reading
- [vLLM Speculative Decoding Documentation](https://docs.vllm.ai)
- [PyTorch Floating Point Precision Guide](https://pytorch.org/docs/stable/notes/numerical_accuracy.html)
- [Original PR #19644](https://github.com/vllm-project/vllm/pull/19644)

:::

::: {.callout-warning collapse="true" title="üêõ Bug Fix - PR #19633: Fix the speculative decoding test by setting the target dtype"}
## [#19633](https://github.com/vllm-project/vllm/pull/19633) - Fix the speculative decoding test by setting the target dtype

#### Overview
This PR addresses test failures in the speculative decoding pipeline caused by dtype mismatches. The failure was introduced by [PR #18751](https://github.com/vllm-project/vllm/pull/18751), which changed default precision behavior. The fix explicitly sets `dtype: "float32"` for all multistep correctness tests to maintain numerical stability.

**Motivation**: Restore CI stability by ensuring consistent precision across speculative decoding tests, preventing numerical inconsistencies between baseline and speculative outputs.

#### Code Changes (Verified)

**File**: `tests/spec_decode/e2e/test_multistep_correctness.py`

```diff
@@ -57,6 +57,9 @@
 
         # Skip cuda graph recording for fast test.
         "enforce_eager": True,
+
+        # The original model is float32, keep it for numerical stability.
+        "dtype": "float32",
     }])
```

```diff
@@ -139,6 +142,9 @@ def test_spec_decode_e2e_with_detokenization(test_llm_generator,
 
         # Print spec metrics.
         "disable_log_stats": False,
+
+        # The original model is float32, keep it for numerical stability.
+        "dtype": "float32",
     }])
```

```diff
@@ -216,6 +222,9 @@ def test_spec_decode_e2e_greedy_correctness_tiny_model_bs1(
 
         # Print spec metrics.
         "disable_log_stats": False,
+
+        # The original model is float32, keep it for numerical stability.
+        "dtype": "float32",
     }])
```

```diff
@@ -464,6 +476,8 @@ def test_spec_decode_e2e_greedy_correctness_real_model_large_bs(
 
         # Skip cuda graph recording for fast test.
         "enforce_eager": True,
+        # The original model is float32, keep it for numerical stability.
+        "dtype": "float32",
     }])
```

**Explanation**: 

- **Systematic Dtype Enforcement**: Added explicit `"dtype": "float32"` to all test configurations in the multistep correctness test suite

- **Regression Fix**: Addresses changes from PR #18751 that modified default precision behavior, causing speculative decoding tests to fail due to numerical differences

- **Comprehensive Coverage**: Applied the fix to 8 different test scenarios within the same file, ensuring consistent behavior across all multistep correctness tests

- **Test Success**: All 64 tests now pass after applying the dtype fix (runtime: 32:35)

- **Comment Consistency**: Each addition includes explanatory comments about float32 being the original model precision for numerical stability

#### PR Discussion & Comments

> **Development Team Discussion:**
> 
> **@gemini-code-assist[bot]** suggested code improvement for maintainability:  
> *"This dtype setting and its accompanying comment are repeated in several places throughout this file. To enhance maintainability and adhere to the DRY principle, consider defining these common keyword arguments in a shared dictionary."*

The bot suggested creating a shared dictionary like:
```python
_COMMON_FLOAT32_KWARGS = {
    # The original model is float32, keep it for numerical stability.
    "dtype": "float32",
}
```

#### Key Takeaways
- **Regression Prevention**: Changes to default precision behavior should consider impact on existing test suites
- **Test Brittleness**: Speculative decoding correctness tests are highly sensitive to floating-point precision changes
- **Code Duplication**: While the fix is effective, the repetitive nature of the changes highlights potential for refactoring common test configurations
- **Cascading Effects**: Single PRs affecting precision can break multiple test suites that depend on consistent numerical behavior

#### Further Reading
- [vLLM Speculative Decoding Documentation](https://docs.vllm.ai)
- [Related PR #18751](https://github.com/vllm-project/vllm/pull/18751) (Original culprit)
- [Original PR #19633](https://github.com/vllm-project/vllm/pull/19633)

:::

::: {.callout-warning collapse="true" title="üêõ Bug Fix - PR #19561: Don't attempt to use triton if no driver is active"}
## [#19561](https://github.com/vllm-project/vllm/pull/19561) - Don't attempt to use triton if no driver is active

#### Overview
This PR fixes a critical issue where vLLM would attempt to use Triton kernels even when no active GPU driver is available, causing runtime crashes. The problem occurs when Triton is installed by external packages (like xgrammar) on non-GPU platforms, leading to conflicts during MLA (Multi-Layer Attention) backend imports.

**Motivation**: Prevent runtime crashes on platforms where Triton is installed but incompatible due to missing or inactive GPU drivers, ensuring graceful fallback behavior.

#### Code Changes (Verified)

**File**: `vllm/triton_utils/importing.py`

```diff
@@ -12,6 +12,36 @@
     find_spec("triton") is not None
     or find_spec("pytorch-triton-xpu") is not None  # Not compatible
 )
+if HAS_TRITON:
+    try:
+        from triton.backends import backends
+
+        # It's generally expected that x.driver exists and has
+        # an is_active method.
+        # The `x.driver and` check adds a small layer of safety.
+        active_drivers = [
+            x.driver for x in backends.values()
+            if x.driver and x.driver.is_active()
+        ]
+        if len(active_drivers) != 1:
+            logger.info(
+                "Triton is installed but %d active driver(s) found "
+                "(expected 1). Disabling Triton to prevent runtime errors.",
+                len(active_drivers))
+            HAS_TRITON = False
+    except ImportError:
+        # This can occur if Triton is partially installed or triton.backends
+        # is missing.
+        logger.warning(
+            "Triton is installed, but `triton.backends` could not be imported. "
+            "Disabling Triton.")
+        HAS_TRITON = False
+    except Exception as e:
+        # Catch any other unexpected errors during the check.
+        logger.warning(
+            "An unexpected error occurred while checking Triton active drivers:"
+            " %s. Disabling Triton.", e)
+        HAS_TRITON = False
 
 if not HAS_TRITON:
     logger.info("Triton not installed or not compatible; certain GPU-related"
```

**Explanation**: 

- **Driver Validation**: Added comprehensive check for active Triton backends by examining `triton.backends.backends.values()` and filtering for drivers where `x.driver.is_active()` returns `True`

- **Expected Driver Count**: Enforces that exactly one active driver should exist; zero drivers indicates no GPU support, while multiple drivers suggests an ambiguous configuration

- **Graceful Fallback**: When driver validation fails, `HAS_TRITON` is set to `False`, allowing vLLM to fall back to non-Triton alternatives

- **Robust Error Handling**: Three-tier exception handling covers ImportError (missing backends), generic exceptions (corrupted installations), and ensures vLLM never crashes during import due to Triton issues

- **Informative Logging**: Specific log messages help users understand why Triton was disabled, distinguishing between driver count issues, import failures, and unexpected errors

#### PR Discussion & Comments

> **Development Team Discussion:**
> 
> **@gemini-code-assist[bot] ‚Üí @kzawora-intel** ‚Äî Enhanced error handling and logging needed for robustness  
> *"The added check for active Triton drivers is a good approach to prevent runtime errors when Triton is installed but not usable."*
> 
> **@kzawora-intel** confirmed implementation of suggestions:  
> *"done"*

#### Key Takeaways
- **Cross-Platform Compatibility**: External packages can install Triton on incompatible platforms, requiring runtime validation rather than just package detection
- **Error Prevention**: Proactive driver validation prevents cryptic runtime crashes like `"RuntimeError: 0 active drivers ([]). There should only be one."`
- **Graceful Degradation**: vLLM should always have fallback paths when hardware-specific optimizations are unavailable
- **Import-Time Validation**: Critical infrastructure checks should occur at import time to fail fast and provide clear error messages

#### Further Reading
- [vLLM Triton Integration Documentation](https://docs.vllm.ai)
- [Triton GPU Programming Guide](https://triton-lang.org/)
- [Original PR #19561](https://github.com/vllm-project/vllm/pull/19561)

:::
4. **[#19644](https://github.com/vllm-project/vllm/pull/19644)** - [Bugfix][2/n] Fix speculative decoding CI - Fix test_ngram_e2e_greedy_correctness
5. **[#19633](https://github.com/vllm-project/vllm/pull/19633)** - [Bugfix][1/n] Fix the speculative decoding test by setting the target dtype
6. **[#19262](https://github.com/vllm-project/vllm/pull/19262)** - [Fix] Convert kv_transfer_config from dict to KVTransferConfig
7. **[#19660](https://github.com/vllm-project/vllm/pull/19660)** - [Misc] Fix skipped max-model-len validation when deriving max model length from tokenizer config
8. **[#19583](https://github.com/vllm-project/vllm/pull/19583)** - [Bugfix][Ray] Set the cuda context eagerly in the ray worker
9. **[#19725](https://github.com/vllm-project/vllm/pull/19725)** - [Bugfix] fix RAY_CGRAPH_get_timeout is not set successfully
10. **[#19872](https://github.com/vllm-project/vllm/pull/19872)** - [CI/Build][Bugfix] Fix deadlock on v1 engine test CI
11. **[#19875](https://github.com/vllm-project/vllm/pull/19875)** - [Fix] import regex instead of re
12. **[#18032](https://github.com/vllm-project/vllm/pull/18032)** - [Benchmark] Fix `Value of type "SampleRequest" is not indexable`
13. **[#19901](https://github.com/vllm-project/vllm/pull/19901)** - [CPU][CI] Fallback sliding window to v0 and fix CPU pooling model tests
14. **[#19589](https://github.com/vllm-project/vllm/pull/19589)** - [CI/Build] Fix torch nightly CI dependencies part 2

## ‚ö° Performance & Optimization (16 PRs)

1. **[#19500](https://github.com/vllm-project/vllm/pull/19500)** - [Hardware][NVIDIA][kernel] Fp4 MOE quant kernel optimization
2. **[#19566](https://github.com/vllm-project/vllm/pull/19566)** - [Perf] Further tunings for SM100 FP8 CUTLASS kernel
3. **[#18777](https://github.com/vllm-project/vllm/pull/18777)** - Export NaNs in logits to scheduler_stats if output is corrupted
4. **[#18354](https://github.com/vllm-project/vllm/pull/18354)** - [V1][Metrics] Deprecate metrics with gpu_ prefix for non GPU specific metrics

## ü§ñ Model Support (8 PRs)

1. **[#19663](https://github.com/vllm-project/vllm/pull/19663)** - [Model] GPT2ForSequenceClassification model

## üèóÔ∏è CI/Build (7 PRs)

1. **[#19508](https://github.com/vllm-project/vllm/pull/19508)** - Adding "AMD: Multi-step Tests" to amdproduction
2. **[#19648](https://github.com/vllm-project/vllm/pull/19648)** - Only build CUTLASS MoE kernels on Hopper
3. **[#19649](https://github.com/vllm-project/vllm/pull/19649)** - [Misc] Remove duplicate multiproc method setting for CPU platform

## üîß Kernel & Hardware (12 PRs)

1. **[#19745](https://github.com/vllm-project/vllm/pull/19745)** - [Kernel] correct cpu worker function parameter type
2. **[#19749](https://github.com/vllm-project/vllm/pull/19749)** - [Kernel] mark TorchSDPABackend swap_blocks NotImplementedError

## üåê Frontend & API (5 PRs)

1. **[#19564](https://github.com/vllm-project/vllm/pull/19564)** - [Misc][Frontend] passthrough `bad_words`

## üìö Documentation (5 PRs)

1. **[#19526](https://github.com/vllm-project/vllm/pull/19526)** - [Misc] update cuda version
2. **[#19851](https://github.com/vllm-project/vllm/pull/19851)** - [Misc] refactor example - openai_transcription_client

## üßπ Miscellaneous & Cleanup (21 PRs)

1. **[#19593](https://github.com/vllm-project/vllm/pull/19593)** - [Misc] Modularize CLI Argument Parsing in Benchmark Scripts
2. **[#19609](https://github.com/vllm-project/vllm/pull/19609)** - [MISC] Remove unused variables in C++
3. **[#19672](https://github.com/vllm-project/vllm/pull/19672)** - [MISC] typo fix
4. **[#19889](https://github.com/vllm-project/vllm/pull/19889)** - [Misc] Clean up useless code

## üöÄ V1 Engine (8 PRs)

1. **[#19164](https://github.com/vllm-project/vllm/pull/19164)** - [custom_op][vllm-plugin] update custom_op class to use op_registry

## üéØ Speculative Decoding (2 PRs)

*Already listed in Bug Fixes section*

## üîß TPU (3 PRs)

1. **[#19620](https://github.com/vllm-project/vllm/pull/19620)** - [TPU] support attention head dim smaller than 128

## üî¥ AMD/ROCm (1 PR)

*Already listed in CI/Build section*

## üõ†Ô∏è Tool Calling (1 PR)

*Already listed in Bug Fixes section*

## üîó Full CUDA Graphs (1 PR)

1. **[#19617](https://github.com/vllm-project/vllm/pull/19617)** - Enable prefix caching with full cuda graphs

:::