---
aliases:
- /vllm-merged-prs-last-week-june-13-20/
categories:
- Open Source
- Python
- FastAPI
- Performance
date: '2025-07-20'
image: /images/opensource/vllm_merged_prs_last_week_june_13_20.png
title: "vLLM Merged PRs last week (June 13-20)"
subtitle: "A look at the PRs merged to vLLM last week"
format: html
---
# vLLM Merged PRs - Last Week (June 13-20, 2025)

**Total PRs: 114**

## Summary by Category

- **Bug Fixes**: 28 PRs (24.6%)
- **Miscellaneous & Cleanup**: 21 PRs (18.4%)
- **Performance & Optimization**: 16 PRs (14.0%)
- **Kernel & Hardware**: 12 PRs (10.5%)
- **V1 Engine**: 8 PRs (7.0%)
- **Model Support**: 8 PRs (7.0%)
- **CI/Build**: 7 PRs (6.1%)
- **Frontend & API**: 5 PRs (4.4%)
- **Documentation**: 5 PRs (4.4%)
- **TPU**: 3 PRs (2.6%)
- **Other categories**: 1 PR each

## Key Highlights

1. **Major Focus on Bug Fixes**: Nearly 25% of PRs were bug fixes, showing active maintenance
2. **Performance Improvements**: Significant work on FP4 MOE kernels and CUTLASS optimizations
3. **V1 Engine Development**: Continued development of the new V1 engine architecture
4. **Speculative Decoding Fixes**: Multiple fixes for speculative decoding test stability
5. **Hardware Support**: Updates for TPU, AMD ROCm, and NVIDIA hardware
6. **Code Quality**: Many cleanup and maintenance PRs to improve code quality

::: {.panel-tabset}

## üêõ Bug Fixes (28 PRs)

::: {.callout-warning collapse="true" title="üêõ Bug Fix - PR #19624: Fix DP Coordinator incorrect debug log message"}
## [#19624](https://github.com/vllm-project/vllm/pull/19624) - Fix DP Coordinator incorrect debug log message

#### Overview
This PR fixes a logging bug in the Data Parallel (DP) Coordinator where debug messages were showing incorrect wave numbers. The coordinator was logging `"Moving DP wave from N to N"` instead of the correct `"Moving DP wave from N to (N + 1)"`. This affects the V1 engine's data parallel processing wave management system.

**Motivation**: Accurate debug logging is crucial for troubleshooting distributed processing coordination issues in vLLM's V1 engine architecture.

#### Code Changes (Verified)

**File**: `vllm/v1/engine/coordinator.py`

```diff
@@ -183,11 +183,12 @@ def process_input_socket(self, front_publish_address: str,
                     # engines are paused, so that we can wake the other
                     # engines.
                     engine_to_exclude, wave = msgspec.msgpack.decode(buffer)
-                    if wave < self.current_wave:
-                        # If the wave number is stale, ensure the message is
-                        # handled by all the engines.
-                        engine_to_exclude = None
                     if not self.engines_running:
+                        if wave < self.current_wave:
+                            # If the wave number is stale, ensure the message
+                            # is handled by all the engines.
+                            engine_to_exclude = None
+
                         self.engines_running = True
                         self.stats_changed = True
                         self._send_start_wave(publish_back, self.current_wave,
@@ -217,8 +218,10 @@ def process_input_socket(self, front_publish_address: str,
                         # (engines_running==False).
                         if self.current_wave <= wave:
+                            new_wave = wave + 1
                             logger.debug("Moving DP wave from %d to %d.",
-                                         self.current_wave, wave)
-                            self.current_wave = wave + 1
+                                         self.current_wave, new_wave)
+                            self.current_wave = new_wave
                             self.engines_running = False
                             self.stats_changed = True
```

**Explanation**: 

- **Main Fix**: Introduced a `new_wave` variable to compute `wave + 1` before logging, ensuring the debug message shows the correct transition from current wave to next wave

- **Code Refactoring**: Moved the stale wave check inside the `if not self.engines_running` block for better logical grouping (noted as "unrelated simplification" by author)

- **Variable Extraction**: Extracted `scheduler_stats` to a local variable to avoid repeated attribute access, improving code readability

#### PR Discussion & Comments

> **Development Team Discussion:**
> 
> **@njhill** commented on code organization improvement:  
> *"This is unrelated simplification, this check only applies to the case inside the if."*
> 
> **@njhill** identified the main bug:  
> *"This is the actual bug fix"*

#### Key Takeaways
- **Bug Impact**: Incorrect debug logging can mislead developers during troubleshooting of distributed processing coordination
- **Simple but Important**: The fix demonstrates how even small logging errors should be corrected for proper debugging experience
- **Code Quality**: The PR also includes minor refactoring improvements alongside the main fix
- **V1 Engine Context**: This affects the newer V1 engine architecture's data parallel processing system

#### Further Reading
- [vLLM V1 Engine Documentation](https://docs.vllm.ai)
- [Original PR #19624](https://github.com/vllm-project/vllm/pull/19624)

:::


::: {.callout-warning collapse="true" title="üêõ Bug Fix - PR #19316: Fix auto dtype casting for BatchFeature"}
## [#19316](https://github.com/vllm-project/vllm/pull/19316) - Fix auto dtype casting for BatchFeature

#### Overview
This PR fixes a critical dtype casting issue for `BatchFeature` objects in vision-language models, particularly affecting DeepSeek VL2. The bug occurred because `BatchFeature` is a `UserDict`, and `isinstance(value, dict)` returns `False`, causing data type casting to fail in the `json_map_leaves` function.

**Motivation**: Proper dtype casting is essential for vision-language models to ensure tensor operations are performed with consistent data types, preventing runtime errors like `"Input type (float) and bias type (c10::BFloat16) should be the same"`.

#### Code Changes (Verified)

**File**: `vllm/inputs/registry.py`

```diff
@@ -168,10 +168,12 @@ def maybe_cast_dtype(x):
         try:
             output = hf_processor(**data, **merged_kwargs, return_tensors="pt")
             # this emulates output.to(dtype=self.model_config.dtype)
-            cast_output = json_map_leaves(maybe_cast_dtype, output)
             if isinstance(output, BatchFeature):
+                cast_output = json_map_leaves(maybe_cast_dtype, output.data)
                 return BatchFeature(cast_output)

+            cast_output = json_map_leaves(maybe_cast_dtype, output)
+
             logger.warning_once(
                 f"{type(hf_processor).__name__} did not return `BatchFeature`. "
                 "Make sure to match the behaviour of `ProcessorMixin` when "
```

**File**: `vllm/model_executor/models/qwen2_vl.py` (and `qwen2_5_vl.py`)

```diff
@@ -1208,9 +1208,9 @@ def _process_image_input(
         assert grid_thw.ndim == 2

         if image_input["type"] == "image_embeds":
-            image_embeds = image_input["image_embeds"].type(self.visual.dtype)
+            image_embeds = image_input["image_embeds"]
         else:
-            pixel_values = image_input["pixel_values"].type(self.visual.dtype)
+            pixel_values = image_input["pixel_values"]
             image_embeds = self.visual(pixel_values, grid_thw=grid_thw)
```

**File**: `vllm/utils.py`

```diff
@@ -190,6 +190,16 @@
     torch.int64: np.int64,
 }

+
+@contextlib.contextmanager
+def set_default_torch_num_threads(num_threads: int):
+    """Sets the default number of threads for PyTorch to the given value."""
+    old_num_threads = torch.get_num_threads()
+    torch.set_num_threads(num_threads)
+    yield
+    torch.set_num_threads(old_num_threads)
+
+
 P = ParamSpec('P')
 T = TypeVar("T")
 U = TypeVar("U")
```

**Explanation**: 

- **Main Fix**: Modified `json_map_leaves` to work with `BatchFeature.data` instead of the `BatchFeature` object directly, since `BatchFeature` inherits from `UserDict`

- **Model Updates**: Removed explicit `.type()` casting in Qwen2 VL models since the casting is now handled correctly at the input processing level

- **Test Infrastructure**: Added `set_default_torch_num_threads` utility to prevent OpenMP deadlocks during test execution

- **Circular Import Fix**: Moved utility functions to `vllm.utils` to avoid circular import issues

#### PR Discussion & Comments

> **Development Team Discussion:**
> 
> **@Isotr0py** explained the circular import resolution:  
> *"I move this to vllm.utils to avoid circular import: ImportError: cannot import name 'LoadConfig' from partially initialized module 'vllm.config'"*
> 
> **@Isotr0py** addressed deadlock prevention:  
> *"Seems that disable openmp by setting torch_num_threads=1 during engine forking can fix the deadlock issue locally"*

#### Key Takeaways
- **Vision-Language Models**: `BatchFeature` objects require special handling since they inherit from `UserDict`, not `dict`
- **Type Safety**: Proper dtype casting prevents runtime errors in mixed-precision scenarios common in modern LLM inference
- **Test Stability**: OpenMP thread management is crucial for preventing deadlocks in multi-process test environments
- **Code Organization**: Moving utility functions to appropriate modules prevents circular dependencies

#### Further Reading
- [vLLM Vision-Language Models Documentation](https://docs.vllm.ai)
- [Original Issue #19219](https://github.com/vllm-project/vllm/issues/19219)
- [Original PR #19316](https://github.com/vllm-project/vllm/pull/19316)

:::

---

**Other Bug Fixes:**

3. **[#19561](https://github.com/vllm-project/vllm/pull/19561)** - [Bugfix] Don't attempt to use triton if no driver is active
4. **[#19644](https://github.com/vllm-project/vllm/pull/19644)** - [Bugfix][2/n] Fix speculative decoding CI - Fix test_ngram_e2e_greedy_correctness
5. **[#19633](https://github.com/vllm-project/vllm/pull/19633)** - [Bugfix][1/n] Fix the speculative decoding test by setting the target dtype
6. **[#19262](https://github.com/vllm-project/vllm/pull/19262)** - [Fix] Convert kv_transfer_config from dict to KVTransferConfig
7. **[#19660](https://github.com/vllm-project/vllm/pull/19660)** - [Misc] Fix skipped max-model-len validation when deriving max model length from tokenizer config
8. **[#19583](https://github.com/vllm-project/vllm/pull/19583)** - [Bugfix][Ray] Set the cuda context eagerly in the ray worker
9. **[#19725](https://github.com/vllm-project/vllm/pull/19725)** - [Bugfix] fix RAY_CGRAPH_get_timeout is not set successfully
10. **[#19872](https://github.com/vllm-project/vllm/pull/19872)** - [CI/Build][Bugfix] Fix deadlock on v1 engine test CI
11. **[#19875](https://github.com/vllm-project/vllm/pull/19875)** - [Fix] import regex instead of re
12. **[#18032](https://github.com/vllm-project/vllm/pull/18032)** - [Benchmark] Fix `Value of type "SampleRequest" is not indexable`
13. **[#19901](https://github.com/vllm-project/vllm/pull/19901)** - [CPU][CI] Fallback sliding window to v0 and fix CPU pooling model tests
14. **[#19589](https://github.com/vllm-project/vllm/pull/19589)** - [CI/Build] Fix torch nightly CI dependencies part 2

## ‚ö° Performance & Optimization (16 PRs)

1. **[#19500](https://github.com/vllm-project/vllm/pull/19500)** - [Hardware][NVIDIA][kernel] Fp4 MOE quant kernel optimization
2. **[#19566](https://github.com/vllm-project/vllm/pull/19566)** - [Perf] Further tunings for SM100 FP8 CUTLASS kernel
3. **[#18777](https://github.com/vllm-project/vllm/pull/18777)** - Export NaNs in logits to scheduler_stats if output is corrupted
4. **[#18354](https://github.com/vllm-project/vllm/pull/18354)** - [V1][Metrics] Deprecate metrics with gpu_ prefix for non GPU specific metrics

## ü§ñ Model Support (8 PRs)

1. **[#19663](https://github.com/vllm-project/vllm/pull/19663)** - [Model] GPT2ForSequenceClassification model

## üèóÔ∏è CI/Build (7 PRs)

1. **[#19508](https://github.com/vllm-project/vllm/pull/19508)** - Adding "AMD: Multi-step Tests" to amdproduction
2. **[#19648](https://github.com/vllm-project/vllm/pull/19648)** - Only build CUTLASS MoE kernels on Hopper
3. **[#19649](https://github.com/vllm-project/vllm/pull/19649)** - [Misc] Remove duplicate multiproc method setting for CPU platform

## üîß Kernel & Hardware (12 PRs)

1. **[#19745](https://github.com/vllm-project/vllm/pull/19745)** - [Kernel] correct cpu worker function parameter type
2. **[#19749](https://github.com/vllm-project/vllm/pull/19749)** - [Kernel] mark TorchSDPABackend swap_blocks NotImplementedError

## üåê Frontend & API (5 PRs)

1. **[#19564](https://github.com/vllm-project/vllm/pull/19564)** - [Misc][Frontend] passthrough `bad_words`

## üìö Documentation (5 PRs)

1. **[#19526](https://github.com/vllm-project/vllm/pull/19526)** - [Misc] update cuda version
2. **[#19851](https://github.com/vllm-project/vllm/pull/19851)** - [Misc] refactor example - openai_transcription_client

## üßπ Miscellaneous & Cleanup (21 PRs)

1. **[#19593](https://github.com/vllm-project/vllm/pull/19593)** - [Misc] Modularize CLI Argument Parsing in Benchmark Scripts
2. **[#19609](https://github.com/vllm-project/vllm/pull/19609)** - [MISC] Remove unused variables in C++
3. **[#19672](https://github.com/vllm-project/vllm/pull/19672)** - [MISC] typo fix
4. **[#19889](https://github.com/vllm-project/vllm/pull/19889)** - [Misc] Clean up useless code

## üöÄ V1 Engine (8 PRs)

1. **[#19164](https://github.com/vllm-project/vllm/pull/19164)** - [custom_op][vllm-plugin] update custom_op class to use op_registry

## üéØ Speculative Decoding (2 PRs)

*Already listed in Bug Fixes section*

## üîß TPU (3 PRs)

1. **[#19620](https://github.com/vllm-project/vllm/pull/19620)** - [TPU] support attention head dim smaller than 128

## üî¥ AMD/ROCm (1 PR)

*Already listed in CI/Build section*

## üõ†Ô∏è Tool Calling (1 PR)

*Already listed in Bug Fixes section*

## üîó Full CUDA Graphs (1 PR)

1. **[#19617](https://github.com/vllm-project/vllm/pull/19617)** - Enable prefix caching with full cuda graphs

:::