{
  "hash": "cd1edb1b11d7ee9f48117564f9e23b2f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\naliases: [\"/what-are-image-embeddings/\"]\ncategories: [\"Computer Vision\", \"Machine Learning\"]\ndate: \"2025-03-16\"\nimage: \"/images/what_are_image_embeddings/thumbnail.png\"\ntitle: \"What are Image Embeddings?\"\nsubtitle: \"Understanding how images are represented as numerical vectors for AI applications\"\ncolab: '<a href=\"https://colab.research.google.com/drive/1T66Ae_EcUo7KqcQcuAftcJ1oJiVZv5YO?usp=sharing\"><img src=\"images/colab.svg\" alt=\"Open In Colab\"></a>'\nformat: \"html\"\n---\n\n\nThis notebook explores the concept of image embeddings, how they work, and their applications in AI. We'll focus on Google's SigLIP 2, a state-of-the-art multilingual vision-language encoder, and demonstrate its practical applications through visualization, clustering, and text-image similarity analysis.\n\n> **Note:** The complete code for this article is available in this [Colab notebook](https://colab.research.google.com/drive/1T66Ae_EcUo7KqcQcuAftcJ1oJiVZv5YO?usp=sharing).\n\n## Introduction\nImage embeddings are numerical representations of images that capture their semantic content in a way that's useful for machine learning algorithms[^1]. At their core, embeddings are dense vectors—typically consisting of hundreds or thousands of floating-point numbers—that represent images in a high-dimensional space where similar images are positioned close to each other[^2].\n\n### Why Do We Need Image Embeddings?\n\nImages in their raw pixel form are:\n\n- **High-dimensional**: A 224x224 RGB image contains 150,528 pixel values\n- **Not semantically organized**: Similar-looking images might have very different pixel values\n- **Difficult to work with**: Comparing raw pixels doesn't capture semantic similarity\n\nEmbeddings solve these problems by:\n\n- **Reducing dimensionality**: Typically to a few hundred or thousand dimensions\n- **Capturing semantics**: Images with similar content have similar embeddings\n- **Enabling efficient search**: Finding similar images becomes a vector similarity search[^3]\n- **Supporting transfer learning**: Pre-trained embeddings can be used for various downstream tasks[^4]\n\n[^1]: Bengio, Y., Courville, A., & Vincent, P. (2013). Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8), 1798-1828. https://doi.org/10.1109/TPAMI.2013.50\n\n[^2]: Pan, S. J., & Yang, Q. (2009). A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10), 1345-1359. https://doi.org/10.1109/TKDE.2009.191\n\n[^3]: Johnson, J., Douze, M., & Jégou, H. (2019). Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3), 535-547. https://doi.org/10.1109/TBDATA.2019.2921572\n\n[^4]: He, K., Girshick, R., & Dollár, P. (2018). Rethinking ImageNet pre-training. Proceedings of the IEEE/CVF International Conference on Computer Vision, 4918-4927. https://arxiv.org/abs/1811.08883\n\n## How Image Embeddings Work\n\nModern image embeddings are typically created using deep neural networks, particularly convolutional neural networks (CNNs)[^5] or vision transformers (ViTs)[^6]. These networks learn to transform raw pixels into compact, semantically meaningful representations through extensive training on large datasets.\n\n![Vision Transformer Architecture](https://i.imgur.com/n0vrUs8.png)\n*Figure 2: Vision Transformer (ViT) architecture. The image is divided into patches which are linearly embedded, positional encodings are added, and the resulting sequence is processed by a standard Transformer encoder. This approach allows transformers to effectively process visual information similarly to how they handle text. Adapted from Dosovitskiy et al. (2021)[^6].*\n\nThe process generally involves:\n\n1. **Training**: Neural networks are trained on large image datasets, often using self-supervised or weakly-supervised learning approaches[^7]\n2. **Feature extraction**: The trained network processes an image through its layers\n3. **Embedding generation**: The network's final or penultimate layer outputs become the embedding vector\n\nThese embeddings can then be used for various tasks:\n\n- **Image similarity**: Finding visually or semantically similar images\n- **Image classification**: Categorizing images into predefined classes\n- **Image retrieval**: Finding relevant images based on text queries\n- **Zero-shot learning**: Recognizing objects the model wasn't explicitly trained on[^8]\n- **Transfer learning**: Using pre-trained embeddings for new tasks with limited data\n\n[^5]: Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems, 25. https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n\n[^6]: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., & Houlsby, N. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations. https://arxiv.org/abs/2010.11929\n\n[^7]: Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020). A simple framework for contrastive learning of visual representations. International Conference on Machine Learning, 1597-1607. https://arxiv.org/abs/2002.05709\n\n[^8]: Xian, Y., Lampert, C. H., Schiele, B., & Akata, Z. (2018). Zero-shot learning—A comprehensive evaluation of the good, the bad and the ugly. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(9), 2251-2265. https://arxiv.org/abs/1707.00600\n\n## SigLIP 2: Google's Advanced Multilingual Vision-Language Encoder\n\nSigLIP 2 represents the latest advancement in image embedding technology[^9]. Developed by Google and released in early 2024, it significantly improves upon its predecessor by offering enhanced semantic understanding, better localization capabilities, and more effective dense feature representation.\n\n[^9]: Beyer, L., Dehghani, M., et al. (2024). SigLIP 2: Next-Generation Multilingual Vision-Language Models. Google Research. https://arxiv.org/abs/2409.01936\n\n### Technical Background and Evolution\n\n#### From CLIP to SigLIP to SigLIP 2\n\nVision-language models have evolved considerably in recent years:\n\n1. **CLIP and ALIGN**: These pioneered the approach of jointly training image and text encoders to understand the semantic relationship between visual data and natural language[^10]\n\n![Contrast function comparison between CLIP and SigLIP](https://i.imgur.com/GH9sai5.png)\n*Figure 1: Comparison of contrast functions in CLIP (contrastive loss) and SigLIP (sigmoid loss). Adapted from Zhai et al. (2023).*\n\n2. **SigLIP (1st generation)**: Improved upon CLIP by replacing its contrastive loss function with a simpler pairwise sigmoid loss[^11]. Instead of requiring a global view of pairwise similarities for normalization (as in contrastive learning), the sigmoid loss operated only on image-text pairs, allowing for better scaling and improved performance even with smaller batch sizes\n\n3. **SigLIP 2**: Extends this foundation by incorporating several additional training techniques into a unified recipe, creating more powerful and versatile vision-language encoders that outperform their predecessors across all model scales[^12]\n\n[^10]: Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G. (2021). Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (pp. 8748-8763). PMLR. https://arxiv.org/abs/2103.00020\n\n[^11]: Zhai, X., Wang, X., Mustafa, B., Steiner, A., Keysers, D., Kolesnikov, A., & Beyer, L. (2023). Sigmoid loss for language image pre-training. In Proceedings of the 40th International Conference on Machine Learning (pp. 40844-40858). PMLR. https://arxiv.org/abs/2303.15343\n\n[^12]: Google. (2024). SigLIP 2 - GitHub Documentation. https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/README_siglip2.md\n\n### How SigLIP 2 Works\n\n#### Enhanced Training Methodology\n\nSigLIP 2's functioning is fundamentally based on its innovative training approach that combines multiple previously independent techniques[^13]:\n\n1. **Extended Training Objectives**: While preserving the original sigmoid loss function, SigLIP 2 integrates several additional training objectives:\n   - Captioning-based pretraining to enhance semantic understanding\n   - Self-supervised losses including self-distillation and masked prediction\n   - Online data curation for improved quality and diversity of training examples\n\n2. **Multilingual Capabilities**: The model is trained on a more diverse data mixture that incorporates de-biasing techniques, leading to significantly better multilingual understanding and improved fairness across different languages and cultures[^14]\n\n3. **Technical Implementation**: SigLIP 2 models use the Gemma tokenizer with a vocabulary size of 256,000 tokens, allowing for better representation of diverse languages[^15]\n\n[^13]: Google Research. (2024). SigLIP 2: Multilingual vision-language models with self-supervised learning. GitHub. https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/README_siglip2.md\n\n[^14]: Google. (2024). SigLIP 2 Technical Report. https://huggingface.co/papers/2502.14786\n\n[^15]: Google. (2024). Gemma Tokenizer. Hugging Face. https://huggingface.co/google/gemma-tokenizer\n\n#### Beyond Simple Cosine Similarity: Advanced Similarity Computation\n\nWhile many discussions of image embeddings focus on simple cosine similarity between vectors, SigLIP 2's similarity computation is actually much more sophisticated[^16]. This advanced approach leads to more accurate and nuanced similarity scores:\n\n1. **Multi-head Attention Pooling (MAP)**: Unlike simpler models that use average pooling to aggregate token representations, SigLIP 2 employs a more sophisticated attention-based pooling mechanism[^17]:\n   - The MAP head learns to focus on the most relevant parts of the image or text\n   - It assigns different weights to different regions or tokens based on their importance\n   - This selective attention mechanism produces more contextually relevant embeddings that capture important details while ignoring noise\n\n2. **Temperature Scaling**: SigLIP 2 applies a learned temperature parameter (τ) to scale similarity scores[^18]:\n   - Raw cosine similarities are divided by this temperature: sim(i,j)/τ\n   - Lower temperature values make the distribution more \"peaked,\" emphasizing differences between high and low similarity pairs\n   - Higher temperature values make the distribution more uniform\n   - The temperature parameter is learned during training to optimize the model's discrimination ability\n\n3. **Bias Term Adjustment**: The similarity calculation includes a learned bias term:\n   - sim'(i,j) = sim(i,j)/τ + b, where b is the learned bias\n   - This bias helps counteract the inherent imbalance between positive and negative pairs during training\n   - It acts as a calibration factor, adjusting the similarity scores to better reflect true semantic relationships\n\n4. **Sigmoid Activation**: Unlike models that use softmax normalization (like CLIP), SigLIP 2 applies a sigmoid function to the adjusted similarity scores:\n   - p(i,j) = sigmoid(sim'(i,j)) = 1/(1+exp(-(sim(i,j)/τ + b)))\n   - This transforms the unbounded similarity scores into well-calibrated probability-like values in the range [0,1]\n   - The sigmoid function allows each image-text pair to be evaluated independently, which is more appropriate for retrieval tasks\n\nThese components work together to ensure that SigLIP 2's similarity calculations go far beyond simple vector dot products. When using SigLIP 2, it's crucial to use the model's built-in comparison mechanism (`logits_per_image` followed by sigmoid activation) rather than manually computing cosine similarity on raw embeddings, as the former incorporates all these learned parameters and transformations that were optimized during training[^19].\n\n[^16]: Hugging Face. (2024). SigLIP 2 Model Documentation. https://huggingface.co/docs/transformers/en/model_doc/siglip2\n\n[^17]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems (pp. 5998-6008). https://arxiv.org/abs/1706.03762\n\n[^18]: Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531. https://arxiv.org/abs/1503.02531\n\n[^19]: Lukyanenko, A. (2024). Paper Review: SigLIP 2 - Multilingual Vision-Language Dense Encoder. https://www.linkedin.com/pulse/paper-review-siglip-2-multilingual-vision-language-dense-lukyanenko-7cvyf\n\n#### Architecture Variants\n\nSigLIP 2 is available in several architectural variants to accommodate different computational constraints and use cases[^20]:\n\n1. **Model Sizes**: The family includes four primary model sizes:\n   - ViT-B (86M parameters)\n   - ViT-L (303M parameters)\n   - ViT-So400m (400M parameters)\n   - ViT-g (1B parameters)\n\n2. **NaFlex Variants**: One of the most significant innovations in SigLIP 2 is the introduction of NaFlex variants, which support dynamic resolution and preserve the input's native aspect ratio[^21]. This feature is particularly valuable for:\n   - Optical character recognition (OCR)\n   - Document understanding\n   - Any task where preserving the original aspect ratio and resolution is important\n\n[^20]: Google. (2024). SigLIP 2 Model Collection. Hugging Face. https://huggingface.co/models?search=google%2Fsiglip2\n\n[^21]: Google. (2024). SigLIP 2 Gemma Toolkit. Google Developers Blog. https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/\n\n### Key Capabilities and Improvements\n\nSigLIP 2 models demonstrate significant improvements over the original SigLIP across several dimensions:\n\n1. **Core Capabilities**: The models outperform their SigLIP counterparts at all scales in:\n   - Zero-shot classification\n   - Image-text retrieval\n   - Transfer performance when used for visual representation in Vision-Language Models (VLMs)\n\n2. **Localization and Dense Features**: The enhanced training recipe leads to substantial improvements in localization and dense prediction tasks, making the models more effective for detailed visual understanding\n\n3. **Multilingual Understanding**: Through its diverse training data and de-biasing techniques, SigLIP 2 achieves much better multilingual understanding and improved fairness compared to previous models\n\n### Practical Applications\n\nThe improvements in SigLIP 2 make it particularly well-suited for:\n\n1. **Zero-shot Image Classification**: Using the model to classify images into categories it wasn't explicitly trained on\n\n2. **Image-Text Retrieval**: Finding relevant images based on text queries or finding appropriate textual descriptions for images\n\n3. **Feature Extraction for VLMs**: Providing high-quality visual representations that can be combined with large language models to build more capable vision-language models\n\n4. **Document and Text-Heavy Image Analysis**: Particularly with the NaFlex variants, which excel at tasks requiring preservation of aspect ratio and resolution\n\n## Practical Applications of Image Embeddings\n\nNow that we understand the theoretical background of image embeddings, let's explore their practical applications. Image embeddings form the foundation for numerous computer vision tasks and enable powerful capabilities like semantic search, clustering, and cross-modal understanding.\n\n### Key Applications of Image Embeddings\n\n1. **Visual Similarity Search**: Find visually similar images based on embedding distance\n2. **Image Clustering**: Group images by semantic content without explicit labels\n3. **Cross-Modal Understanding**: Connect images with text descriptions\n4. **Fine-Grained Recognition**: Identify specific attributes and details\n5. **Transfer Learning**: Apply pre-trained embeddings to new, domain-specific tasks\n\nSigLIP 2, with its powerful multilingual capabilities and improved semantic understanding, enables these applications with state-of-the-art performance. While SigLIP 2 comes in various sizes (Base, Large, So400m, and Giant) and configurations, we'll focus on the So400m model, which provides an excellent balance of quality and efficiency.\n\n## Implementing SigLIP 2: Practical Examples\n\nNow that we understand the theoretical background of image embeddings and SigLIP 2, let's implement it to see how it works in practice. We'll use the Hugging Face Transformers library, which provides easy access to SigLIP 2 models.\n\n### Resources for Following Along\n\nTo follow along with these examples, you'll need access to these resources:\n\n- **SigLIP 2 on Hugging Face**: [google/siglip2-so400m-patch14-384](https://huggingface.co/google/siglip2-so400m-patch14-384)\n- **Official Documentation**: [GitHub - SigLIP 2 README](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/README_siglip2.md)\n- **Zero-Shot Classification Guide**: [Hugging Face Documentation](https://huggingface.co/docs/transformers/en/tasks/zero_shot_image_classification)\n- **Required Python Libraries**: \n  - [Transformers](https://huggingface.co/docs/transformers/index)\n  - [PyTorch](https://pytorch.org/docs/stable/index.html)\n  - [UMAP-Learn](https://umap-learn.readthedocs.io/en/latest/)\n  - [Scikit-learn](https://scikit-learn.org/stable/)\n- **Recommended Environment**: Python 3.8+ with GPU support\n\n::: {#09d26bc9 .cell execution_count=1}\n``` {.python .cell-code}\n# Import necessary libraries\nimport sys\nimport os\nimport time\nimport requests  # For fetching images from URLs: https://docs.python-requests.org/\nimport numpy as np  # For numerical operations: https://numpy.org/doc/stable/\nimport matplotlib.pyplot as plt  # For visualization: https://matplotlib.org/stable/\nimport torch  # PyTorch deep learning framework: https://pytorch.org/docs/stable/\nfrom PIL import Image  # For image processing: https://pillow.readthedocs.io/\nfrom sklearn.cluster import KMeans  # For clustering: https://scikit-learn.org/stable/modules/clustering.html\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom transformers import pipeline, AutoModel, AutoProcessor  # Hugging Face Transformers: https://huggingface.co/docs/transformers/\nfrom transformers.image_utils import load_image\n```\n:::\n\n\n### Loading the SigLIP 2 Model\n\nWe'll use the So400m variant of SigLIP 2 for our examples, which offers an excellent balance of quality and efficiency. The most recent models are available with the \"google/siglip2-\" prefix.\n\n::: {#0b31ed4a .cell execution_count=2}\n``` {.python .cell-code}\n# We'll use the SO400M model which offers good performance\nmodel_name = \"google/siglip2-so400m-patch14-384\"\n\n# Define a function to extract embeddings from an image\ndef get_image_embedding(image_path_or_url, model, processor):\n    \"\"\"Extract embeddings from an image file or URL\n    \n    NOTE: For most SigLIP applications, you should NOT extract embeddings separately.\n    Instead, use the model to process image-text pairs together via model(**inputs)\n    to get direct similarity scores through the model's logits_per_image.\n    \n    This function is provided for educational purposes or for specific use cases\n    where you need the raw embeddings.\n    \"\"\"\n    # Load image from URL or local path\n    if isinstance(image_path_or_url, str):\n        if image_path_or_url.startswith(('http://', 'https://')):\n            image = Image.open(requests.get(image_path_or_url, stream=True).raw)\n        else:\n            image = Image.open(image_path_or_url)\n    else:\n        # Assuming it's already a PIL Image\n        image = image_path_or_url\n    \n    # Process image and extract embedding\n    inputs = processor(images=image, return_tensors=\"pt\")\n    \n    with torch.no_grad():\n        # Just get image features directly\n        image_embedding = model.get_image_features(**inputs)\n        image_embedding = image_embedding / image_embedding.norm(dim=1, keepdim=True)\n    \n    return image_embedding.squeeze().detach().numpy(), image\n```\n:::\n\n\n### Example 1: Zero-Shot Image Classification\n\nLet's use SigLIP 2 for zero-shot image classification. We'll load an image and classify it against different text prompts.\n\n::: {#df006d8c .cell execution_count=3}\n``` {.python .cell-code}\n# Set up the zero-shot classification pipeline\nfrom transformers import pipeline\nfrom PIL import Image\nimport requests\nimport matplotlib.pyplot as plt\n\n# SigLIP 2 uses the Gemma tokenizer which requires specific parameters\npipe = pipeline(\n    model=model_name, \n    task=\"zero-shot-image-classification\",\n)\n\ninputs = {\n    \"images\": [\n        \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000285.jpg\", # bear\n        \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000776.jpg\", # teddy bear\n    ],\n    \"texts\": [\n        \"bear looking into the camera\",\n        \"bear looking away from the camera\",\n        \"a bunch of teddy bears\",\n        \"two teddy bears\",\n        \"three teddy bears\"\n    ],\n}\n\n# Load images for display\ndisplay_images = []\nfor img_url in inputs[\"images\"]:\n    img = Image.open(requests.get(img_url, stream=True).raw)\n    display_images.append(img)\n\noutputs = pipe(inputs[\"images\"], candidate_labels=inputs[\"texts\"])\n\n# Display the outputs\nfor i, output in enumerate(outputs):\n    print(f\"Image {i+1} results:\")\n    for result in output:\n        print(f\"{result['label']}: {result['score']:.4f}\")\n    print()\n\n# Visualize the results with images on top\nfig, axes = plt.subplots(2, 2, figsize=(15, 8), gridspec_kw={'height_ratios': [0.6, 1]})\n\n# Display the images in the top row\nfor i, img in enumerate(display_images):\n    # Use 'equal' instead of 'auto' to maintain the correct aspect ratio\n    axes[0, i].imshow(img, aspect='equal')\n    axes[0, i].set_title(f\"Image {i+1}\")\n    axes[0, i].axis('off')\n\n# Display the classification results in the bottom row\nfor i, output in enumerate(outputs):\n    labels = [result['label'] for result in output]\n    scores = [result['score'] for result in output]\n    \n    axes[1, i].bar(range(len(labels)), scores)\n    axes[1, i].set_xticks(range(len(labels)))\n    axes[1, i].set_xticklabels(labels, rotation=45, ha='right')\n    axes[1, i].set_ylim(0, 1)\n    axes[1, i].set_title(f\"Image {i+1} Classification Results\")\n    axes[1, i].set_ylabel(\"Probability\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nDevice set to use mps:0\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nImage 1 results:\nbear looking into the camera: 0.9468\nbear looking away from the camera: 0.5860\ntwo teddy bears: 0.0000\nthree teddy bears: 0.0000\na bunch of teddy bears: 0.0000\n\nImage 2 results:\na bunch of teddy bears: 0.9882\nthree teddy bears: 0.9434\ntwo teddy bears: 0.0669\nbear looking away from the camera: 0.0099\nbear looking into the camera: 0.0093\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](2025-03-16_what_are_image_embeddings_files/figure-html/cell-4-output-3.png){width=1414 height=757}\n:::\n:::\n\n\n### Example 2: Image-Text Similarity\n\nNow let's explore how we can use SigLIP 2 to compute similarity between multiple images and texts.\n\n::: {#432314bc .cell execution_count=4}\n``` {.python .cell-code}\n# Load the model and processor\nfrom transformers import AutoModel, AutoProcessor\nmodel = AutoModel.from_pretrained(model_name)\nprocessor = AutoProcessor.from_pretrained(model_name)\n\n# Define a set of sample images from COCO dataset for demonstration\nimage_urls = [\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000285.jpg\",  # bear\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000632.jpg\",  # train\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000724.jpg\",  # umbrella\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000776.jpg\",  # teddy bear\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000785.jpg\",  # clock\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000802.jpg\",  # train\n]\n\n# Extract embeddings and store images\nembeddings = []\nimages = []\nfor i, url in enumerate(image_urls[:3]):  # Limiting to first 3 images to save time\n    print(f\"Processing image {i+1}/{len(image_urls[:3])}: {url}\")\n    embedding, image = get_image_embedding(url, model, processor)\n    embeddings.append(embedding)\n    images.append(image)\n\n# Convert to numpy array for further processing\nembeddings = np.array(embeddings)\nprint(f\"Embedded {len(embeddings)} images. Embedding shape: {embeddings.shape}\")\n\n# Display the images\nfig, axes = plt.subplots(1, len(images), figsize=(15, 5))\nfor i, (image, ax) in enumerate(zip(images, axes)):\n    ax.imshow(image, aspect='equal')\n    ax.set_title(f\"Image {i+1}\")\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n\n# Text descriptions\ntexts = [\n    \"a wild bear\",\n    \"a train on tracks\",\n    \"a person with an umbrella\",\n    \"a child's toy\",\n    \"a stop sign\",\n    \"a picture of a bedroom\",\n    \"Cozy bedroom retreat filled with books, plants, and warm natural light\",\n    \"a picture of a timepiece\",\n    \"a picture of a vehicle for transportation\"\n]\n\n# Get text embeddings using the processor and model\ndef get_text_embedding(text, model, processor):\n    \"\"\"Extract text embedding from a text string\n    \n    NOTE: For most SigLIP applications, you should NOT extract embeddings separately.\n    Instead, use the model to process image-text pairs together via model(**inputs)\n    to get direct similarity scores through the model's logits_per_image.\n    \n    This function is provided for educational purposes or for specific use cases\n    where you need the raw embeddings.\n    \"\"\"\n    inputs = processor(text=text, return_tensors=\"pt\", padding=True)\n    \n    with torch.no_grad():\n        # Just get text features directly\n        text_embedding = model.get_text_features(**inputs)\n        text_embedding = text_embedding / text_embedding.norm(dim=1, keepdim=True)\n    \n    return text_embedding.squeeze().detach().numpy()\n\n# Get embeddings for the text queries\ntext_embeddings = []\nfor i, query in enumerate(texts):\n    print(f\"Processing text {i+1}/{len(texts)}: '{query}'\")\n    text_embeddings.append(get_text_embedding(query, model, processor))\ntext_embeddings = np.array(text_embeddings)\nprint(f\"Embedded {len(text_embeddings)} text queries. Embedding shape: {text_embeddings.shape}\")\nprint(\"NOTE: While we extracted text embeddings separately, for similarity calculations\")\nprint(\"we'll use the model's native capability to process image-text pairs together\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing image 1/3: https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000285.jpg\nProcessing image 2/3: https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000632.jpg\nProcessing image 3/3: https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000724.jpg\nEmbedded 3 images. Embedding shape: (3, 1152)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](2025-03-16_what_are_image_embeddings_files/figure-html/cell-5-output-2.png){width=1329 height=470}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing text 1/9: 'a wild bear'\nProcessing text 2/9: 'a train on tracks'\nProcessing text 3/9: 'a person with an umbrella'\nProcessing text 4/9: 'a child's toy'\nProcessing text 5/9: 'a stop sign'\nProcessing text 6/9: 'a picture of a bedroom'\nProcessing text 7/9: 'Cozy bedroom retreat filled with books, plants, and warm natural light'\nProcessing text 8/9: 'a picture of a timepiece'\nProcessing text 9/9: 'a picture of a vehicle for transportation'\nEmbedded 9 text queries. Embedding shape: (9, 1152)\nNOTE: While we extracted text embeddings separately, for similarity calculations\nwe'll use the model's native capability to process image-text pairs together\n```\n:::\n:::\n\n\n### Understanding Embeddings: A Closer Look at the Numbers\n\nWhat exactly are these embedding vectors we've been generating? Let's take a closer look at what these numbers actually represent:\n\n#### Anatomy of an Embedding Vector\n\nBoth image and text embeddings in SigLIP 2 are **1152-dimensional vectors** - essentially long lists of 1152 floating-point numbers. Each number typically ranges from -1 to 1 after normalization. These numbers represent:\n\n- **For images**: Abstract visual features like shapes, textures, objects, spatial arrangements, and semantic concepts\n- **For text**: Linguistic features, semantic meanings, and conceptual relationships between words\n\n#### Reading the Numbers\n\nWhen you look at an embedding vector like `[0.1253, -0.0891, 0.0332, ...]`:\n\n- **Each position** (dimension) captures a specific latent feature that the model learned during training\n- **The value** at each position indicates how strongly that feature is present in the image or text\n- **Positive vs. negative values** represent different aspects of the same feature dimension\n- **The magnitude** (absolute value) shows the strength of that feature's presence\n\n#### Pattern Recognition\n\nTwo similar images (like two different bears) will have similar patterns in their embedding vectors because:\n\n- They share many of the same visual features\n- The model has learned to map similar semantic content to similar regions in the embedding space\n\nThis is why a photo of a bear and the text \"a wild bear\" would have some similarities in their embedding patterns, despite being different modalities.\n\n#### Dimensionality\n\nWhy 1152 dimensions? This specific size represents a balance between:\n\n- Being **large enough** to capture complex visual and textual nuances\n- Being **small enough** to be computationally efficient (compared to raw pixels)\n- Following the **architectural decisions** made when designing the ViT (Vision Transformer) backbone\n\nWhen we visualize only the first 10 dimensions below, we're seeing just a tiny slice (less than 1%) of the full representation, but it gives us an intuitive sense of how these embeddings work.\n\n::: {#6beda291 .cell execution_count=5}\n``` {.python .cell-code}\n# Visualizing truncated embeddings to better understand their structure\nprint(\"Displaying truncated embeddings to visualize their structure:\")\n\n# Function to display truncated embedding values\ndef display_truncated_embedding(embedding, title, n_values=10):\n    \"\"\"Format and display a truncated embedding vector\"\"\"\n    truncated = embedding[:n_values]\n    formatted = [f\"{value:.4f}\" for value in truncated]\n    print(f\"\\n{title} embedding (first {n_values} values):\")\n    print(\"[\" + \", \".join(formatted) + \", ...]\")\n    print(f\"Shape: {embedding.shape} (full embedding)\")\n    return truncated\n\n# Visualize the first few values of each image embedding\nprint(\"\\n=== IMAGE EMBEDDINGS ===\")\nfor i, embedding in enumerate(embeddings):\n    display_truncated_embedding(embedding, f\"Image {i+1}\")\n\n# Visualize the first few values of select text embeddings\nprint(\"\\n=== TEXT EMBEDDINGS ===\")\nfor i, text in enumerate(texts[:5]):  # Just show first 5 text embeddings\n    display_truncated_embedding(text_embeddings[i], f\"'{text}'\")\n\n# Create a visual representation of embeddings alongside images\nfig, axes = plt.subplots(len(images), 2, figsize=(12, 4*len(images)), \n                         gridspec_kw={'width_ratios': [1, 2]})\n\nfor i, (image, embedding) in enumerate(zip(images, embeddings)):\n    # Display the image\n    axes[i, 0].imshow(image, aspect='equal')\n    axes[i, 0].set_title(f\"Image {i+1}\")\n    axes[i, 0].axis('off')\n    \n    # Display a truncated embedding as a bar chart\n    truncated = embedding[:10]  # First 10 values\n    axes[i, 1].bar(range(len(truncated)), truncated)\n    axes[i, 1].set_title(f\"Truncated Embedding (first 10 of {len(embedding)} values)\")\n    axes[i, 1].set_xlabel(\"Dimension\")\n    axes[i, 1].set_ylabel(\"Value\")\n    axes[i, 1].set_ylim(-0.5, 0.5)  # Set consistent y limits\n    \n    # Add text annotation\n    embedding_text = \", \".join([f\"{x:.3f}\" for x in truncated[:5]]) + \"...\"\n    axes[i, 1].text(0.5, 0.9, f\"[{embedding_text}]\", \n                   transform=axes[i, 1].transAxes, \n                   ha='center', va='center',\n                   bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n# Also visualize a few text embeddings for comparison\nfig, axes = plt.subplots(3, 1, figsize=(10, 6))\ntext_indices = [0, 1, 2]  # First 3 text embeddings\n\nfor i, idx in enumerate(text_indices):\n    text = texts[idx]\n    embedding = text_embeddings[idx]\n    truncated = embedding[:10]  # First 10 values\n    \n    axes[i].bar(range(len(truncated)), truncated)\n    axes[i].set_title(f\"Text: '{text}'\")\n    axes[i].set_xlabel(\"Dimension\")\n    axes[i].set_ylabel(\"Value\")\n    axes[i].set_ylim(-0.5, 0.5)  # Set consistent y limits\n    \n    # Add text annotation\n    embedding_text = \", \".join([f\"{x:.3f}\" for x in truncated[:5]]) + \"...\"\n    axes[i].text(0.5, 0.9, f\"[{embedding_text}]\", \n                 transform=axes[i].transAxes, \n                 ha='center', va='center',\n                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDisplaying truncated embeddings to visualize their structure:\n\n=== IMAGE EMBEDDINGS ===\n\nImage 1 embedding (first 10 values):\n[-0.0196, -0.0035, -0.0117, 0.0082, 0.0116, 0.0339, 0.0126, -0.0231, -0.0532, 0.0226, ...]\nShape: (1152,) (full embedding)\n\nImage 2 embedding (first 10 values):\n[-0.0001, -0.0121, -0.0136, -0.0283, -0.0190, 0.0025, 0.0138, -0.0315, -0.0365, -0.0170, ...]\nShape: (1152,) (full embedding)\n\nImage 3 embedding (first 10 values):\n[0.0493, -0.0029, 0.0380, 0.0021, -0.0271, 0.0050, -0.0256, -0.0109, -0.0355, 0.0189, ...]\nShape: (1152,) (full embedding)\n\n=== TEXT EMBEDDINGS ===\n\n'a wild bear' embedding (first 10 values):\n[-0.0010, 0.0143, 0.0112, 0.0271, -0.0025, 0.0073, 0.0091, -0.5672, -0.0343, 0.0279, ...]\nShape: (1152,) (full embedding)\n\n'a train on tracks' embedding (first 10 values):\n[-0.0050, 0.0231, 0.0155, 0.0137, -0.0108, 0.0024, 0.0228, -0.5232, -0.0480, 0.0492, ...]\nShape: (1152,) (full embedding)\n\n'a person with an umbrella' embedding (first 10 values):\n[-0.0078, 0.0360, 0.0230, -0.0247, 0.0002, 0.0237, 0.0287, -0.4820, -0.0380, 0.0248, ...]\nShape: (1152,) (full embedding)\n\n'a child's toy' embedding (first 10 values):\n[0.0053, 0.0187, 0.0033, -0.0016, -0.0208, 0.0209, 0.0297, -0.5040, -0.0459, 0.0216, ...]\nShape: (1152,) (full embedding)\n\n'a stop sign' embedding (first 10 values):\n[0.0159, 0.0036, 0.0119, 0.0171, -0.0232, -0.0025, 0.0078, -0.5381, -0.0299, 0.0398, ...]\nShape: (1152,) (full embedding)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](2025-03-16_what_are_image_embeddings_files/figure-html/cell-6-output-2.png){width=1142 height=1140}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2025-03-16_what_are_image_embeddings_files/figure-html/cell-6-output-3.png){width=949 height=564}\n:::\n:::\n\n\n### Interpreting the Embedding Visualizations\n\nLooking at the truncated embedding visualizations above, we can make several important observations:\n\n#### What We're Seeing\n\nThe bar charts show the first 10 dimensions of embedding vectors that are actually 1152 dimensions long. Think of these as the first few \"notes\" in a much longer \"melody\" that represents each image or text.\n\n#### Image Embedding Patterns\n\nIn the image embeddings above:\n\n1. **Different images have different patterns** - Notice how the bear image has a different pattern of positive and negative values compared to the room or stop sign\n\n2. **Magnitude variations** - Some dimensions have larger values than others, indicating their importance in representing the image\n\n3. **Sign patterns** - The pattern of positive and negative values across dimensions forms a unique \"signature\" for each image\n\n#### Text Embedding Patterns\n\nFor the text embeddings:\n\n1. **Semantic encoding** - Each text query (\"a wild bear\", \"a train on tracks\", etc.) produces a unique pattern reflecting its semantic meaning\n\n2. **Comparable with images** - These text embeddings live in the same 1152-dimensional space as the image embeddings, which is what allows the model to compare them directly\n\n3. **Different signature** - The text \"a wild bear\" has a different pattern from the bear image, but they share enough similarities to have high similarity scores\n\n#### The Full Picture\n\nRemember that what we're seeing is just the first 10 dimensions of 1152. The full power of these embeddings comes from the complex patterns across all dimensions working together. The model has learned to encode similar concepts (whether in image or text form) into similar regions of this high-dimensional space.\n\nWhen computing similarity, all 1152 dimensions are compared, not just these first few that we're visualizing. This is why two vectors that might look different in their first 10 dimensions could still be considered similar when all dimensions are considered.\n\n::: {#bfd3a02d .cell execution_count=6}\n``` {.python .cell-code}\n# Compute similarity between our images and texts\n# Instead of computing dot product manually, let's use the model's built-in functionality\n\n# Create a function to compute similarity between images and texts using the model directly\ndef compute_image_text_similarity(images, texts, model, processor):\n    \"\"\"Compute similarity between images and texts using the model's native capabilities\"\"\"\n    similarity_matrix = np.zeros((len(images), len(texts)))\n    \n    for i, image in enumerate(images):\n        # Process each image with all text descriptions\n        inputs = processor(\n            text=texts, \n            images=image, \n            return_tensors=\"pt\", \n            padding=\"max_length\", \n            max_length=64\n        )\n        \n        with torch.no_grad():\n            outputs = model(**inputs)\n            # The model directly computes logits_per_image which represents similarity\n            logits = outputs.logits_per_image\n            # Convert to probabilities\n            probs = torch.sigmoid(logits)\n            \n            # Store the similarity scores for this image\n            similarity_matrix[i] = probs[0].detach().numpy()\n    \n    return similarity_matrix\n\n# Compute similarity using the model's native capabilities\nprint(\"Computing image-text similarity using the model's built-in functionality...\")\nsimilarity_matrix = compute_image_text_similarity(images, texts, model, processor)\nprint(\"Similarity computation complete.\")\n\n# Display similarity matrix\nplt.figure(figsize=(10, 8))\nplt.imshow(similarity_matrix, vmin=0, vmax=1, cmap='viridis')\nplt.colorbar(label='Similarity Score')\nplt.xticks(np.arange(len(texts)), texts, rotation=45, ha='right')\nplt.yticks(np.arange(len(images)), [f\"Image {i+1}\" for i in range(len(images))])\nplt.title('Image-Text Similarity Matrix')\n\n# Add text annotations with the score values\nfor i in range(len(images)):\n    for j in range(len(texts)):\n        plt.text(j, i, f'{similarity_matrix[i, j]:.2f}', \n                 ha='center', va='center', \n                 color='white' if similarity_matrix[i, j] < 0.5 else 'black')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nComputing image-text similarity using the model's built-in functionality...\nSimilarity computation complete.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](2025-03-16_what_are_image_embeddings_files/figure-html/cell-7-output-2.png){width=898 height=779}\n:::\n:::\n\n\n### Connecting Images to Meaning: How Embeddings Enable Cross-Modal Understanding\n\nLooking at the similarity matrix above, we can now understand how the embedding vectors we visualized earlier enable the model to connect images with text:\n\n#### From Numbers to Matching\n\n1. **The bear image (Image 1)** shows highest similarity with \"a wild bear\" text. Looking back at their embedding visualizations, while they don't look identical in the first 10 dimensions, the complete 1152-dimensional pattern contains enough similarity for the model to make this connection.\n\n2. **Similar concepts, similar embeddings** - When we see a high similarity score (like between the bear image and bear text), it means their complete embedding vectors are pointing in similar directions in the 1152-dimensional space, even if the individual values aren't identical.\n\n3. **Embedding space geometry** - You can think of each embedding as a point in a 1152-dimensional space. Similar concepts (whether images or text) are positioned closer together in this space.\n\n#### The Magic of Shared Embedding Space\n\nWhat makes these embeddings so powerful is that both images and text are mapped to the same embedding space. This means:\n\n- The bear image and the text \"a wild bear\" produce vectors that point in similar directions\n- The bedroom image and text about bedrooms create vectors in another region of the space\n- The stop sign image and text about stop signs cluster in yet another region\n\nIt's as if the model has created a giant 1152-dimensional map where similar concepts are placed near each other, regardless of whether they come from images or text.\n\n#### From Individual Values to Overall Meaning\n\nLooking at individual embedding values (like `0.1253` or `-0.0891`) doesn't tell us much on its own. It's the pattern across all dimensions that matters. Each dimension might represent complex features like:\n\n- \"Furry texture\" (potentially high in the bear image)\n- \"Red color\" (potentially high in the stop sign image)\n- \"Indoor setting\" (potentially high in the bedroom image)\n- \"Natural environment\" (potentially high in the bear image)\n\nBut these features aren't explicitly defined - they emerge organically during training as the model learns to map similar concepts to similar embedding regions.\n\nThis is why image embeddings are so powerful: they transform pixels into semantic representations that can be directly compared with text, enabling applications like image search, classification, and multimodal understanding.\n\n## Example 3: Visualizing Embeddings with Clustering\n\nLet's use clustering to group our images based on their semantic content. For a more meaningful analysis, we'll use a larger set of images from the COCO dataset and visualize them using UMAP before clustering.\n\n::: {#88326522 .cell execution_count=7}\n``` {.python .cell-code}\n# Import additional libraries for enhanced visualization\nfrom umap import UMAP\nimport matplotlib.pyplot as plt\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\nimport numpy as np\nfrom tqdm.notebook import tqdm\n\n# Define a larger set of sample images from COCO dataset\ncoco_image_urls = [\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000285.jpg\",  # bear\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000632.jpg\",  # train\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000724.jpg\",  # umbrella\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000776.jpg\",  # teddy bear\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000785.jpg\",  # clock\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000802.jpg\",  # train\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000872.jpg\",  # person with umbrella\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000885.jpg\",  # dining table\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000934.jpg\",  # person\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001000.jpg\",  # zebra\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001296.jpg\",  # sheep\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001425.jpg\",  # airplane\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001490.jpg\",  # giraffe\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001503.jpg\",  # bird\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001532.jpg\",  # dog\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001584.jpg\",  # boat\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001675.jpg\",  # person on bike\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001761.jpg\",  # cat\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001818.jpg\",  # horse\n    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000002153.jpg\",  # car\n]\n\n# Extract embeddings for all images\nprint(\"Extracting embeddings for all images...\")\nlarge_embeddings = []\nlarge_images = []\n\nfor i, url in enumerate(tqdm(coco_image_urls)):\n    try:\n        embedding, image = get_image_embedding(url, model, processor)\n        large_embeddings.append(embedding)\n        large_images.append(image)\n    except Exception as e:\n        print(f\"Error processing image {i+1}: {e}\")\n\n# Convert to numpy array\nlarge_embeddings = np.array(large_embeddings)\nprint(f\"Successfully embedded {len(large_embeddings)} images. Embedding shape: {large_embeddings.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExtracting embeddings for all images...\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"433dedc611954c79bf79ff08324105c5\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nError processing image 9: cannot identify image file <_io.BytesIO object at 0x382d00c70>\nSuccessfully embedded 19 images. Embedding shape: (19, 1152)\n```\n:::\n:::\n\n\n### Visualizing High-Dimensional Embeddings with UMAP\n\nUniform Manifold Approximation and Projection (UMAP)[^24] is a dimensionality reduction technique that helps us visualize high-dimensional embeddings in 2D space while preserving their local and global structure. Unlike simpler methods like PCA, UMAP can capture non-linear relationships in the data, making it ideal for visualizing complex embedding spaces.\n\n::: {#cb0a796f .cell execution_count=8}\n``` {.python .cell-code}\n# Apply UMAP for dimensionality reduction to visualize embeddings in 2D\nprint(\"Applying UMAP dimensionality reduction...\")\numap_model = UMAP(n_components=2, n_neighbors=5, min_dist=0.1, metric='cosine', random_state=42)  # Using UMAP algorithm for dimensionality reduction\numap_embeddings = umap_model.fit_transform(large_embeddings)\n\n# Function to plot images on UMAP projection\ndef plot_images_on_umap(embeddings_2d, images, figsize=(12, 10), image_zoom=0.7):\n    \"\"\"Plot images on a 2D projection (like UMAP or t-SNE)\"\"\"\n    fig, ax = plt.subplots(figsize=figsize)\n    \n    # First scatter the points to see the overall distribution\n    ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5, s=10)\n    \n    # Determine the data bounds\n    x_min, x_max = embeddings_2d[:, 0].min(), embeddings_2d[:, 0].max()\n    y_min, y_max = embeddings_2d[:, 1].min(), embeddings_2d[:, 1].max()\n    \n    # Calculate padding to ensure square aspect ratio\n    x_range = x_max - x_min\n    y_range = y_max - y_min\n    max_range = max(x_range, y_range) * 1.1  # Add 10% padding\n    \n    x_mid = (x_min + x_max) / 2\n    y_mid = (y_min + y_max) / 2\n    \n    # Set equal aspect ratio for the plot\n    ax.set_aspect('equal')\n    \n    # Set limits to ensure square aspect ratio\n    ax.set_xlim(x_mid - max_range/2, x_mid + max_range/2)\n    ax.set_ylim(y_mid - max_range/2, y_mid + max_range/2)\n    \n    # Then plot small versions of each image at its 2D location\n    for i, (x, y) in enumerate(embeddings_2d):\n        img = images[i]\n        # Preserve aspect ratio when resizing\n        width, height = img.size\n        # Calculate new dimensions while maintaining aspect ratio\n        if width > height:\n            new_width = int(width * image_zoom)\n            new_height = int(height * (new_width / width))\n        else:\n            new_height = int(height * image_zoom)\n            new_width = int(width * (new_height / height))\n            \n        try:\n            # Use LANCZOS for better quality, fall back to other methods if not available\n            img = img.resize((new_width, new_height), Image.LANCZOS)\n        except AttributeError:\n            # For newer Pillow versions where LANCZOS might be removed\n            img = img.resize((new_width, new_height), Image.BICUBIC)\n        \n        # Convert PIL image to a format matplotlib can use\n        # Increase the zoom parameter to make images larger\n        img_box = OffsetImage(img, zoom=0.15)\n        ab = AnnotationBbox(img_box, (x, y), frameon=True, pad=0.1)\n        ax.add_artist(ab)\n    \n    plt.title(\"UMAP Projection of Image Embeddings\")\n    plt.tight_layout()\n    return fig, ax\n\n# Visualize the UMAP embedding\nprint(\"Visualizing UMAP projection with images...\")\nfig, ax = plot_images_on_umap(umap_embeddings, large_images)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nApplying UMAP dimensionality reduction...\nVisualizing UMAP projection with images...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/opt/anaconda3/envs/quarto-python/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/opt/anaconda3/envs/quarto-python/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](2025-03-16_what_are_image_embeddings_files/figure-html/cell-9-output-3.png){width=946 height=949}\n:::\n:::\n\n\n### Using K-means Clustering on Embeddings\n\nNow that we've visualized our embeddings in 2D space, let's use K-means clustering[^25] to identify groups of semantically similar images. K-means is an unsupervised learning algorithm that groups data points with similar features together based on their Euclidean distance in the embedding space.\n\n::: {#40f468fd .cell execution_count=9}\n``` {.python .cell-code}\n# Apply K-means clustering on the original high-dimensional embeddings\nn_clusters = 5  # Increase the number of clusters for a more nuanced analysis\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nclusters = kmeans.fit_predict(large_embeddings)\n\n# Visualize clustering results on the UMAP projection\nplt.figure(figsize=(12, 10))\nscatter = plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], \n                     c=clusters, cmap='viridis', s=100, alpha=0.8)\n\n# Determine the data bounds\nx_min, x_max = umap_embeddings[:, 0].min(), umap_embeddings[:, 0].max()\ny_min, y_max = umap_embeddings[:, 1].min(), umap_embeddings[:, 1].max()\n\n# Calculate padding to ensure square aspect ratio\nx_range = x_max - x_min\ny_range = y_max - y_min\nmax_range = max(x_range, y_range) * 1.1  # Add 10% padding\n\nx_mid = (x_min + x_max) / 2\ny_mid = (y_min + y_max) / 2\n\n# Set equal aspect ratio for the plot\nplt.gca().set_aspect('equal')\n\n# Set limits to ensure square aspect ratio\nplt.xlim(x_mid - max_range/2, x_mid + max_range/2)\nplt.ylim(y_mid - max_range/2, y_mid + max_range/2)\n\nplt.colorbar(scatter, label='Cluster')\nplt.title(f'UMAP Projection with K-means Clustering (k={n_clusters})')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2025-03-16_what_are_image_embeddings_files/figure-html/cell-10-output-1.png){width=1074 height=945}\n:::\n:::\n\n\n[^24]: McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform manifold approximation and projection for dimension reduction. *arXiv preprint arXiv:1802.03426*. [arXiv:1802.03426](https://arxiv.org/abs/1802.03426)\n\n[^25]: Lloyd, S. (1982). Least squares quantization in PCM. *IEEE Transactions on Information Theory*, 28(2), 129-137. https://doi.org/10.1109/TIT.1982.1056489\n\n### Visualizing Images by Cluster\n\nLet's visualize the actual images in each cluster to see what semantic groupings the model has identified.\n\n::: {#bd99b3da .cell execution_count=10}\n``` {.python .cell-code}\n# Display images by cluster\nfor cluster_id in range(n_clusters):\n    # Get indices of images in this cluster\n    cluster_indices = np.where(clusters == cluster_id)[0]\n    n_images_in_cluster = len(cluster_indices)\n    \n    if n_images_in_cluster > 0:\n        # Calculate grid layout dimensions\n        grid_cols = min(5, n_images_in_cluster)\n        grid_rows = (n_images_in_cluster + grid_cols - 1) // grid_cols\n        \n        fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(grid_cols * 3, grid_rows * 3))\n        plt.suptitle(f'Cluster {cluster_id+1}: {n_images_in_cluster} Images')\n        \n        # Flatten axes array for easy iteration\n        if grid_rows == 1 and grid_cols == 1:\n            axes = np.array([axes])\n        elif grid_rows == 1 or grid_cols == 1:\n            axes = axes.flatten()\n            \n        # Plot each image in the cluster\n        for i, idx in enumerate(cluster_indices):\n            if i < len(axes):\n                row, col = i // grid_cols, i % grid_cols\n                if grid_rows == 1 and grid_cols == 1:\n                    ax = axes[0]\n                elif grid_rows == 1 or grid_cols == 1:\n                    ax = axes[i]\n                else:\n                    ax = axes[row, col]\n                    \n                ax.imshow(large_images[idx], aspect='equal')\n                ax.set_title(f\"Image {idx+1}\")\n                ax.axis('off')\n        \n        # Hide unused subplots\n        for i in range(n_images_in_cluster, grid_rows * grid_cols):\n            row, col = i // grid_cols, i % grid_cols\n            if grid_rows == 1 and grid_cols == 1:\n                pass  # No unused subplots in a 1x1 grid\n            elif grid_rows == 1 or grid_cols == 1:\n                if i < len(axes):\n                    axes[i].axis('off')\n            else:\n                if row < grid_rows and col < grid_cols:\n                    axes[row, col].axis('off')\n                \n        plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust for the suptitle\n        plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2025-03-16_what_are_image_embeddings_files/figure-html/cell-11-output-1.png){width=1426 height=568}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2025-03-16_what_are_image_embeddings_files/figure-html/cell-11-output-2.png){width=494 height=287}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2025-03-16_what_are_image_embeddings_files/figure-html/cell-11-output-3.png){width=1097 height=287}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2025-03-16_what_are_image_embeddings_files/figure-html/cell-11-output-4.png){width=449 height=287}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2025-03-16_what_are_image_embeddings_files/figure-html/cell-11-output-5.png){width=1419 height=287}\n:::\n:::\n\n\n### Analysis of Semantic Clustering\n\nThe clusters formed above demonstrate how SigLIP 2's embeddings group images based on semantic content rather than just visual similarity. This type of semantic clustering is valuable for:\n\n1. **Content organization**: Automatically categorizing large collections of images\n2. **Recommendation systems**: Finding semantically related content\n3. **Anomaly detection**: Identifying images that don't fit expected semantic patterns\n4. **Dataset exploration**: Understanding the distribution of semantic concepts\n\nThe UMAP visualization provides insight into how the high-dimensional embedding space is organized, while K-means clustering identifies discrete groups within this space. Together, they offer a powerful way to explore and understand the semantic relationships captured by SigLIP 2's image embeddings.\n\n## Conclusion\n\nIn this notebook, we've explored the concept of image embeddings and specifically delved into SigLIP 2, Google's advanced multilingual vision-language encoder. We've seen how image embeddings work, the technical evolution from CLIP to SigLIP to SigLIP 2, and the key capabilities that make SigLIP 2 stand out.\n\nThrough practical examples, we've demonstrated:\n\n1. How to perform zero-shot image classification\n2. How to compute image-text similarity\n3. How to visualize and cluster embeddings\n4. How to extract image embeddings for downstream tasks\n5. How to compute image-to-image similarity\n6. How to build a simple image search engine\n\nImage embeddings like those produced by SigLIP 2 are foundational to modern computer vision applications, enabling efficient search, classification, and multimodal understanding. As models continue to evolve, we can expect even more powerful and versatile embeddings that further bridge the gap between vision and language understanding.\n\nThe flexible architecture and variant options make SigLIP 2 adaptable to a wide range of applications, from resource-constrained edge devices to high-performance systems requiring maximum accuracy. By understanding these tradeoffs, you can select the most appropriate SigLIP 2 variant for your specific use case, whether you prioritize efficiency, accuracy, or specialized capabilities like document understanding.\n\nThe multilingual capabilities and enhanced training methodology of SigLIP 2 make it particularly valuable for building more inclusive and accurate AI systems that can understand visual content across different languages and cultures.\n\n## Conclusion: The Power and Versatility of Image Embeddings\n\nIn this notebook, we've explored the concept of image embeddings with a focus on SigLIP 2, Google's advanced multilingual vision-language encoder. We've seen how these sophisticated representations go far beyond simple vector spaces, incorporating advanced mechanisms that significantly enhance their utility.\n\n### Key Takeaways\n\n1. **Advanced Similarity Computation**: SigLIP 2 doesn't just rely on simple cosine similarity between embeddings. It incorporates:\n   - MAP head pooling for better representation aggregation\n   - Temperature scaling to control similarity sharpness\n   - Bias terms to adjust for training imbalances\n   - Sigmoid activation to convert similarities to probabilities\n\n2. **Powerful Applications**: These sophisticated embeddings enable a wide range of applications:\n   - Visualization and exploration through clustering\n   - Unsupervised grouping based on semantic content\n   - Cross-modal understanding between images and text\n   - Semantic search engines with high precision\n   - Fine-grained recognition of subtle differences and similarities\n\n3. **Proper Usage**: As we've demonstrated, to get the most out of SigLIP 2, it's crucial to use the model's built-in similarity calculation mechanisms rather than trying to manually compute cosine similarity on raw embeddings.\n\nThe quality of SigLIP 2's embeddings makes these applications more accurate and robust than ever before. Its multilingual capabilities and improved semantic understanding make it particularly valuable for diverse global applications.\n\nAs image embedding models continue to evolve, we can expect even more powerful capabilities that further bridge the gap between visual content and natural language understanding. These embeddings form the foundation of modern computer vision systems and are becoming increasingly important in multimodal AI applications that combine vision, language, and other modalities.\n\nWhether you're building a visual search engine, a content recommendation system, or a multimodal understanding application, image embeddings like those produced by SigLIP 2 provide a solid foundation for bringing semantic understanding to your visual data—just be sure to leverage their full capabilities by using the model's built-in similarity mechanisms!\n\n### Important Note on Processing Image-Text Pairs\n\nAn important detail when working with vision-language models like SigLIP is understanding how to properly compute similarity between images and text.\n\n#### The Proper Way: Process Image-Text Pairs Together\n\nWhile it's possible to extract image and text embeddings separately (as we did in some examples for educational purposes), the proper way to compute image-text similarity is to use the model's native capability to process image-text pairs together:\n\n```python\n# The right way to compute image-text similarity with vision-language models\ninputs = processor(text=texts, images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits_per_image  # Direct similarity scores\nprobabilities = torch.sigmoid(logits)  # Convert to probabilities\n```\n\n#### Why This Matters\n\nVision-language models like SigLIP are specifically trained to compute similarity between image-text pairs in a particular way. When we extract embeddings separately and then compute similarity using dot products, we're not fully leveraging the model's capabilities.\n\nThe model's native `logits_per_image` output includes any internal transformations, normalization, or calibration that the model has learned during training. This leads to more accurate similarity scores compared to taking embeddings separately and computing similarity manually[^22].\n\n#### When to Use Direct Embeddings\n\nThere are still valid use cases for extracting embeddings directly:\n\n1. **Image-to-image similarity**: When comparing within the same modality\n2. **Building search indices**: For efficient retrieval systems\n3. **Transfer learning**: Using the embeddings as input features for downstream tasks\n\nHowever, for direct image-text similarity comparisons, always prefer the model's built-in methods for processing the pairs together[^23].\n\n[^22]: Hugging Face. (2024). Zero-shot Image Classification with Transformers. https://huggingface.co/docs/transformers/en/tasks/zero_shot_image_classification\n\n[^23]: Pinecone. (2024). Zero-shot Image Classification with CLIP. https://www.pinecone.io/learn/series/image-search/zero-shot-image-classification-clip/\n\n## References\n\n1. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G. (2021). Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (pp. 8748-8763). PMLR. [arXiv:2103.00020](https://arxiv.org/abs/2103.00020)\n\n2. Zhai, X., Wang, X., Mustafa, B., Steiner, A., Keysers, D., Kolesnikov, A., & Beyer, L. (2023). Sigmoid loss for language image pre-training. In Proceedings of the 40th International Conference on Machine Learning (pp. 40844-40858). PMLR. [arXiv:2303.15343](https://arxiv.org/abs/2303.15343)\n\n3. Beyer, L., Dehghani, M., et al. (2024). SigLIP 2: Next-Generation Multilingual Vision-Language Models. Google Research. [arXiv:2409.01936](https://arxiv.org/abs/2409.01936)\n\n4. Google Research. (2024). SigLIP 2: Multilingual vision-language models with self-supervised learning. GitHub. [Repository](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/README_siglip2.md)\n\n5. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ... & Rush, A. M. (2020). Transformers: State-of-the-art natural language processing. *In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations* (pp. 38-45). [ACL Anthology](https://aclanthology.org/2020.emnlp-demos.6/)\n\n6. McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform manifold approximation and projection for dimension reduction. *arXiv preprint arXiv:1802.03426*. [arXiv:1802.03426](https://arxiv.org/abs/1802.03426)\n\n7. Google. (2024). SigLIP 2 SO400M Patch14-384 Model. Hugging Face. [Model Card](https://huggingface.co/google/siglip2-so400m-patch14-384)\n\n8. Hugging Face. (2024). Zero-Shot Image Classification with Transformers. [Documentation](https://huggingface.co/docs/transformers/en/tasks/zero_shot_image_classification)\n\n9. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. *In Advances in Neural Information Processing Systems* (pp. 5998-6008). [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n\n10. Bengio, Y., Courville, A., & Vincent, P. (2013). Representation learning: A review and new perspectives. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 35(8), 1798-1828. [IEEE](https://doi.org/10.1109/TPAMI.2013.50)\n\n11. Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. *Advances in Neural Information Processing Systems*, 25. [NeurIPS](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n\n12. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. *In International Conference on Learning Representations*. [arXiv:2010.11929](https://arxiv.org/abs/2010.11929)\n\n13. Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020). A simple framework for contrastive learning of visual representations. *International Conference on Machine Learning*, 1597-1607. [arXiv:2002.05709](https://arxiv.org/abs/2002.05709)\n\n14. Johnson, J., Douze, M., & Jégou, H. (2019). Billion-scale similarity search with GPUs. *IEEE Transactions on Big Data*, 7(3), 535-547. [IEEE](https://doi.org/10.1109/TBDATA.2019.2921572)\n\n15. Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*. [arXiv:1503.02531](https://arxiv.org/abs/1503.02531)\n\n",
    "supporting": [
      "2025-03-16_what_are_image_embeddings_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script src=\"https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js\" crossorigin=\"anonymous\"></script>\n"
      ],
      "include-after-body": [
        "<script type=application/vnd.jupyter.widget-state+json>\n{\"state\":{\"083586e17c2c4003bd8a19137f7eafa6\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_80f15810df4846589317d8b0334e3f94\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_b930599ac26247f8b6f63f0dea183b56\",\"tabbable\":null,\"tooltip\":null,\"value\":\" 20/20 [00:15&lt;00:00,  1.33it/s]\"}},\"166190b78f7945d7b1304cce6ab0339b\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"195a67df29bf4e13a6688a4222e7a5e0\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"success\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_3108b26ebbe6403ba36c152579d3a8c9\",\"max\":20,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_c80b0271416f4595a79b0218c0434847\",\"tabbable\":null,\"tooltip\":null,\"value\":20}},\"2b364e5674df4d57a335ea29938d28cf\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"3108b26ebbe6403ba36c152579d3a8c9\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"34c35ef8fc2f43b5a2bdc5f31bf295a8\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"36ac3db22ca74dedb0c7aea1e40732bc\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"success\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_4631809c1a2d4ee5886d961a96ad5873\",\"max\":20,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_c57f492b8e67430599f02d25c9d38b6b\",\"tabbable\":null,\"tooltip\":null,\"value\":20}},\"39fc2fbe11da450e9b18eaff76c61f43\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"3b102ca02a20476db3184967cc3010ce\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"3f8e3df1091345279ecf840cbcebde25\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"4028f25812d84e4cb6cea3385043295b\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"433dedc611954c79bf79ff08324105c5\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_67ced7fe756746efbdbbf0e70f469afb\",\"IPY_MODEL_c3b894fa964e45269bb9c7cba5986a15\",\"IPY_MODEL_fb7e16546f2f45878c142a41e6217eff\"],\"layout\":\"IPY_MODEL_eeb5629fa37b4ecb97e90fc8072e67db\",\"tabbable\":null,\"tooltip\":null}},\"4631809c1a2d4ee5886d961a96ad5873\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"4698f682f0f04d6199467a49aa64a0fe\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"46dddfb2cf0546be8c4be74b68897b34\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"success\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_4028f25812d84e4cb6cea3385043295b\",\"max\":20,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_f4601398131343ef8e1404bd3cba46c6\",\"tabbable\":null,\"tooltip\":null,\"value\":20}},\"4de0edec722b418185fdfe044819f1bd\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_2b364e5674df4d57a335ea29938d28cf\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_39fc2fbe11da450e9b18eaff76c61f43\",\"tabbable\":null,\"tooltip\":null,\"value\":\"100%\"}},\"4ed35cd467554aa39e3c7c83ba076d94\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"4fbd7cabdc4c4dd6b098560c3936ee17\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"5ad9f38cff244728a4608e87798fa2bf\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"5ea2d1c1226046f7a8c2b532f951c607\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"5f64a278ac0041e6b702d3885414b84e\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"67ced7fe756746efbdbbf0e70f469afb\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_4ed35cd467554aa39e3c7c83ba076d94\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_ede9faede9ab41c7b9d44ea96f95416c\",\"tabbable\":null,\"tooltip\":null,\"value\":\"100%\"}},\"6a3922da7c574b6288723e2624b3c1a1\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"6a396b735c1c45b397124e9f643d8ada\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_4de0edec722b418185fdfe044819f1bd\",\"IPY_MODEL_195a67df29bf4e13a6688a4222e7a5e0\",\"IPY_MODEL_ffcd0356f2cf40019cc78d5cccd5cbab\"],\"layout\":\"IPY_MODEL_5ad9f38cff244728a4608e87798fa2bf\",\"tabbable\":null,\"tooltip\":null}},\"73c9d618e41e43f4b61056565455311f\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"7c2f70b1cc384263be3b82e5ca39d0b8\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_814c88f4b42a4e4e8ea61161f7203556\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_3b102ca02a20476db3184967cc3010ce\",\"tabbable\":null,\"tooltip\":null,\"value\":\"100%\"}},\"80f15810df4846589317d8b0334e3f94\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"814c88f4b42a4e4e8ea61161f7203556\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"8d929a75c7024f16819dd28ee892820f\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"9e1d18e16a6d4550981d2b30b7e56121\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_7c2f70b1cc384263be3b82e5ca39d0b8\",\"IPY_MODEL_36ac3db22ca74dedb0c7aea1e40732bc\",\"IPY_MODEL_b37989446ba04cb99d9192ac0a592510\"],\"layout\":\"IPY_MODEL_faf1e9c0394c460cb678afd8ff4860e4\",\"tabbable\":null,\"tooltip\":null}},\"a5c73056da014cf5b6645988205e0895\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"aa364eaaf1154f058f09ef0fe3e988bc\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"b21281bed97143a699d6c99761677e9f\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"b37989446ba04cb99d9192ac0a592510\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_3f8e3df1091345279ecf840cbcebde25\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_4698f682f0f04d6199467a49aa64a0fe\",\"tabbable\":null,\"tooltip\":null,\"value\":\" 20/20 [00:16&lt;00:00,  1.24it/s]\"}},\"b930599ac26247f8b6f63f0dea183b56\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"be1554cbe79f423d922b1ce1773a371d\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"c3b894fa964e45269bb9c7cba5986a15\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"success\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_b21281bed97143a699d6c99761677e9f\",\"max\":20,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_4fbd7cabdc4c4dd6b098560c3936ee17\",\"tabbable\":null,\"tooltip\":null,\"value\":20}},\"c57f492b8e67430599f02d25c9d38b6b\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"c80b0271416f4595a79b0218c0434847\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"ca3e1483f82947b2965acec9b13f8b2e\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"ce18ce82fb7a4129ba0dc4ed948c8ed8\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_166190b78f7945d7b1304cce6ab0339b\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_5ea2d1c1226046f7a8c2b532f951c607\",\"tabbable\":null,\"tooltip\":null,\"value\":\" 20/20 [00:15&lt;00:00,  1.36it/s]\"}},\"ce2a733200904a0d8ec2370dda0a8334\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_8d929a75c7024f16819dd28ee892820f\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_aa364eaaf1154f058f09ef0fe3e988bc\",\"tabbable\":null,\"tooltip\":null,\"value\":\"100%\"}},\"d26b113ee77e415da61d7fdf3aa8ef5b\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"e1af80f4bc4c488da12502f7ecf116d3\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"e3c6b1fec12f41ceafa4224c1f818d19\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"e45bc6fa7aa1413eac9918587a168faf\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_e7f32b29a2e647edb0e90378c8f955fd\",\"IPY_MODEL_ed3931d3c0914c34b1b52f822ec10006\",\"IPY_MODEL_ce18ce82fb7a4129ba0dc4ed948c8ed8\"],\"layout\":\"IPY_MODEL_be1554cbe79f423d922b1ce1773a371d\",\"tabbable\":null,\"tooltip\":null}},\"e7f32b29a2e647edb0e90378c8f955fd\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_6a3922da7c574b6288723e2624b3c1a1\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_e3c6b1fec12f41ceafa4224c1f818d19\",\"tabbable\":null,\"tooltip\":null,\"value\":\"100%\"}},\"ed3931d3c0914c34b1b52f822ec10006\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"success\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_a5c73056da014cf5b6645988205e0895\",\"max\":20,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_d26b113ee77e415da61d7fdf3aa8ef5b\",\"tabbable\":null,\"tooltip\":null,\"value\":20}},\"ede9faede9ab41c7b9d44ea96f95416c\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"ee75274bac1e4457a1b52e46c4b977af\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_ce2a733200904a0d8ec2370dda0a8334\",\"IPY_MODEL_46dddfb2cf0546be8c4be74b68897b34\",\"IPY_MODEL_083586e17c2c4003bd8a19137f7eafa6\"],\"layout\":\"IPY_MODEL_73c9d618e41e43f4b61056565455311f\",\"tabbable\":null,\"tooltip\":null}},\"eeb5629fa37b4ecb97e90fc8072e67db\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"f4601398131343ef8e1404bd3cba46c6\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"faf1e9c0394c460cb678afd8ff4860e4\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"fb7e16546f2f45878c142a41e6217eff\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_34c35ef8fc2f43b5a2bdc5f31bf295a8\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_ca3e1483f82947b2965acec9b13f8b2e\",\"tabbable\":null,\"tooltip\":null,\"value\":\" 20/20 [00:14&lt;00:00,  1.33it/s]\"}},\"ffcd0356f2cf40019cc78d5cccd5cbab\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_5f64a278ac0041e6b702d3885414b84e\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_e1af80f4bc4c488da12502f7ecf116d3\",\"tabbable\":null,\"tooltip\":null,\"value\":\" 20/20 [00:14&lt;00:00,  1.30it/s]\"}}},\"version_major\":2,\"version_minor\":0}\n</script>\n"
      ]
    }
  }
}