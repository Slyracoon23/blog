{
  "hash": "d5543e48f9f872fd41e6f9dbd84b2fa4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\naliases: [\"/gemini-image-segmentation/\"]\ncategories: [\"Computer Vision\", \"Machine Learning\", \"Gemini API\"]\ndate: \"2025-04-27\"\nimage: \"/images/gemini_image_segmentation/thumbnail.png\"\ntitle: \"2D Spatial Understanding with Gemini 2.5\"\nsubtitle: \"Object detection, bounding boxes, and segmentation with Gemini 2.5 API\"\ncolab: '<a href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb\"><img src=\"images/colab.svg\" alt=\"Open In Colab\"></a>'\nformat: \"html\"\n---\n\n\n::: {.callout-note}\n## Disclaimer\nThis is a copy of the AI Jupyter post from [this Colab notebook](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb#scrollTo=906e07f6e562). All rights and credit go to the original author.\n:::\n\nThis notebook explores Gemini 2.5's spatial understanding capabilities, including object detection, bounding boxes, and segmentation. Building on the [Spatial understanding example](https://aistudio.google.com/starter-apps/spatial) from AI Studio, we'll demonstrate how to use the Gemini API to detect objects in images, draw bounding boxes, and generate segmentation masks.\n\n> **Note:** The complete code for this article is available in this [Colab notebook](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb).\n\n## Introduction\n\nThe ability to understand spatial relationships and identify objects in images is a fundamental aspect of computer vision. Gemini 2.0, with its multimodal capabilities, excels at this task without requiring specialized computer vision training or object detection models. Using Gemini's API, you can:\n\n- **Detect objects** and draw bounding boxes around them\n- **Search for specific items** within an image\n- **Label objects** in multiple languages\n- **Apply the model's reasoning abilities** to understand spatial relationships\n- **Generate segmentation masks** for precise object boundaries (with Gemini 2.5)\n\nIn this post, we'll explore how to implement these capabilities using the Google Gen AI SDK, with practical examples for each use case.\n\n## Setting Up the Environment\n\nBefore we dive into the examples, let's set up our environment by installing the required packages, configuring the API key, and initializing the client.\n\n::: {#320c02bb .cell execution_count=1}\n``` {.python .cell-code}\n# Install the Google Gen AI SDK\n!pip install -U -q google-genai\n\n# Import the necessary libraries\nimport os\nimport requests\nimport io\nimport json\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont, ImageColor\nimport torch\nfrom dotenv import load_dotenv\n\n\n\nload_dotenv(dotenv_path='/Users/earlpotters/Documents/personal/blog/.env')\n\n# Initialize the Google Gen AI client\nfrom google import genai\nfrom google.genai import types\n\n# Set your API key\n# In a production environment, use environment variables or secure secret management\nGOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')  # Replace with your API key\nclient = genai.Client(api_key=GOOGLE_API_KEY)\n```\n:::\n\n\n## Choosing the Right Model\n\nSpatial understanding works best with Gemini's newer models. For our examples, we'll use the `gemini-2.5-pro-exp-03-25` model, which offers enhanced spatial reasoning capabilities and supports segmentation. You can also use other Gemini 2.0 models like `gemini-2.0-flash` for faster processing, though with potentially less accurate results.\n\n::: {#8f25d6c0 .cell execution_count=2}\n``` {.python .cell-code}\n# Select a model for spatial understanding\nmodel_name = \"gemini-2.5-pro-exp-03-25\"  # Best for segmentation and detailed spatial analysis\n# Alternative models: \"gemini-2.0-flash\" for faster processing\n\n# Configure system instructions for better results\nbounding_box_system_instructions = \"\"\"\n    Return bounding boxes as a JSON array with labels. Never return masks or code fencing. Limit to 25 objects.\n    If an object is present multiple times, name them according to their unique characteristic (colors, size, position, unique characteristics, etc..).\n    \"\"\"\n\n# Configure safety settings\nsafety_settings = [\n    types.SafetySetting(\n        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        threshold=\"BLOCK_ONLY_HIGH\",\n    ),\n]\n```\n:::\n\n\n## Utility Functions for Visualization\n\nWe'll create some helper functions to parse the model's output and visualize the bounding boxes and segmentation masks.\n\n::: {#d59b1fe7 .cell execution_count=3}\n``` {.python .cell-code}\n# Function to draw bounding boxes on an image\ndef plot_bounding_boxes(im, bounding_boxes):\n    \"\"\"\n    Plots bounding boxes on an image with markers for each name, using PIL, normalized coordinates, and different colors.\n\n    Args:\n        im: The PIL Image object.\n        bounding_boxes: A list of BoundingBox objects.\n    \"\"\"\n    # Create a copy of the image to draw on\n    img = im.copy()\n    width, height = img.size\n    \n    # Create a drawing object\n    draw = ImageDraw.Draw(img)\n    \n    # Define a list of colors for different objects\n    colors = [\n        'red', 'green', 'blue', 'yellow', 'orange', 'pink', 'purple',\n        'brown', 'gray', 'beige', 'turquoise', 'cyan', 'magenta',\n        'lime', 'navy', 'maroon', 'teal', 'olive', 'coral', 'lavender',\n        'violet', 'gold', 'silver'\n    ] + [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n    \n    # Try to load a font that supports CJK characters\n    font = None\n    try:\n        # Try different fonts that might support CJK characters\n        font_paths = [\n            \"NotoSansCJK-Regular.ttc\",\n            \"/System/Library/Fonts/ヒラギノ角ゴシック W3.ttc\",  # Common on macOS\n            \"/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc\",  # Common on Linux\n            \"/Library/Fonts/Arial Unicode.ttf\"\n        ]\n        \n        for path in font_paths:\n            try:\n                font = ImageFont.truetype(path, size=14)\n                break\n            except (OSError, IOError):\n                continue\n                \n    except Exception as e:\n        print(f\"Could not load CJK font: {e}\")\n    \n    # If no CJK fonts are available, use a basic approach that avoids Unicode issues\n    if font is None:\n        print(\"Warning: No CJK font found. Text with non-Latin characters may not display correctly.\")\n        font = ImageFont.load_default()\n    \n    # Iterate over the bounding boxes\n    for i, bounding_box in enumerate(bounding_boxes):\n        # Select a color from the list\n        color = colors[i % len(colors)]\n        \n        # Convert normalized coordinates to absolute coordinates\n        abs_y1 = int(bounding_box.box_2d[0]/1000 * height)\n        abs_x1 = int(bounding_box.box_2d[1]/1000 * width)\n        abs_y2 = int(bounding_box.box_2d[2]/1000 * height)\n        abs_x2 = int(bounding_box.box_2d[3]/1000 * width)\n        \n        # Ensure coordinates are in the correct order\n        if abs_x1 > abs_x2:\n            abs_x1, abs_x2 = abs_x2, abs_x1\n        if abs_y1 > abs_y2:\n            abs_y1, abs_y2 = abs_y2, abs_y1\n        \n        # Draw the bounding box\n        draw.rectangle(((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=4)\n        \n        # Draw the text label if present\n        if hasattr(bounding_box, \"label\"):\n            try:\n                draw.text((abs_x1 + 8, abs_y1 + 6), bounding_box.label, fill=color, font=font)\n            except UnicodeEncodeError:\n                # Fallback for Unicode errors - print ASCII version of label\n                ascii_label = bounding_box.label.encode('ascii', 'replace').decode('ascii')\n                draw.text((abs_x1 + 8, abs_y1 + 6), ascii_label, fill=color, font=font)\n    \n    return img\n```\n:::\n\n\n## Object Detection with Bounding Boxes\n\nOur first example demonstrates basic object detection. We'll ask Gemini to identify objects in an image and draw bounding boxes around them.\n\n::: {#192c0b5c .cell execution_count=4}\n``` {.python .cell-code}\n# Load a sample image\ndef load_image(url):\n    \"\"\"Load an image from a URL or local path.\"\"\"\n    if url.startswith(('http://', 'https://')):\n        response = requests.get(url, stream=True)\n        img = Image.open(io.BytesIO(response.content))\n    else:\n        img = Image.open(url)\n    return img\n\n# Sample image URLs\nimage_urls = {\n    \"cupcakes\": \"https://storage.googleapis.com/generativeai-downloads/images/Cupcakes.jpg\",\n    \"socks\": \"https://storage.googleapis.com/generativeai-downloads/images/socks.jpg\",\n    \"vegetables\": \"https://storage.googleapis.com/generativeai-downloads/images/vegetables.jpg\",\n    \"bento\": \"https://storage.googleapis.com/generativeai-downloads/images/Japanese_Bento.png\",\n    \"origami\": \"https://storage.googleapis.com/generativeai-downloads/images/origamis.jpg\"\n}\n\n# Download a sample image\nimage_url = image_urls[\"cupcakes\"]\nimage = load_image(image_url)\n\n# Resize the image for better performance\nimage.thumbnail([640, 640], Image.Resampling.LANCZOS)\n\n# Display the original image\nimage\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n![](2025-04-27_gemini-image-segmentation_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\nNow let's detect objects in the image. We'll ask Gemini to identify the cupcakes and label them based on their toppings.\n\n::: {#6a0a8fa1 .cell execution_count=5}\n``` {.python .cell-code}\n# Import Pydantic for schema definition\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\n# Define our Pydantic model for object detection\nclass BoundingBox(BaseModel):\n    box_2d: List[int] = Field(description=\"Normalized coordinates [y1, x1, y2, x2] from 0-1000\")\n    label: str = Field(description=\"Description of the object's appearance\")\n\n# Define our prompt for object detection\nprompt = \"Detect the 2d bounding boxes of the cupcakes (with 'label' as topping description)\"\n\n# Send the request to the Gemini API\nresponse = client.models.generate_content(\n    model=model_name,\n    contents=[prompt, image],\n    config=types.GenerateContentConfig(\n        response_schema=list[BoundingBox],\n        response_mime_type=\"application/json\",\n        system_instruction=bounding_box_system_instructions,\n        temperature=0.5,\n        safety_settings=safety_settings,\n    )\n)\n\n# Display the model's response\nprint(\"Model response:\")\nprint(response.text)\n\n# Visualize the bounding boxes\nresult_image = plot_bounding_boxes(image, response.parsed)\nresult_image\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel response:\n[\n  {\"box_2d\": [393, 62, 556, 207], \"label\": \"red sprinkle frosting\"},\n  {\"box_2d\": [384, 250, 540, 371], \"label\": \"pink frosting with sprinkles\"},\n  {\"box_2d\": [369, 396, 500, 503], \"label\": \"pink frosting with sprinkles\"},\n  {\"box_2d\": [442, 434, 594, 565], \"label\": \"pink frosting with candy eyes\"},\n  {\"box_2d\": [371, 528, 521, 651], \"label\": \"pink frosting with blue candy balls\"},\n  {\"box_2d\": [375, 739, 534, 867], \"label\": \"chocolate frosting\"},\n  {\"box_2d\": [556, 40, 729, 201], \"label\": \"vanilla frosting with sprinkles and candy eyes\"},\n  {\"box_2d\": [544, 295, 700, 445], \"label\": \"chocolate base, vanilla frosting with sprinkles and candy eyes\"},\n  {\"box_2d\": [546, 514, 713, 664], \"label\": \"vanilla frosting with sprinkles and candy eyes\"},\n  {\"box_2d\": [479, 629, 638, 771], \"label\": \"vanilla frosting with sprinkles\"},\n  {\"box_2d\": [511, 800, 688, 962], \"label\": \"vanilla frosting with colorful candy pieces\"},\n  {\"box_2d\": [744, 135, 921, 307], \"label\": \"vanilla frosting with two candy eyes\"},\n  {\"box_2d\": [658, 353, 819, 514], \"label\": \"chocolate base, vanilla frosting with three candy eyes\"}\n]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n![](2025-04-27_gemini-image-segmentation_files/figure-html/cell-6-output-2.png){}\n:::\n:::\n\n\nAs you can see, Gemini successfully identified each cupcake and provided a descriptive label for each topping. The model returns bounding box coordinates in a normalized format (0-1000 range) with the structure `[y1, x1, y2, x2]`, where:\n\n- `y1`: Top edge (normalized)\n- `x1`: Left edge (normalized)\n- `y2`: Bottom edge (normalized)\n- `x2`: Right edge (normalized)\n\nNote that Gemini places the y-coordinates first, contrary to the common convention in computer vision libraries where x-coordinates typically come first.\n\n## Searching Within an Image\n\nGemini can also perform targeted searches within images, identifying specific objects that match certain criteria. Let's try this with a different image.\n\n::: {#71eceb9b .cell execution_count=6}\n``` {.python .cell-code}\n# Load a different image for search example\nimage = load_image(image_urls[\"socks\"])\nimage.thumbnail([640, 640], Image.Resampling.LANCZOS)\n\n# Define a search prompt\nprompt = \"Show me the positions of the socks with the face\"\n\n# Send the request to the Gemini API\nresponse = client.models.generate_content(\n    model=model_name,\n    contents=[prompt, image],\n    config=types.GenerateContentConfig(\n        response_schema=list[BoundingBox],\n        response_mime_type=\"application/json\",\n        system_instruction=bounding_box_system_instructions,\n        temperature=0.5,\n        safety_settings=safety_settings,\n    )\n)\n\n# Display the model's response\nprint(\"Model response:\")\nprint(response.text)\n\n# Visualize the search results\nresult_image = plot_bounding_boxes(image, response.parsed)\nresult_image\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel response:\n[\n  {\"box_2d\": [57, 249, 387, 516], \"label\": \"light blue sock with face (top left)\"},\n  {\"box_2d\": [235, 631, 650, 860], \"label\": \"light blue sock with face (top right)\"}\n]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=6}\n![](2025-04-27_gemini-image-segmentation_files/figure-html/cell-7-output-2.png){}\n:::\n:::\n\n\nThis example demonstrates Gemini's ability to understand natural language queries about visual content. The model identified specifically the socks with faces on them, ignoring other socks in the image. This capability is particularly useful for:\n\n- **Content moderation**: Finding specific objects or content that may require review\n- **Visual search**: Enabling users to search for specific items within images\n- **Product identification**: Locating particular products in retail or inventory images\n- **Data annotation**: Automating the process of identifying and labeling specific objects\n\n## Multilingual Capabilities\n\nGemini's multimodal understanding extends to multiple languages. Let's demonstrate this by asking the model to label food items in a Japanese bento box with both Japanese characters and English translations.\n\n::: {#68712f0c .cell execution_count=7}\n``` {.python .cell-code}\n# Load the Japanese bento image\nimage = load_image(image_urls[\"bento\"])\nimage.thumbnail([640, 640], Image.Resampling.LANCZOS)\n\n# Define a multilingual prompt\nprompt = \"Detect food, label them with Japanese characters + english translation.\"\n\n# Send the request to the Gemini API\nresponse = client.models.generate_content(\n    model=model_name,\n    contents=[prompt, image],\n    config=types.GenerateContentConfig(\n        response_schema=list[BoundingBox],\n        response_mime_type=\"application/json\",\n        system_instruction=bounding_box_system_instructions,\n        temperature=0.5,\n        safety_settings=safety_settings,\n    )\n)\n\n# Visualize the multilingual labels\nresult_image = plot_bounding_boxes(image, response.parsed)\nresult_image\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](2025-04-27_gemini-image-segmentation_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\nThis example showcases Gemini's multilingual capabilities. The model correctly identified different Japanese food items and provided both Japanese characters and English translations in the labels. This functionality is valuable for:\n\n- **Cross-cultural applications**: Creating inclusive experiences for users from different linguistic backgrounds\n- **Translation services**: Providing visual translation for food items, products, or signs\n- **Educational tools**: Teaching vocabulary in different languages with visual references\n- **Cultural understanding**: Helping users understand items from different cultures\n\n## Advanced Reasoning with Spatial Understanding\n\nGemini can go beyond simple object detection to perform more complex spatial reasoning tasks. Let's demonstrate this by asking the model to find the shadow of a specific origami figure.\n\n::: {#690766d7 .cell execution_count=8}\n``` {.python .cell-code}\n# Load the origami image\nimage = load_image(image_urls[\"origami\"])\nimage.thumbnail([640, 640], Image.Resampling.LANCZOS)\n\n# Define a prompt that requires spatial reasoning\nprompt = \"Draw a square around the fox's shadow\"\n\n# Send the request to the Gemini API\nresponse = client.models.generate_content(\n    model=model_name,\n    contents=[prompt, image],\n    config=types.GenerateContentConfig(\n        response_schema=list[BoundingBox],\n        response_mime_type=\"application/json\",\n        system_instruction=bounding_box_system_instructions,\n        temperature=0.5,\n        safety_settings=safety_settings,\n    )\n)\n\n# Visualize the result of the spatial reasoning task\nresult_image = plot_bounding_boxes(image, response.parsed)\nresult_image\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n![](2025-04-27_gemini-image-segmentation_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\nThis example demonstrates Gemini's sophisticated visual reasoning capabilities. The model was able to:\n\n1. Identify the fox origami figure in the image\n2. Understand the concept of a shadow\n3. Locate the shadow cast by the fox figure\n4. Draw a bounding box specifically around the shadow\n\nThis type of spatial reasoning can be applied to numerous real-world scenarios:\n\n- **Scene understanding**: Analyzing relationships between objects in a scene\n- **Visual reasoning**: Answering questions about spatial arrangements\n- **Assistive technology**: Helping visually impaired users understand spatial relationships\n\n## Image Segmentation with Gemini 2.5\n\nWith Gemini 2.5 models, we can go beyond bounding boxes to generate more precise segmentation masks that outline the exact boundaries of objects. Let's implement the necessary utilities and demonstrate this capability.\n\n::: {#71ec77e7 .cell execution_count=9}\n``` {.python .cell-code}\n# Utilities for segmentation masks\nimport dataclasses\nimport base64\n\n@dataclasses.dataclass(frozen=True)\nclass SegmentationMask:\n    # bounding box pixel coordinates (not normalized)\n    y0: int  # in [0..height - 1]\n    x0: int  # in [0..width - 1]\n    y1: int  # in [0..height - 1]\n    x1: int  # in [0..width - 1]\n    mask: np.array  # [img_height, img_width] with values 0..255\n    label: str\n\ndef parse_segmentation_masks(\n    predicted_str: str, *, img_height: int, img_width: int\n) -> list[SegmentationMask]:\n    \"\"\"Parse segmentation masks from model output.\"\"\"\n    items = json.loads(predicted_str)\n    masks = []\n    for item in items:\n        # Extract bounding box coordinates\n        abs_y0 = int(item[\"box_2d\"][0] / 1000 * img_height)\n        abs_x0 = int(item[\"box_2d\"][1] / 1000 * img_width)\n        abs_y1 = int(item[\"box_2d\"][2] / 1000 * img_height)\n        abs_x1 = int(item[\"box_2d\"][3] / 1000 * img_width)\n        \n        # Validate bounding box\n        if abs_y0 >= abs_y1 or abs_x0 >= abs_x1:\n            print(\"Invalid bounding box\", item[\"box_2d\"])\n            continue\n            \n        label = item[\"label\"]\n        png_str = item[\"mask\"]\n        \n        # Validate mask format\n        if not png_str.startswith(\"data:image/png;base64,\"):\n            print(\"Invalid mask\")\n            continue\n            \n        # Decode mask\n        png_str = png_str.removeprefix(\"data:image/png;base64,\")\n        png_str = base64.b64decode(png_str)\n        mask = Image.open(io.BytesIO(png_str))\n        \n        # Calculate dimensions\n        bbox_height = abs_y1 - abs_y0\n        bbox_width = abs_x1 - abs_x0\n        if bbox_height < 1 or bbox_width < 1:\n            print(\"Invalid bounding box\")\n            continue\n            \n        # Resize mask to match bounding box\n        mask = mask.resize((bbox_width, bbox_height), resample=Image.Resampling.BILINEAR)\n        np_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n        np_mask[abs_y0:abs_y1, abs_x0:abs_x1] = mask\n        \n        masks.append(SegmentationMask(abs_y0, abs_x0, abs_y1, abs_x1, np_mask, label))\n    return masks\n\ndef overlay_mask_on_img(\n    img: Image,\n    mask: np.ndarray,\n    color: str,\n    alpha: float = 0.7\n) -> Image.Image:\n    \"\"\"Overlay a segmentation mask on an image.\"\"\"\n    if not (0.0 <= alpha <= 1.0):\n        raise ValueError(\"Alpha must be between 0.0 and 1.0\")\n\n    # Convert the color name to RGB\n    try:\n        color_rgb = ImageColor.getrgb(color)\n    except ValueError as e:\n        raise ValueError(f\"Invalid color name '{color}'. Error: {e}\")\n\n    # Prepare the image for alpha compositing\n    img_rgba = img.convert(\"RGBA\")\n    width, height = img_rgba.size\n\n    # Create the colored overlay\n    alpha_int = int(alpha * 255)\n    overlay_color_rgba = color_rgb + (alpha_int,)\n\n    # Create a transparent layer\n    colored_mask_layer_np = np.zeros((height, width, 4), dtype=np.uint8)\n\n    # Apply the overlay color where the mask is active\n    mask_np_logical = mask > 127\n    colored_mask_layer_np[mask_np_logical] = overlay_color_rgba\n\n    # Convert back to PIL and composite\n    colored_mask_layer_pil = Image.fromarray(colored_mask_layer_np, 'RGBA')\n    result_img = Image.alpha_composite(img_rgba, colored_mask_layer_pil)\n\n    return result_img\n\ndef plot_segmentation_masks(img: Image, segmentation_masks: list[SegmentationMask]):\n    \"\"\"Plot segmentation masks, bounding boxes, and labels on an image.\"\"\"\n    # Define colors\n    colors = [\n        'red', 'green', 'blue', 'yellow', 'orange', 'pink', 'purple',\n        'brown', 'gray', 'beige', 'turquoise', 'cyan', 'magenta'\n    ] + [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n    \n    # Try to load a font that supports CJK characters\n    font = None\n    try:\n        # Try different fonts that might support CJK characters\n        font_paths = [\n            \"NotoSansCJK-Regular.ttc\",\n            \"/System/Library/Fonts/ヒラギノ角ゴシック W3.ttc\",  # Common on macOS\n            \"/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc\",  # Common on Linux\n            \"/Library/Fonts/Arial Unicode.ttf\"\n        ]\n        \n        for path in font_paths:\n            try:\n                font = ImageFont.truetype(path, size=14)\n                break\n            except (OSError, IOError):\n                continue\n                \n    except Exception as e:\n        print(f\"Could not load CJK font: {e}\")\n    \n    # If no CJK fonts are available, use a basic approach that avoids Unicode issues\n    if font is None:\n        print(\"Warning: No CJK font found. Text with non-Latin characters may not display correctly.\")\n        font = ImageFont.load_default()\n    \n    # Create a copy of the image\n    img = img.copy()\n    \n    # Step 1: Overlay all masks\n    for i, mask in enumerate(segmentation_masks):\n        color = colors[i % len(colors)]\n        img = overlay_mask_on_img(img, mask.mask, color)\n\n    # Step 2: Draw all bounding boxes\n    draw = ImageDraw.Draw(img)\n    for i, mask in enumerate(segmentation_masks):\n        color = colors[i % len(colors)]\n        draw.rectangle(\n            ((mask.x0, mask.y0), (mask.x1, mask.y1)), outline=color, width=4\n        )\n\n    # Step 3: Draw all text labels\n    for i, mask in enumerate(segmentation_masks):\n        color = colors[i % len(colors)]\n        if mask.label != \"\":\n            try:\n                draw.text((mask.x0 + 8, mask.y0 - 20), mask.label, fill=color, font=font)\n            except UnicodeEncodeError:\n                # Fallback for Unicode errors - print ASCII version of label\n                ascii_label = mask.label.encode('ascii', 'replace').decode('ascii')\n                draw.text((mask.x0 + 8, mask.y0 - 20), ascii_label, fill=color, font=font)\n    \n    return img\n```\n:::\n\n\nNow let's test the segmentation capability with an image containing cupcakes.\n\n::: {#cae88e2d .cell execution_count=10}\n``` {.python .cell-code}\n# Define Pydantic model for segmentation masks\nclass SegmentationMaskModel(BaseModel):\n    box_2d: List[int] = Field(description=\"Normalized coordinates [y0, x0, y1, x1] from 0-1000\")\n    mask: str = Field(description=\"Base64-encoded PNG image representing the segmentation mask\")\n    label: str = Field(description=\"Description of the object\")\n\n# Load the cupcakes image for segmentation\nimage = load_image(image_urls[\"cupcakes\"])\nimage.thumbnail([1024, 1024], Image.Resampling.LANCZOS)\n\n# Define a prompt for segmentation\nprompt = \"\"\"Give the segmentation masks for each cupcake. \nOutput a JSON list of segmentation masks where each entry contains the 2D bounding box in the key \"box_2d\", \nthe segmentation mask in key \"mask\", and the text label in the key \"label\" describing the topping.\"\"\"\n\n# Send the request to the Gemini API (note: no system instruction for segmentation)\nresponse = client.models.generate_content(\n    model=model_name,\n    contents=[prompt, image],\n    config=types.GenerateContentConfig(\n        response_schema=list[SegmentationMaskModel],\n        response_mime_type=\"application/json\",\n        temperature=0.5,\n        safety_settings=safety_settings,\n    )\n)\n\n# Parse and visualize the segmentation masks\nsegmentation_masks = parse_segmentation_masks(response.text, img_height=image.size[1], img_width=image.size[0])\nresult_image = plot_segmentation_masks(image, segmentation_masks)\nresult_image\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n![](2025-04-27_gemini-image-segmentation_files/figure-html/cell-11-output-1.png){}\n:::\n:::\n\n\n## Understanding Gemini's Segmentation Output\n\nGemini's segmentation output is more complex than simple bounding boxes. Let's break down what the model returns:\n\n1. **Bounding box (`box_2d`)**: A 4-element array `[y0, x0, y1, x1]` with normalized coordinates between 0 and 1000.\n\n2. **Label (`label`)**: A text string describing the segmented object.\n\n3. **Mask (`mask`)**: A base64-encoded PNG image representing the segmentation mask. This mask is:\n   - Sized to match the dimensions of the bounding box\n   - Contains grayscale values (0-255) indicating the probability that each pixel belongs to the object\n   - Needs to be decoded, resized, and applied to the original image\n\nThe segmentation process involves:\n\n1. **Decoding**: Converting the base64 string to an image\n2. **Resizing**: Matching the mask to the bounding box dimensions\n3. **Thresholding**: Deciding which pixels belong to the object (typically values > 127)\n4. **Integration**: Placing the mask in the correct position in the full-sized image\n5. **Visualization**: Overlaying the mask with a semi-transparent color\n\nThis detailed segmentation capability allows for much more precise object delineation than bounding boxes alone, making it valuable for:\n\n- **Image editing**: Precisely separating objects from backgrounds\n- **Medical imaging**: Outlining organs or anomalies\n- **Product visualization**: Creating cutouts of products\n- **AR/VR applications**: Precise occlusion and placement of virtual objects\n\n## Conclusion\n\nIn this post, we've explored Gemini 2.0's spatial understanding capabilities, from basic object detection with bounding boxes to sophisticated segmentation with Gemini 2.5. These capabilities enable a wide range of applications without requiring specialized computer vision expertise or custom models.\n\nKey takeaways:\n\n1. **Simple integration**: With just a few lines of code, you can implement powerful object detection and segmentation.\n\n2. **Natural language interface**: Use plain language to describe what you're looking for, making the API accessible to users without technical expertise.\n\n3. **Multilingual support**: Label objects in multiple languages, facilitating cross-cultural applications.\n\n4. **Advanced reasoning**: Leverage Gemini's understanding of spatial relationships to solve complex visual tasks.\n\n5. **Precise segmentation**: With Gemini 2.5, get pixel-perfect object boundaries for detailed image analysis.\n\nThese capabilities open up numerous possibilities for developers, from enhancing accessibility to creating immersive AR experiences. By combining Gemini's visual understanding with its language capabilities, you can build intuitive, powerful applications that bridge the gap between vision and language.\n\nFor more examples and applications, check out the [Spatial understanding example](https://aistudio.google.com/starter-apps/spatial) from AI Studio, or explore the [Gemini 2.0 cookbook](https://github.com/google-gemini/cookbook/tree/main/gemini-2/) for other examples of Gemini's capabilities.\n\n",
    "supporting": [
      "2025-04-27_gemini-image-segmentation_files"
    ],
    "filters": [],
    "includes": {}
  }
}