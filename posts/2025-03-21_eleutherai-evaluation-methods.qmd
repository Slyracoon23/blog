---
aliases:
- /eleutherai-evaluation-methods/
categories:
- Large Language Models
date: '2025-03-21'
image: https://example.com/eleutherai-evaluation-image.webp
title: "EleutherAI's lm-evaluation-harness: Architecture and Configuration"
subtitle: "A comprehensive guide to configuration, task architecture, and model integration"
---

EleutherAI has developed one of the most robust frameworks for evaluating language models (LMs): the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness). This post explores the framework's architecture, configuration system, and integration patterns to help you understand how to use, extend, and contribute to this powerful evaluation ecosystem.

## What is lm-evaluation-harness?

The Language Model Evaluation Harness is a unified framework that allows testing of generative language models on a wide variety of benchmarks. It serves as the backend for Hugging Face's Open LLM Leaderboard and has been used in hundreds of research papers by organizations including NVIDIA, Cohere, BigScience, BigCode, and Mosaic ML. The framework ensures reproducibility by using publicly available prompts and supports custom evaluations.

Key features include:

- Over 60 standard academic benchmarks with hundreds of subtasks
- Support for models via transformers (including quantization via GPTQ), GPT-NeoX, and Megatron-DeepSpeed
- Fast inference with vLLM
- Support for commercial APIs (OpenAI, TextSynth)
- Evaluation on adapter models (like LoRA) through PEFT
- Support for local models and benchmarks
- Customizable prompts and metrics

## Getting Started

### Installation

Basic installation:
```bash
git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness
cd lm-evaluation-harness
pip install -e .
```

For development:
```bash
pip install -e ".[dev]"
```

The framework also provides optional dependencies for extended functionality, with details available in the GitHub repository.

### Basic Usage

Evaluate a HuggingFace model on a specific task:
```bash
python -m lm_eval --model hf --model_args pretrained=gpt2 --tasks hellaswag --num_fewshot 5
```

Or using the Python API:
```python
from lm_eval import evaluator, tasks
from lm_eval.models import get_model

model = get_model("hf", pretrained="gpt2")
results = evaluator.evaluate(model, tasks=["hellaswag"], num_fewshot=5)
```



## Core Request Types

At the heart of the evaluation harness are three fundamental request types that measure different aspects of language model capabilities:

### 1. `generate_until`

This request type measures a model's ability to generate text given a prompt. The model receives:
- An input string
- Generation parameters (like maximum length or stopping sequences)

The model then generates text until it reaches stopping criteria, and the output is evaluated against reference answers. This is commonly used for tasks like question answering, summarization, and general text generation.

```python
def generate_until(self, requests: list[Instance]) -> list[str]:
    # Each request contains input string and generation parameters
    # Returns generated text
```

### 2. `loglikelihood`

This request measures how likely a model would generate a specific target string given an input. For each request:
- The model receives an input string and a target string
- It returns (log_probability, is_greedy)
  - log_probability: The log probability of generating the target given the input
  - is_greedy: Whether the target is the most likely sequence the model would generate

This is useful for multiple-choice tasks where we can compare the likelihood of different answers.

```python
def loglikelihood(self, requests: list[Instance]) -> list[tuple[float, bool]]:
    # Each request contains input string and target string
    # Returns (log probability, is_greedy)
```

### 3. `loglikelihood_rolling`

A special case of loglikelihood, used to evaluate perplexity on a dataset. The model calculates the likelihood of generating an entire input string from scratch.

```python
def loglikelihood_rolling(self, requests: list[Instance]) -> list[tuple[float, bool]]:
    # Each request contains just an input string
    # Returns log probability of the entire string
```

## Task Configuration System

The evaluation harness uses a declarative YAML configuration system to define tasks. These files reside in a hierarchical directory structure under `lm_eval/tasks/`, making it easy to add new tasks without writing extensive code.

### YAML-Based Task Definition

A complete task configuration includes:

```yaml
task: my_qa_task
dataset_path: huggingface_dataset_name
dataset_name: configuration_name
dataset_kwargs:
  data_dir: subdirectory_path
training_split: train
validation_split: validation
test_split: test
fewshot_split: train
task_type: multiple_choice
doc_to_text: "Context: {{context}}\nQuestion: {{question}}\nAnswer:"
doc_to_target: "{{answers.text[0]}}"
metric_list:
  - metric: exact_match
    aggregation: mean
    higher_is_better: true
```

This configuration:
- Names the task and specifies its type
- Defines dataset source and configuration
- Specifies which data splits to use for evaluation, few-shot examples, etc.
- Provides templates for formatting inputs and expected outputs
- Defines how to evaluate model outputs

### Few-Shot Learning Configuration

The harness supports multiple approaches for few-shot example selection:

**Split-Based Sampling**
```yaml
fewshot_split: train
fewshot_num: 5
```

**Hardcoded Examples**
```yaml
fewshot_config:
  sampler: first_n
  samples:
    - input: "Example query 1"
      answer: "Correct response 1"
    - input: "Example query 2"
      answer: "Correct response 2"
```

**Programmatic Generation**
```python
def list_fewshot_samples() -> list[dict]:
    return [
        {"input": "Dynamic example 1", "answer": "Response 1"},
        {"input": "Dynamic example 2", "answer": "Response 2"}
    ]
```

### Metric Configuration

Evaluation metrics are declared through the `metric_list` parameter:

```yaml
metric_list: 
  - metric: accuracy
    aggregation: mean
    higher_is_better: true
  - metric: bleu
    aggregation: geometric_mean
    kwargs:
      max_order: 4
```

The framework supports both built-in metrics and custom implementations through a plugin architecture. Metric aggregation strategies (mean, sum, etc.) and optimization directions (higher_is_better) must be explicitly declared.

## Model Integration Architecture

### Core Interface Requirements

To evaluate a new model, you implement a subclass of `LM` with the three core request methods:

```python
class MyCustomLM(LM):
    def __init__(self, model_path, **kwargs):
        # Initialize your model
        
    def loglikelihood(self, requests):
        # Implement loglikelihood calculation
        
    def loglikelihood_rolling(self, requests):
        # Implement rolling loglikelihood calculation
        
    def generate_until(self, requests):
        # Implement text generation
```

### Input Processing Pipeline

The framework employs strict tokenization handling to ensure evaluation validity:

```
Input Tokens: [0 1 2 3|4 5 6 7 8 9]
Model Logits:   1 2 3|4 5 6 7 8 9
Continuation:        4 5 6 7 8 9
```

This architecture prevents data leakage by truncating final target tokens during likelihood calculation.

### Chat Template Integration

For chat models, additional methods enable proper templating and formatting:

```python
class ChatLM(LM):
    @property
    def tokenizer_name(self) -> str:
        return "unique_template_identifier"
    
    def chat_template(self, config: Union[bool, str]) -> str:
        # Return Jinja template string
    
    def apply_chat_template(self, history: list[dict]) -> str:
        # Format chat history into prompt
```

The system supports multiple template configurations through the `--apply_chat_template` CLI flag, with automatic caching based on `tokenizer_name` to prevent template collisions.

## Advanced Features and Configuration

### Groups and Benchmarks

The harness supports grouping tasks into benchmarks, with flexible aggregation:

```yaml
group: nli_tasks
task:
  - cb
  - anli_r1
  - rte
aggregate_metric_list:
  - metric: accuracy
    aggregation: mean
    higher_is_better: true
```

This creates an "nli_tasks" benchmark that averages accuracy across three natural language inference tasks.

### Performance Optimization

The `Reorderer` utility improves runtime estimation through length-based batching:

```python
from lm_eval.utils import Reorderer

class OptimizedLM(LM):
    def __init__(self):
        self.reorderer = Reorderer(requests, lambda x: len(x))
    
    def process_batch(self, requests):
        sorted_requests = self.reorderer.get_reordered()
        # Process ordered requests
```

This pattern groups requests by input length to minimize padding and maximize computational efficiency.

### Model and Task Registration

Models become CLI-accessible through decorator-based registration:

```python
from lm_eval.api.registry import register_model

@register_model("custom-model", "alt-name")
class CustomLM(LM):
    # Implementation
```

New tasks require JSON metadata registration:

```json
{
  "task": "custom-task",
  "group": "knowledge-test",
  "aliases": ["ct-v1", "custom-test"]
}
```

This enables task discovery and provides backward-compatible aliases for evaluation suite versioning.

## Under the Hood: Request Processing

When evaluating a model, the harness:

1. Loads the task configuration
2. Processes the dataset
3. Formats prompts using templates
4. Creates batches of requests
5. Calls the appropriate request method on the model
6. Post-processes outputs and calculates metrics

The architecture is designed to be modular, allowing each component to be customized while maintaining a consistent evaluation pipeline.

## Using vLLM for Fast Evaluation

For large-scale evaluations, the framework supports [vLLM](https://github.com/vllm-project/vllm) for significant speed improvements:

```bash
python -m lm_eval --model vllm --model_args pretrained=meta-llama/Llama-2-7b-hf --tasks mmlu
```

For even larger models like Llama 405B, EleutherAI recommends using vLLM's OpenAI-compliant API to host the model, and then using the `local-completions` model type for evaluation.

## Multimodal Evaluation Support

As of September 2024, the framework has started supporting text+image multimodal input tasks through the `hf-multimodal` and `vllm-vlm` model types, with the `mmmu` task as a prototype feature. For more extensive multimodal evaluation, users are encouraged to check out [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval), which forked from lm-evaluation-harness.

## Diagnostic and Testing Practices

### Model Validation

Implementation requires test cases covering core functionality:

```python
def test_loglikelihood():
    lm = CustomLM()
    results = lm.loglikelihood([("Input", "Target")])
    assert isinstance(results[0][0], float)

def test_generation():
    lm = CustomLM()
    outputs = lm.generate_until([("Input", {"until": ["\n"]})])
    assert isinstance(outputs[0], str)
```

### Debugging Configuration

Verbose logging activation aids task development:

```bash
export LOGLEVEL=DEBUG
python -m lm_eval --model hf --tasks custom-task --num_fewshot 5
```

Debug output reveals template rendering, few-shot selection, and metric calculation details.

## Best Practices and Common Pitfalls

1. **Tokenization Alignment**
   - Verify model logits align with target token positions
   - Prevent off-by-one errors in likelihood calculation
   - Use reference implementations from `HFLM` as guides

2. **Template Safety**
   - Escape special characters in Jinja templates
   - Validate few-shot example field consistency
   - Implement template versioning through `tokenizer_name`

3. **Performance Considerations**
   - Implement request reordering for large evaluations
   - Utilize batch processing where supported
   - Profile memory usage during generation tasks

4. **Evaluation Validity**
   - Separate few-shot and test splits
   - Audit metric implementations for task appropriateness
   - Verify chat template application through debug output

## Contributing to lm-evaluation-harness

EleutherAI welcomes contributions to improve the framework. The project follows these priorities for addressing concerns about prompting and evaluation details:

1. Use procedures with widespread agreement among LLM trainers
2. Follow clear and unambiguous official implementations
3. Use procedures with widespread agreement among LLM evaluators
4. Choose from common implementations when there's no universal agreement, preferring those found in LLM training papers

They maintain an active [Discord server](https://discord.gg/eleutherai) with the `#lm-thunderdome` channel dedicated to developing this project and `#release-discussion` for support.

## Conclusion

EleutherAI's evaluation framework provides a standardized way to assess language model capabilities across a wide range of tasks. By separating the evaluation logic from model implementation, it enables fair comparison between different models and architectures. The declarative configuration system makes it easy to add new tasks and benchmarks, contributing to the growing ecosystem of LLM evaluation.

Whether you're developing a new model or researching evaluation methodologies, understanding these evaluation methods is crucial for rigorous assessment of language model capabilities.

## References

1. [EleutherAI lm-evaluation-harness GitHub Repository](https://github.com/EleutherAI/lm-evaluation-harness)
2. [Official Task Guide](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/task_guide.md)
3. [New Task Guide](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md)
4. [Weights & Biases: Evaluating LLMs with EleutherAI](https://wandb.ai/wandb_gen/llm-evaluation/reports/Evaluating-Large-Language-Models-LLMs-with-Eleuther-AI--VmlldzoyOTI0MDQ3)
5. [Mozilla AI: LM Buddy Evaluation Concepts](https://mozilla-ai.github.io/lm-buddy/evaluation_concepts.html)
6. [Red Hat: Evaluating Large Language Models](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_cloud_service/1/html/monitoring_data_science_models/evaluating-large-language-models_monitor)
7. [Unitxt LM Evaluation Documentation](https://www.unitxt.ai/en/1.14.0/docs/lm_eval.html)
8. [TrustyAI LM Evaluation Tutorial](https://trustyai-explainability.github.io/trustyai-site/main/lm-eval-tutorial.html)
9. [NVIDIA NGC LM Evaluation Harness Container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/eval-factory/containers/lm-evaluation-harness)
10. [EleutherAI: Language Model Evaluation Projects](https://www.eleuther.ai/projects/large-language-model-evaluation)
