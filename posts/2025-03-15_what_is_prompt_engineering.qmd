---
aliases:
- /what-is-prompt-engineering/
categories:
- Large Language Models
date: '2024-03-05'
image: /images/what_is_prompt_engineering/thumbnail.jpg
title: "What is Prompt Engineering?"
subtitle: "Understanding the art and science of crafting effective prompts for large language models"
format: html
---

I recently had an enlightening experience at work. I was assigned to a project that required using a large language model (specifically Gemini 2.0 Flash) to extract information from a series of documents.

My first instinct was to break the task down into its individual parts or develop some basic building blocks that could help me understand the components of the task. This seemed like a sensible approach—one that would have been considered best practice just a couple of years ago. However, in today's world of advanced LLMs, despite their often opaque and inexplicable mechanisms, I was encouraged to try something much simpler yet paradoxically more challenging.

A colleague suggested: "Why don't you just throw all the data into Gemini and prompt the model directly?"

Surprised, I responded: "You want me to zero-shot this complex task with just plain English?"

They casually replied: "Yeah."

## What is Prompt Engineering?

Prompt engineering is the practice of designing and refining inputs to AI systems, particularly large language models (LLMs), to elicit desired outputs. It's a relatively new discipline that sits at the intersection of natural language processing, human-computer interaction, and cognitive science.

At its core, prompt engineering involves crafting queries, instructions, or contexts that guide an AI model toward generating specific, accurate, and useful responses. This process has become increasingly important as LLMs like GPT-4, Claude, and Gemini have grown more capable but also more sensitive to the nuances of how questions are framed.

![Example of how LLMs work](https://imgur.com/NjzjK7h)

## In-Context Learning: The Technical Side of Prompt Engineering

While "prompt engineering" is the colloquial term that has gained popularity, researchers and AI developers often refer to this practice as **in-context learning**. This more technical framing helps explain what's actually happening when we craft prompts for large language models.

In-context learning describes how LLMs use the context provided within the prompt itself to condition their outputs. Unlike traditional machine learning approaches where models are explicitly trained on labeled examples before deployment, LLMs can "learn" from examples or instructions provided directly in the prompt at inference time.

### How Tokens Condition the Output

At a technical level, here's what happens:

1. **Tokenization** - Your prompt is broken down into tokens (word fragments, punctuation, etc.)
2. **Context Window** - These tokens occupy the model's context window (a fixed-size memory buffer)
3. **Conditioning** - Each token influences probability distributions for subsequent tokens
4. **Generation** - The model generates new tokens based on these conditioned probabilities

The tokens in your prompt essentially "condition" the statistical patterns that the model has learned during pre-training, steering it toward certain outputs. This is why carefully chosen examples, specific instructions, or role definitions can dramatically alter results—they shift the probability distribution of what tokens the model will generate next.

### The Mathematics of Token Conditioning

Mathematically, LLMs operate by modeling the probability distribution of the next token given all previous tokens. If we represent the sequence of tokens as $x_1, x_2, ..., x_n$, the model computes:

$$P(x_n | x_1, x_2, ..., x_{n-1})$$

This conditional probability determines which token is most likely to follow the sequence. The model's output is generated by sampling from this distribution or selecting the highest probability token at each step.

When you provide a prompt, you're essentially fixing the first $k$ tokens in this sequence, which forces the model to compute:

$$P(x_{k+1} | x_1, x_2, ..., x_k)$$

In transformer-based models like GPT, Claude, or Gemini, this conditional probability is computed using attention mechanisms. Each token's representation is influenced by all previous tokens according to attention weights $\alpha_{ij}$:

$$\text{attention}(x_i) = \sum_{j=1}^{i-1} \alpha_{ij} \cdot v_j$$

Where $v_j$ is the value vector for token $j$ and $\alpha_{ij}$ represents how much token $i$ should attend to token $j$.

The brilliant insight of in-context learning is that by carefully crafting the prompt tokens $(x_1, x_2, ..., x_k)$, we can steer these probability distributions in ways that make the model behave as if it were explicitly trained for our specific task, even though it's merely continuing a sequence according to patterns it learned during pre-training.

What makes this approach revolutionary is that the same base model can perform radically different tasks without any fine-tuning or retraining, simply by changing the prompt. The model effectively adapts its behavior based solely on the context provided within the prompt itself.

This technical understanding helps explain why structured techniques like chain-of-thought prompting, few-shot examples, and system role definitions work so effectively—they're all ways of conditioning the token probabilities in directions that align with our goals.

## Examples of Prompt Engineering in Practice

Here are some key prompt engineering techniques that can significantly improve your results when working with LLMs:

1. **Clear Instructions** - Specify format, length, and focus areas in your prompts
   - Example: "Summarize in 5 bullet points focusing on technical concepts"
   - Use delimiters like """ or ### to separate instructions from content

2. **Format Specification** - Explicitly define how you want the output structured 
   - Request specific output formats like lists, tables, or JSON
   - Define categories and labels for extracted information

3. **Few-Shot Learning** - Provide examples of the input-output pairs you expect
   - Include 2-3 examples of ideal responses before your actual request
   - Especially useful for classification, extraction, or specific formats

4. **Role Specification** - Assign an expert role to guide the model's perspective
   - Example: "You are an expert Python developer specializing in data science"
   - Helps frame responses with appropriate domain knowledge and terminology

5. **Iterative Refinement** - Improve prompts based on model outputs
   - Start simple, then adjust based on results
   - Add constraints or clarifications to address shortcomings

As LLMs continue to evolve, these fundamental techniques provide a solid foundation for effectively leveraging these powerful tools across various applications.

## Drawbacks and Limitations of Prompt Engineering

While prompt engineering offers powerful capabilities, it also comes with significant limitations and challenges:

### Inconsistency and Reliability Issues

One of the most frustrating aspects of prompt engineering is its inherent variability. The same prompt can produce different results across:

- Multiple runs with the same model
- Different versions of the same model
- Various models from different providers

This inconsistency makes it difficult to develop robust applications where predictable, reliable outputs are essential. Even when a prompt works perfectly in testing, minor variations in input data or context can lead to unexpected outputs in production environments.

### Context Window Limitations

Every LLM has a finite context window—the maximum number of tokens it can process at once. This creates practical limitations:

- Complex tasks requiring extensive context may not fit within the window
- Long documents must be chunked, potentially losing important connections
- Cost increases with context length in most commercial implementations

As models grow larger, these limitations are gradually being addressed, but they remain a significant constraint for many real-world applications.

### Prompt Sensitivity and Brittleness

Small changes in prompt wording can dramatically alter outputs, creating what researchers call "prompt brittleness." This sensitivity means:

- Minor modifications can break previously functional prompts
- Maintaining consistent performance requires careful prompt version control
- Users without prompt engineering expertise may struggle to get reliable results

This brittleness often leads to complex, over-engineered prompts that attempt to anticipate and prevent all possible misinterpretations—further increasing complexity and maintenance challenges.

### The "Prompt Leak" Problem

Models sometimes ignore parts of complex prompts or "leak" information about their instructions into their outputs. This can lead to:

- Confidential prompt instructions appearing in generated content
- Conflicting instructions being selectively followed or ignored
- Inconsistent adherence to specified constraints or formats

These issues become particularly problematic in applications where security, privacy, or strict adherence to guidelines is critical.

### Ethical and Bias Considerations

Perhaps most concerning are the ethical dimensions of prompt engineering:

- Biases in training data can be amplified through carefully crafted prompts
- Adversarial prompting can potentially bypass safety measures
- Prompts designed to extract maximum performance may reinforce problematic patterns

As prompt engineering becomes more sophisticated, the responsibility to consider these ethical implications grows correspondingly important.

### The Skills Gap and Expertise Requirements

Effective prompt engineering currently requires specialized knowledge that combines:

- Understanding of LLM technical capabilities and limitations
- Domain expertise relevant to the specific task
- Experience with prompt design patterns and best practices

This skills gap means that many organizations struggle to effectively leverage LLMs, even when they have access to the most advanced models available.

## Finding Balance: The Future of Prompt Engineering

Despite these limitations, prompt engineering remains a valuable approach for interfacing with large language models. The field is rapidly evolving, with researchers and practitioners developing:

- Automated prompt optimization techniques
- Tools to test prompt robustness across different inputs
- Libraries of reusable prompt patterns for common tasks
- Guidelines for responsible prompt design

As models become more capable and interfaces more sophisticated, we may see a shift from explicit prompt engineering toward more natural interactions with AI systems. However, understanding the fundamentals of how prompts influence model behavior will remain valuable knowledge for anyone working with these powerful tools.
