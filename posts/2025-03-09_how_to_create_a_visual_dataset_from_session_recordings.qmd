---
aliases:
- /how-to-create-a-visual-dataset-from-session-recordings/
categories:
- Dataset Preparation
date: '2025-03-09'
image: /images/how_to_create_a_visual_dataset_from_session_recordings/thumbnail.jpg
title: "How to Create a Visual Dataset from Session Recordings"
subtitle: "A step-by-step guide to transforming user session recordings into valuable training data for vision models"
format: html
---

I have been building AI applications for a while now (over 2 years!). During this time, I've somewhat dreaded doing "real" machine learning and AI work. As a developer, I initially thought I shouldn't need to worry about data and model performance, but (un)fortunately these models are truly data-oriented. If you want to achieve strong results with LLMs and other AI/machine learning systems, you will eventually have to work with data and label it yourself. I'm sorry, but it's unavoidable.

Luckily for you, we'll go through an exercise of how to get from a general LLM capability to an actual labeled dataset that will grow and hopefully become useful for downstream tasks like evaluations, fine-tuning, and more.

## Overview

Before we start, I'll walk through a hypothetical but realistic example that demonstrates a common capability of modern LLMs. We want to use the latest models with vision and language components and leverage the vision capability to generate captions.

Why?!

From a business perspective, it can be valuable to take screenshots of UI elements and generate HTML, or capture screenshots of websites to generate code, or even search for sites that "look" similar. This turns out to be genuinely useful and therefore valuable.

Your job is to create an LLM that, given a screenshot of a UI element, can output a caption/description of that image.

At a high level, we'll follow these steps:

1. Define the task and capabilities of the LLM (tasks, capabilities, input → output paradigm)
2. Review what kind of data we need or have (collect, synthesize, etc.)
3. Create scripts and pipelines to collect, process, clean, and store data
4. Label your data and publish your dataset

Finally, we'll review what we learned and have a follow-up discussion about the evolution of your dataset. Like most things, it's constantly evolving, and there might be new and potentially better ways of doing things.

image.png
Let's begin!

## What do we want the LLM to do?

Even though LLMs are amazing machines that can take instructions and output something incredibly useful, they still make mistakes and can be unreliable. The first big mistake is thinking these models are useless, which is simply not true—they have demonstrated they're extremely useful. The second biggest mistake is assuming they can do everything. I would dearly wish they could zero-shot startups, but that's just not the case.

Let's get practical. For this task, it's quite simple: we want to give an LLM an image (a screenshot of some UI element on a website) and have it output a description of the image. That's it. We'll use a fixed input prompt, with the image being the only variable.

Let's visualize what we're trying to achieve with a simple diagram:

![A simple diagram showing the input (screenshot of UI element) going into an LLM with vision capabilities, and the output being a text description of that UI element](/images/how_to_create_a_visual_dataset_from_session_recordings/simple-llm-diagram.png)

This is a straightforward task: given an image input, generate a descriptive text output. The model needs to understand UI elements, their relationships, and be able to describe them accurately in natural language.

## Show me the data

Now that we've established the task, a natural question to ask yourself is: what data will effectively represent the tasks I want the LLM to perform? Much like teaching people, you need to provide a collection of examples with inputs and expected outputs.

Before we go any further, it's good to know where you can get some free data.

[insert meme about it's free data]

You can use Hugging Face, which hosts many excellent datasets that are already public and open-source. Alternatively, if you have raw datasets lying around from previous projects or jobs, it might be worth cleaning them up.

I had a bunch of rrweb JSON data that I converted into images, so I'll use that.

You can click [here](/posts/2025-03-09_how_to_turn_rrweb_jsons_to_videos_images_and_snapshots.qmd) to see how I created these images from rrweb.

## Process and clean your data

In this case, I had screenshots of two UI elements. Since I wanted to augment my data, I created a dataset from these two images.

Let me show you some examples of the data I worked with:

![Screenshot of a website dashboard interface with navigation menu on the left and content area on the right](/images/how_to_create_a_visual_dataset_from_session_recordings/ui-image1.png)

![Screenshot of a form interface with input fields and submit button](/images/how_to_create_a_visual_dataset_from_session_recordings/ui-image2.png)



This variety helps ensure the model can generalize across different UI contexts and isn't overfitting to specific visual patterns.

The simplest solution (though potentially time-consuming—but remember, premature optimization is the root of all evil) is to start labeling them by hand. You really only need around 10 examples from each UI screenshot. Depending on how many types of UI elements you have, you can collect anywhere from 20 to 100 examples. Don't go overboard; it's important to start small and grow once you've established a pipeline.

![Sample screenshots from my dataset showing UI elements with red bounding boxes highlighting specific components. The left shows a navigation menu, and the right shows a form input field.](/images/how_to_create_a_visual_dataset_from_session_recordings/sample_visualizations.png)

Here you can see me visiting a site and using the developer tools to create a red box or frame around the element I wanted to capture. This visual cue was added to help the LLM know where to focus its responses. This isn't strictly necessary, but I thought it might help. At the end of the day, data is what you make of it. You do hope it roughly aligns with how people will actually use it.

## Label and publish

Once you have a folder full of images, it's time to focus on the labeling or output side. This is where you'll apply your human intuition and superior intellect. You need to write out the expected outputs for these LLMs—reference answers that you claim are the "right" answers. You have to be opinionated. It's your job to know the difference between good and bad, right and wrong. The line may be blurry, but you must hold conviction and draw that line.

```python
image_path,original_path,image_hash,snapshot_id,timestamp,element_id,snapshot_name,metadata,captions
example_image_1.png,example_original_1.png,hash123,1,1678886400,1,snapshot_1,"{width: 800, height: 600}","{model_1: caption_1, model_2: caption_2}"
example_image_2.png,example_original_2.png,hash456,2,1678890000,2,snapshot_2,"{width: 1024, height: 768}","{model_1: caption_3, model_3: caption_4}"
example_image_3.png,example_original_3.png,hash789,1,1678886400,3,snapshot_1,"{width: 800, height: 600}","{model_2: caption_5, model_4: caption_6}"
```


You can literally do this in Excel if needed, but I created a notebook that used an existing LLM to label the images for me. I then modified the labels if I didn't like them. I'm not saying you have to do all the work yourself, but you do need to give your stamp of approval.

And finialy you push it hugging face with the following code.

```python
# Install the Hugging Face datasets library if you haven't already
!pip install datasets

from datasets import Dataset
import pandas as pd
import os

# Assuming you have your data in a pandas DataFrame or CSV file
df = pd.read_csv('your_labeled_data.csv')

# Convert to Hugging Face Dataset
dataset = Dataset.from_pandas(df)

# Push to Hugging Face Hub
dataset.push_to_hub("your-username/your-dataset-name", private=False)
```


Once your dataset is published to Hugging Face Hub, it will be available for others to use and contribute to. The platform provides version control, documentation features, and community engagement metrics to help you track how your dataset is being used.

![Screenshot of the Hugging Face Hub interface showing a published dataset with metadata, files, and community metrics](/images/how_to_create_a_visual_dataset_from_session_recordings/huggingface-hub.png)

## Conclusion

That's it! You now have a dataset in Hugging Face that you can use as a building block for the next steps like evaluations, fine-tuning, and training.
