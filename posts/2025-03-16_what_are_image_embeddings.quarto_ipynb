{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "aliases: [\"/what-are-image-embeddings/\"]\n",
        "categories: [\"Computer Vision\", \"Machine Learning\"]\n",
        "date: \"2025-03-16\"\n",
        "image: \"/images/what_are_image_embeddings/thumbnail.png\"\n",
        "title: \"What are Image Embeddings?\"\n",
        "subtitle: \"Understanding how images are represented as numerical vectors for AI applications\"\n",
        "colab: '<a href=\"https://colab.research.google.com/drive/1T66Ae_EcUo7KqcQcuAftcJ1oJiVZv5YO?usp=sharing\"><img src=\"images/colab.svg\" alt=\"Open In Colab\"></a>'\n",
        "format: \"html\"\n",
        "---\n",
        "\n",
        "\n",
        "This notebook explores the concept of image embeddings, how they work, and their applications in AI. We'll focus on Google's SigLIP 2, a state-of-the-art multilingual vision-language encoder, and demonstrate its practical applications through visualization, clustering, and text-image similarity analysis.\n",
        "\n",
        "> **Note:** The complete code for this article is available in this [Colab notebook](https://colab.research.google.com/drive/1T66Ae_EcUo7KqcQcuAftcJ1oJiVZv5YO?usp=sharing).\n",
        "\n",
        "## Introduction\n",
        "Image embeddings are numerical representations of images that capture their semantic content in a way that's useful for machine learning algorithms[^1]. At their core, embeddings are dense vectors—typically consisting of hundreds or thousands of floating-point numbers—that represent images in a high-dimensional space where similar images are positioned close to each other[^2].\n",
        "\n",
        "### Why Do We Need Image Embeddings?\n",
        "\n",
        "Images in their raw pixel form are:\n",
        "\n",
        "- **High-dimensional**: A 224x224 RGB image contains 150,528 pixel values\n",
        "- **Not semantically organized**: Similar-looking images might have very different pixel values\n",
        "- **Difficult to work with**: Comparing raw pixels doesn't capture semantic similarity\n",
        "\n",
        "Embeddings solve these problems by:\n",
        "\n",
        "- **Reducing dimensionality**: Typically to a few hundred or thousand dimensions\n",
        "- **Capturing semantics**: Images with similar content have similar embeddings\n",
        "- **Enabling efficient search**: Finding similar images becomes a vector similarity search[^3]\n",
        "- **Supporting transfer learning**: Pre-trained embeddings can be used for various downstream tasks[^4]\n",
        "\n",
        "[^1]: Bengio, Y., Courville, A., & Vincent, P. (2013). Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8), 1798-1828. https://doi.org/10.1109/TPAMI.2013.50\n",
        "\n",
        "[^2]: Pan, S. J., & Yang, Q. (2009). A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10), 1345-1359. https://doi.org/10.1109/TKDE.2009.191\n",
        "\n",
        "[^3]: Johnson, J., Douze, M., & Jégou, H. (2019). Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3), 535-547. https://doi.org/10.1109/TBDATA.2019.2921572\n",
        "\n",
        "[^4]: He, K., Girshick, R., & Dollár, P. (2018). Rethinking ImageNet pre-training. Proceedings of the IEEE/CVF International Conference on Computer Vision, 4918-4927. https://arxiv.org/abs/1811.08883\n",
        "\n",
        "## How Image Embeddings Work\n",
        "\n",
        "Modern image embeddings are typically created using deep neural networks, particularly convolutional neural networks (CNNs)[^5] or vision transformers (ViTs)[^6]. These networks learn to transform raw pixels into compact, semantically meaningful representations through extensive training on large datasets.\n",
        "\n",
        "![Vision Transformer Architecture](https://i.imgur.com/n0vrUs8.png)\n",
        "*Figure 2: Vision Transformer (ViT) architecture. The image is divided into patches which are linearly embedded, positional encodings are added, and the resulting sequence is processed by a standard Transformer encoder. This approach allows transformers to effectively process visual information similarly to how they handle text. Adapted from Dosovitskiy et al. (2021)[^6].*\n",
        "\n",
        "The process generally involves:\n",
        "\n",
        "1. **Training**: Neural networks are trained on large image datasets, often using self-supervised or weakly-supervised learning approaches[^7]\n",
        "2. **Feature extraction**: The trained network processes an image through its layers\n",
        "3. **Embedding generation**: The network's final or penultimate layer outputs become the embedding vector\n",
        "\n",
        "These embeddings can then be used for various tasks:\n",
        "\n",
        "- **Image similarity**: Finding visually or semantically similar images\n",
        "- **Image classification**: Categorizing images into predefined classes\n",
        "- **Image retrieval**: Finding relevant images based on text queries\n",
        "- **Zero-shot learning**: Recognizing objects the model wasn't explicitly trained on[^8]\n",
        "- **Transfer learning**: Using pre-trained embeddings for new tasks with limited data\n",
        "\n",
        "[^5]: Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems, 25. https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
        "\n",
        "[^6]: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., & Houlsby, N. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations. https://arxiv.org/abs/2010.11929\n",
        "\n",
        "[^7]: Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020). A simple framework for contrastive learning of visual representations. International Conference on Machine Learning, 1597-1607. https://arxiv.org/abs/2002.05709\n",
        "\n",
        "[^8]: Xian, Y., Lampert, C. H., Schiele, B., & Akata, Z. (2018). Zero-shot learning—A comprehensive evaluation of the good, the bad and the ugly. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(9), 2251-2265. https://arxiv.org/abs/1707.00600\n",
        "\n",
        "## SigLIP 2: Google's Advanced Multilingual Vision-Language Encoder\n",
        "\n",
        "SigLIP 2 represents the latest advancement in image embedding technology[^9]. Developed by Google and released in early 2024, it significantly improves upon its predecessor by offering enhanced semantic understanding, better localization capabilities, and more effective dense feature representation.\n",
        "\n",
        "[^9]: Beyer, L., Dehghani, M., et al. (2024). SigLIP 2: Next-Generation Multilingual Vision-Language Models. Google Research. https://arxiv.org/abs/2409.01936\n",
        "\n",
        "### Technical Background and Evolution\n",
        "\n",
        "#### From CLIP to SigLIP to SigLIP 2\n",
        "\n",
        "Vision-language models have evolved considerably in recent years:\n",
        "\n",
        "1. **CLIP and ALIGN**: These pioneered the approach of jointly training image and text encoders to understand the semantic relationship between visual data and natural language[^10]\n",
        "\n",
        "![Contrast function comparison between CLIP and SigLIP](https://i.imgur.com/GH9sai5.png)\n",
        "*Figure 1: Comparison of contrast functions in CLIP (contrastive loss) and SigLIP (sigmoid loss). Adapted from Zhai et al. (2023).*\n",
        "\n",
        "2. **SigLIP (1st generation)**: Improved upon CLIP by replacing its contrastive loss function with a simpler pairwise sigmoid loss[^11]. Instead of requiring a global view of pairwise similarities for normalization (as in contrastive learning), the sigmoid loss operated only on image-text pairs, allowing for better scaling and improved performance even with smaller batch sizes\n",
        "\n",
        "3. **SigLIP 2**: Extends this foundation by incorporating several additional training techniques into a unified recipe, creating more powerful and versatile vision-language encoders that outperform their predecessors across all model scales[^12]\n",
        "\n",
        "[^10]: Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G. (2021). Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (pp. 8748-8763). PMLR. https://arxiv.org/abs/2103.00020\n",
        "\n",
        "[^11]: Zhai, X., Wang, X., Mustafa, B., Steiner, A., Keysers, D., Kolesnikov, A., & Beyer, L. (2023). Sigmoid loss for language image pre-training. In Proceedings of the 40th International Conference on Machine Learning (pp. 40844-40858). PMLR. https://arxiv.org/abs/2303.15343\n",
        "\n",
        "[^12]: Google. (2024). SigLIP 2 - GitHub Documentation. https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/README_siglip2.md\n",
        "\n",
        "### How SigLIP 2 Works\n",
        "\n",
        "#### Enhanced Training Methodology\n",
        "\n",
        "SigLIP 2's functioning is fundamentally based on its innovative training approach that combines multiple previously independent techniques[^13]:\n",
        "\n",
        "1. **Extended Training Objectives**: While preserving the original sigmoid loss function, SigLIP 2 integrates several additional training objectives:\n",
        "   - Captioning-based pretraining to enhance semantic understanding\n",
        "   - Self-supervised losses including self-distillation and masked prediction\n",
        "   - Online data curation for improved quality and diversity of training examples\n",
        "\n",
        "2. **Multilingual Capabilities**: The model is trained on a more diverse data mixture that incorporates de-biasing techniques, leading to significantly better multilingual understanding and improved fairness across different languages and cultures[^14]\n",
        "\n",
        "3. **Technical Implementation**: SigLIP 2 models use the Gemma tokenizer with a vocabulary size of 256,000 tokens, allowing for better representation of diverse languages[^15]\n",
        "\n",
        "[^13]: Google Research. (2024). SigLIP 2: Multilingual vision-language models with self-supervised learning. GitHub. https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/README_siglip2.md\n",
        "\n",
        "[^14]: Google. (2024). SigLIP 2 Technical Report. https://huggingface.co/papers/2502.14786\n",
        "\n",
        "[^15]: Google. (2024). Gemma Tokenizer. Hugging Face. https://huggingface.co/google/gemma-tokenizer\n",
        "\n",
        "#### Beyond Simple Cosine Similarity: Advanced Similarity Computation\n",
        "\n",
        "While many discussions of image embeddings focus on simple cosine similarity between vectors, SigLIP 2's similarity computation is actually much more sophisticated[^16]. This advanced approach leads to more accurate and nuanced similarity scores:\n",
        "\n",
        "1. **Multi-head Attention Pooling (MAP)**: Unlike simpler models that use average pooling to aggregate token representations, SigLIP 2 employs a more sophisticated attention-based pooling mechanism[^17]:\n",
        "   - The MAP head learns to focus on the most relevant parts of the image or text\n",
        "   - It assigns different weights to different regions or tokens based on their importance\n",
        "   - This selective attention mechanism produces more contextually relevant embeddings that capture important details while ignoring noise\n",
        "\n",
        "2. **Temperature Scaling**: SigLIP 2 applies a learned temperature parameter (τ) to scale similarity scores[^18]:\n",
        "   - Raw cosine similarities are divided by this temperature: sim(i,j)/τ\n",
        "   - Lower temperature values make the distribution more \"peaked,\" emphasizing differences between high and low similarity pairs\n",
        "   - Higher temperature values make the distribution more uniform\n",
        "   - The temperature parameter is learned during training to optimize the model's discrimination ability\n",
        "\n",
        "3. **Bias Term Adjustment**: The similarity calculation includes a learned bias term:\n",
        "   - sim'(i,j) = sim(i,j)/τ + b, where b is the learned bias\n",
        "   - This bias helps counteract the inherent imbalance between positive and negative pairs during training\n",
        "   - It acts as a calibration factor, adjusting the similarity scores to better reflect true semantic relationships\n",
        "\n",
        "4. **Sigmoid Activation**: Unlike models that use softmax normalization (like CLIP), SigLIP 2 applies a sigmoid function to the adjusted similarity scores:\n",
        "   - p(i,j) = sigmoid(sim'(i,j)) = 1/(1+exp(-(sim(i,j)/τ + b)))\n",
        "   - This transforms the unbounded similarity scores into well-calibrated probability-like values in the range [0,1]\n",
        "   - The sigmoid function allows each image-text pair to be evaluated independently, which is more appropriate for retrieval tasks\n",
        "\n",
        "These components work together to ensure that SigLIP 2's similarity calculations go far beyond simple vector dot products. When using SigLIP 2, it's crucial to use the model's built-in comparison mechanism (`logits_per_image` followed by sigmoid activation) rather than manually computing cosine similarity on raw embeddings, as the former incorporates all these learned parameters and transformations that were optimized during training[^19].\n",
        "\n",
        "[^16]: Hugging Face. (2024). SigLIP 2 Model Documentation. https://huggingface.co/docs/transformers/en/model_doc/siglip2\n",
        "\n",
        "[^17]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems (pp. 5998-6008). https://arxiv.org/abs/1706.03762\n",
        "\n",
        "[^18]: Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531. https://arxiv.org/abs/1503.02531\n",
        "\n",
        "[^19]: Lukyanenko, A. (2024). Paper Review: SigLIP 2 - Multilingual Vision-Language Dense Encoder. https://www.linkedin.com/pulse/paper-review-siglip-2-multilingual-vision-language-dense-lukyanenko-7cvyf\n",
        "\n",
        "#### Architecture Variants\n",
        "\n",
        "SigLIP 2 is available in several architectural variants to accommodate different computational constraints and use cases[^20]:\n",
        "\n",
        "1. **Model Sizes**: The family includes four primary model sizes:\n",
        "   - ViT-B (86M parameters)\n",
        "   - ViT-L (303M parameters)\n",
        "   - ViT-So400m (400M parameters)\n",
        "   - ViT-g (1B parameters)\n",
        "\n",
        "2. **NaFlex Variants**: One of the most significant innovations in SigLIP 2 is the introduction of NaFlex variants, which support dynamic resolution and preserve the input's native aspect ratio[^21]. This feature is particularly valuable for:\n",
        "   - Optical character recognition (OCR)\n",
        "   - Document understanding\n",
        "   - Any task where preserving the original aspect ratio and resolution is important\n",
        "\n",
        "[^20]: Google. (2024). SigLIP 2 Model Collection. Hugging Face. https://huggingface.co/models?search=google%2Fsiglip2\n",
        "\n",
        "[^21]: Google. (2024). SigLIP 2 Gemma Toolkit. Google Developers Blog. https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/\n",
        "\n",
        "### Key Capabilities and Improvements\n",
        "\n",
        "SigLIP 2 models demonstrate significant improvements over the original SigLIP across several dimensions:\n",
        "\n",
        "1. **Core Capabilities**: The models outperform their SigLIP counterparts at all scales in:\n",
        "   - Zero-shot classification\n",
        "   - Image-text retrieval\n",
        "   - Transfer performance when used for visual representation in Vision-Language Models (VLMs)\n",
        "\n",
        "2. **Localization and Dense Features**: The enhanced training recipe leads to substantial improvements in localization and dense prediction tasks, making the models more effective for detailed visual understanding\n",
        "\n",
        "3. **Multilingual Understanding**: Through its diverse training data and de-biasing techniques, SigLIP 2 achieves much better multilingual understanding and improved fairness compared to previous models\n",
        "\n",
        "### Practical Applications\n",
        "\n",
        "The improvements in SigLIP 2 make it particularly well-suited for:\n",
        "\n",
        "1. **Zero-shot Image Classification**: Using the model to classify images into categories it wasn't explicitly trained on\n",
        "\n",
        "2. **Image-Text Retrieval**: Finding relevant images based on text queries or finding appropriate textual descriptions for images\n",
        "\n",
        "3. **Feature Extraction for VLMs**: Providing high-quality visual representations that can be combined with large language models to build more capable vision-language models\n",
        "\n",
        "4. **Document and Text-Heavy Image Analysis**: Particularly with the NaFlex variants, which excel at tasks requiring preservation of aspect ratio and resolution\n",
        "\n",
        "## Practical Applications of Image Embeddings\n",
        "\n",
        "Now that we understand the theoretical background of image embeddings, let's explore their practical applications. Image embeddings form the foundation for numerous computer vision tasks and enable powerful capabilities like semantic search, clustering, and cross-modal understanding.\n",
        "\n",
        "### Key Applications of Image Embeddings\n",
        "\n",
        "1. **Visual Similarity Search**: Find visually similar images based on embedding distance\n",
        "2. **Image Clustering**: Group images by semantic content without explicit labels\n",
        "3. **Cross-Modal Understanding**: Connect images with text descriptions\n",
        "4. **Fine-Grained Recognition**: Identify specific attributes and details\n",
        "5. **Transfer Learning**: Apply pre-trained embeddings to new, domain-specific tasks\n",
        "\n",
        "SigLIP 2, with its powerful multilingual capabilities and improved semantic understanding, enables these applications with state-of-the-art performance. While SigLIP 2 comes in various sizes (Base, Large, So400m, and Giant) and configurations, we'll focus on the So400m model, which provides an excellent balance of quality and efficiency.\n",
        "\n",
        "## Implementing SigLIP 2: Practical Examples\n",
        "\n",
        "Now that we understand the theoretical background of image embeddings and SigLIP 2, let's implement it to see how it works in practice. We'll use the Hugging Face Transformers library, which provides easy access to SigLIP 2 models.\n",
        "\n",
        "### Resources for Following Along\n",
        "\n",
        "To follow along with these examples, you'll need access to these resources:\n",
        "\n",
        "- **SigLIP 2 on Hugging Face**: [google/siglip2-so400m-patch14-384](https://huggingface.co/google/siglip2-so400m-patch14-384)\n",
        "- **Official Documentation**: [GitHub - SigLIP 2 README](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/README_siglip2.md)\n",
        "- **Zero-Shot Classification Guide**: [Hugging Face Documentation](https://huggingface.co/docs/transformers/en/tasks/zero_shot_image_classification)\n",
        "- **Required Python Libraries**: \n",
        "  - [Transformers](https://huggingface.co/docs/transformers/index)\n",
        "  - [PyTorch](https://pytorch.org/docs/stable/index.html)\n",
        "  - [UMAP-Learn](https://umap-learn.readthedocs.io/en/latest/)\n",
        "  - [Scikit-learn](https://scikit-learn.org/stable/)\n",
        "- **Recommended Environment**: Python 3.8+ with GPU support\n"
      ],
      "id": "2fad5b02"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import requests  # For fetching images from URLs: https://docs.python-requests.org/\n",
        "import numpy as np  # For numerical operations: https://numpy.org/doc/stable/\n",
        "import matplotlib.pyplot as plt  # For visualization: https://matplotlib.org/stable/\n",
        "import torch  # PyTorch deep learning framework: https://pytorch.org/docs/stable/\n",
        "from PIL import Image  # For image processing: https://pillow.readthedocs.io/\n",
        "from sklearn.cluster import KMeans  # For clustering: https://scikit-learn.org/stable/modules/clustering.html\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import pipeline, AutoModel, AutoProcessor  # Hugging Face Transformers: https://huggingface.co/docs/transformers/\n",
        "from transformers.image_utils import load_image"
      ],
      "id": "c69722d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the SigLIP 2 Model\n",
        "\n",
        "We'll use the So400m variant of SigLIP 2 for our examples, which offers an excellent balance of quality and efficiency. The most recent models are available with the \"google/siglip2-\" prefix.\n"
      ],
      "id": "e3aa2c93"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We'll use the SO400M model which offers good performance\n",
        "model_name = \"google/siglip2-so400m-patch14-384\"\n",
        "\n",
        "# Define a function to extract embeddings from an image\n",
        "def get_image_embedding(image_path_or_url, model, processor):\n",
        "    \"\"\"Extract embeddings from an image file or URL\n",
        "    \n",
        "    NOTE: For most SigLIP applications, you should NOT extract embeddings separately.\n",
        "    Instead, use the model to process image-text pairs together via model(**inputs)\n",
        "    to get direct similarity scores through the model's logits_per_image.\n",
        "    \n",
        "    This function is provided for educational purposes or for specific use cases\n",
        "    where you need the raw embeddings.\n",
        "    \"\"\"\n",
        "    # Load image from URL or local path\n",
        "    if isinstance(image_path_or_url, str):\n",
        "        if image_path_or_url.startswith(('http://', 'https://')):\n",
        "            image = Image.open(requests.get(image_path_or_url, stream=True).raw)\n",
        "        else:\n",
        "            image = Image.open(image_path_or_url)\n",
        "    else:\n",
        "        # Assuming it's already a PIL Image\n",
        "        image = image_path_or_url\n",
        "    \n",
        "    # Process image and extract embedding\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Just get image features directly\n",
        "        image_embedding = model.get_image_features(**inputs)\n",
        "        image_embedding = image_embedding / image_embedding.norm(dim=1, keepdim=True)\n",
        "    \n",
        "    return image_embedding.squeeze().detach().numpy(), image"
      ],
      "id": "71b87c09",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Zero-Shot Image Classification\n",
        "\n",
        "Let's use SigLIP 2 for zero-shot image classification. We'll load an image and classify it against different text prompts.\n"
      ],
      "id": "f0d54f73"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set up the zero-shot classification pipeline\n",
        "from transformers import pipeline\n",
        "from PIL import Image\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# SigLIP 2 uses the Gemma tokenizer which requires specific parameters\n",
        "pipe = pipeline(\n",
        "    model=model_name, \n",
        "    task=\"zero-shot-image-classification\",\n",
        ")\n",
        "\n",
        "inputs = {\n",
        "    \"images\": [\n",
        "        \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000285.jpg\", # bear\n",
        "        \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000776.jpg\", # teddy bear\n",
        "    ],\n",
        "    \"texts\": [\n",
        "        \"bear looking into the camera\",\n",
        "        \"bear looking away from the camera\",\n",
        "        \"a bunch of teddy bears\",\n",
        "        \"two teddy bears\",\n",
        "        \"three teddy bears\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Load images for display\n",
        "display_images = []\n",
        "for img_url in inputs[\"images\"]:\n",
        "    img = Image.open(requests.get(img_url, stream=True).raw)\n",
        "    display_images.append(img)\n",
        "\n",
        "outputs = pipe(inputs[\"images\"], candidate_labels=inputs[\"texts\"])\n",
        "\n",
        "# Display the outputs\n",
        "for i, output in enumerate(outputs):\n",
        "    print(f\"Image {i+1} results:\")\n",
        "    for result in output:\n",
        "        print(f\"{result['label']}: {result['score']:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Visualize the results with images on top\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 8), gridspec_kw={'height_ratios': [0.6, 1]})\n",
        "\n",
        "# Display the images in the top row\n",
        "for i, img in enumerate(display_images):\n",
        "    # Use 'equal' instead of 'auto' to maintain the correct aspect ratio\n",
        "    axes[0, i].imshow(img, aspect='equal')\n",
        "    axes[0, i].set_title(f\"Image {i+1}\")\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "# Display the classification results in the bottom row\n",
        "for i, output in enumerate(outputs):\n",
        "    labels = [result['label'] for result in output]\n",
        "    scores = [result['score'] for result in output]\n",
        "    \n",
        "    axes[1, i].bar(range(len(labels)), scores)\n",
        "    axes[1, i].set_xticks(range(len(labels)))\n",
        "    axes[1, i].set_xticklabels(labels, rotation=45, ha='right')\n",
        "    axes[1, i].set_ylim(0, 1)\n",
        "    axes[1, i].set_title(f\"Image {i+1} Classification Results\")\n",
        "    axes[1, i].set_ylabel(\"Probability\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "f1777dfa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Image-Text Similarity\n",
        "\n",
        "Now let's explore how we can use SigLIP 2 to compute similarity between multiple images and texts.\n"
      ],
      "id": "bddd116c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the model and processor\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Define a set of sample images from COCO dataset for demonstration\n",
        "image_urls = [\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000285.jpg\",  # bear\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000632.jpg\",  # train\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000724.jpg\",  # umbrella\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000776.jpg\",  # teddy bear\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000785.jpg\",  # clock\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000802.jpg\",  # train\n",
        "]\n",
        "\n",
        "# Extract embeddings and store images\n",
        "embeddings = []\n",
        "images = []\n",
        "for i, url in enumerate(image_urls[:3]):  # Limiting to first 3 images to save time\n",
        "    print(f\"Processing image {i+1}/{len(image_urls[:3])}: {url}\")\n",
        "    embedding, image = get_image_embedding(url, model, processor)\n",
        "    embeddings.append(embedding)\n",
        "    images.append(image)\n",
        "\n",
        "# Convert to numpy array for further processing\n",
        "embeddings = np.array(embeddings)\n",
        "print(f\"Embedded {len(embeddings)} images. Embedding shape: {embeddings.shape}\")\n",
        "\n",
        "# Display the images\n",
        "fig, axes = plt.subplots(1, len(images), figsize=(15, 5))\n",
        "for i, (image, ax) in enumerate(zip(images, axes)):\n",
        "    ax.imshow(image, aspect='equal')\n",
        "    ax.set_title(f\"Image {i+1}\")\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Text descriptions\n",
        "texts = [\n",
        "    \"a wild bear\",\n",
        "    \"a train on tracks\",\n",
        "    \"a person with an umbrella\",\n",
        "    \"a child's toy\",\n",
        "    \"a stop sign\",\n",
        "    \"a picture of a bedroom\",\n",
        "    \"Cozy bedroom retreat filled with books, plants, and warm natural light\",\n",
        "    \"a picture of a timepiece\",\n",
        "    \"a picture of a vehicle for transportation\"\n",
        "]\n",
        "\n",
        "# Get text embeddings using the processor and model\n",
        "def get_text_embedding(text, model, processor):\n",
        "    \"\"\"Extract text embedding from a text string\n",
        "    \n",
        "    NOTE: For most SigLIP applications, you should NOT extract embeddings separately.\n",
        "    Instead, use the model to process image-text pairs together via model(**inputs)\n",
        "    to get direct similarity scores through the model's logits_per_image.\n",
        "    \n",
        "    This function is provided for educational purposes or for specific use cases\n",
        "    where you need the raw embeddings.\n",
        "    \"\"\"\n",
        "    inputs = processor(text=text, return_tensors=\"pt\", padding=True)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Just get text features directly\n",
        "        text_embedding = model.get_text_features(**inputs)\n",
        "        text_embedding = text_embedding / text_embedding.norm(dim=1, keepdim=True)\n",
        "    \n",
        "    return text_embedding.squeeze().detach().numpy()\n",
        "\n",
        "# Get embeddings for the text queries\n",
        "text_embeddings = []\n",
        "for i, query in enumerate(texts):\n",
        "    print(f\"Processing text {i+1}/{len(texts)}: '{query}'\")\n",
        "    text_embeddings.append(get_text_embedding(query, model, processor))\n",
        "text_embeddings = np.array(text_embeddings)\n",
        "print(f\"Embedded {len(text_embeddings)} text queries. Embedding shape: {text_embeddings.shape}\")\n",
        "print(\"NOTE: While we extracted text embeddings separately, for similarity calculations\")\n",
        "print(\"we'll use the model's native capability to process image-text pairs together\")"
      ],
      "id": "799b47a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Embeddings: A Closer Look at the Numbers\n",
        "\n",
        "What exactly are these embedding vectors we've been generating? Let's take a closer look at what these numbers actually represent:\n",
        "\n",
        "#### Anatomy of an Embedding Vector\n",
        "\n",
        "Both image and text embeddings in SigLIP 2 are **1152-dimensional vectors** - essentially long lists of 1152 floating-point numbers. Each number typically ranges from -1 to 1 after normalization. These numbers represent:\n",
        "\n",
        "- **For images**: Abstract visual features like shapes, textures, objects, spatial arrangements, and semantic concepts\n",
        "- **For text**: Linguistic features, semantic meanings, and conceptual relationships between words\n",
        "\n",
        "#### Reading the Numbers\n",
        "\n",
        "When you look at an embedding vector like `[0.1253, -0.0891, 0.0332, ...]`:\n",
        "\n",
        "- **Each position** (dimension) captures a specific latent feature that the model learned during training\n",
        "- **The value** at each position indicates how strongly that feature is present in the image or text\n",
        "- **Positive vs. negative values** represent different aspects of the same feature dimension\n",
        "- **The magnitude** (absolute value) shows the strength of that feature's presence\n",
        "\n",
        "#### Pattern Recognition\n",
        "\n",
        "Two similar images (like two different bears) will have similar patterns in their embedding vectors because:\n",
        "\n",
        "- They share many of the same visual features\n",
        "- The model has learned to map similar semantic content to similar regions in the embedding space\n",
        "\n",
        "This is why a photo of a bear and the text \"a wild bear\" would have some similarities in their embedding patterns, despite being different modalities.\n",
        "\n",
        "#### Dimensionality\n",
        "\n",
        "Why 1152 dimensions? This specific size represents a balance between:\n",
        "\n",
        "- Being **large enough** to capture complex visual and textual nuances\n",
        "- Being **small enough** to be computationally efficient (compared to raw pixels)\n",
        "- Following the **architectural decisions** made when designing the ViT (Vision Transformer) backbone\n",
        "\n",
        "When we visualize only the first 10 dimensions below, we're seeing just a tiny slice (less than 1%) of the full representation, but it gives us an intuitive sense of how these embeddings work.\n"
      ],
      "id": "56243bc2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualizing truncated embeddings to better understand their structure\n",
        "print(\"Displaying truncated embeddings to visualize their structure:\")\n",
        "\n",
        "# Function to display truncated embedding values\n",
        "def display_truncated_embedding(embedding, title, n_values=10):\n",
        "    \"\"\"Format and display a truncated embedding vector\"\"\"\n",
        "    truncated = embedding[:n_values]\n",
        "    formatted = [f\"{value:.4f}\" for value in truncated]\n",
        "    print(f\"\\n{title} embedding (first {n_values} values):\")\n",
        "    print(\"[\" + \", \".join(formatted) + \", ...]\")\n",
        "    print(f\"Shape: {embedding.shape} (full embedding)\")\n",
        "    return truncated\n",
        "\n",
        "# Visualize the first few values of each image embedding\n",
        "print(\"\\n=== IMAGE EMBEDDINGS ===\")\n",
        "for i, embedding in enumerate(embeddings):\n",
        "    display_truncated_embedding(embedding, f\"Image {i+1}\")\n",
        "\n",
        "# Visualize the first few values of select text embeddings\n",
        "print(\"\\n=== TEXT EMBEDDINGS ===\")\n",
        "for i, text in enumerate(texts[:5]):  # Just show first 5 text embeddings\n",
        "    display_truncated_embedding(text_embeddings[i], f\"'{text}'\")\n",
        "\n",
        "# Create a visual representation of embeddings alongside images\n",
        "fig, axes = plt.subplots(len(images), 2, figsize=(12, 4*len(images)), \n",
        "                         gridspec_kw={'width_ratios': [1, 2]})\n",
        "\n",
        "for i, (image, embedding) in enumerate(zip(images, embeddings)):\n",
        "    # Display the image\n",
        "    axes[i, 0].imshow(image, aspect='equal')\n",
        "    axes[i, 0].set_title(f\"Image {i+1}\")\n",
        "    axes[i, 0].axis('off')\n",
        "    \n",
        "    # Display a truncated embedding as a bar chart\n",
        "    truncated = embedding[:10]  # First 10 values\n",
        "    axes[i, 1].bar(range(len(truncated)), truncated)\n",
        "    axes[i, 1].set_title(f\"Truncated Embedding (first 10 of {len(embedding)} values)\")\n",
        "    axes[i, 1].set_xlabel(\"Dimension\")\n",
        "    axes[i, 1].set_ylabel(\"Value\")\n",
        "    axes[i, 1].set_ylim(-0.5, 0.5)  # Set consistent y limits\n",
        "    \n",
        "    # Add text annotation\n",
        "    embedding_text = \", \".join([f\"{x:.3f}\" for x in truncated[:5]]) + \"...\"\n",
        "    axes[i, 1].text(0.5, 0.9, f\"[{embedding_text}]\", \n",
        "                   transform=axes[i, 1].transAxes, \n",
        "                   ha='center', va='center',\n",
        "                   bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Also visualize a few text embeddings for comparison\n",
        "fig, axes = plt.subplots(3, 1, figsize=(10, 6))\n",
        "text_indices = [0, 1, 2]  # First 3 text embeddings\n",
        "\n",
        "for i, idx in enumerate(text_indices):\n",
        "    text = texts[idx]\n",
        "    embedding = text_embeddings[idx]\n",
        "    truncated = embedding[:10]  # First 10 values\n",
        "    \n",
        "    axes[i].bar(range(len(truncated)), truncated)\n",
        "    axes[i].set_title(f\"Text: '{text}'\")\n",
        "    axes[i].set_xlabel(\"Dimension\")\n",
        "    axes[i].set_ylabel(\"Value\")\n",
        "    axes[i].set_ylim(-0.5, 0.5)  # Set consistent y limits\n",
        "    \n",
        "    # Add text annotation\n",
        "    embedding_text = \", \".join([f\"{x:.3f}\" for x in truncated[:5]]) + \"...\"\n",
        "    axes[i].text(0.5, 0.9, f\"[{embedding_text}]\", \n",
        "                 transform=axes[i].transAxes, \n",
        "                 ha='center', va='center',\n",
        "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "1513ceef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpreting the Embedding Visualizations\n",
        "\n",
        "Looking at the truncated embedding visualizations above, we can make several important observations:\n",
        "\n",
        "#### What We're Seeing\n",
        "\n",
        "The bar charts show the first 10 dimensions of embedding vectors that are actually 1152 dimensions long. Think of these as the first few \"notes\" in a much longer \"melody\" that represents each image or text.\n",
        "\n",
        "#### Image Embedding Patterns\n",
        "\n",
        "In the image embeddings above:\n",
        "\n",
        "1. **Different images have different patterns** - Notice how the bear image has a different pattern of positive and negative values compared to the room or stop sign\n",
        "\n",
        "2. **Magnitude variations** - Some dimensions have larger values than others, indicating their importance in representing the image\n",
        "\n",
        "3. **Sign patterns** - The pattern of positive and negative values across dimensions forms a unique \"signature\" for each image\n",
        "\n",
        "#### Text Embedding Patterns\n",
        "\n",
        "For the text embeddings:\n",
        "\n",
        "1. **Semantic encoding** - Each text query (\"a wild bear\", \"a train on tracks\", etc.) produces a unique pattern reflecting its semantic meaning\n",
        "\n",
        "2. **Comparable with images** - These text embeddings live in the same 1152-dimensional space as the image embeddings, which is what allows the model to compare them directly\n",
        "\n",
        "3. **Different signature** - The text \"a wild bear\" has a different pattern from the bear image, but they share enough similarities to have high similarity scores\n",
        "\n",
        "#### The Full Picture\n",
        "\n",
        "Remember that what we're seeing is just the first 10 dimensions of 1152. The full power of these embeddings comes from the complex patterns across all dimensions working together. The model has learned to encode similar concepts (whether in image or text form) into similar regions of this high-dimensional space.\n",
        "\n",
        "When computing similarity, all 1152 dimensions are compared, not just these first few that we're visualizing. This is why two vectors that might look different in their first 10 dimensions could still be considered similar when all dimensions are considered.\n"
      ],
      "id": "b72ab8f9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute similarity between our images and texts\n",
        "# Instead of computing dot product manually, let's use the model's built-in functionality\n",
        "\n",
        "# Create a function to compute similarity between images and texts using the model directly\n",
        "def compute_image_text_similarity(images, texts, model, processor):\n",
        "    \"\"\"Compute similarity between images and texts using the model's native capabilities\"\"\"\n",
        "    similarity_matrix = np.zeros((len(images), len(texts)))\n",
        "    \n",
        "    for i, image in enumerate(images):\n",
        "        # Process each image with all text descriptions\n",
        "        inputs = processor(\n",
        "            text=texts, \n",
        "            images=image, \n",
        "            return_tensors=\"pt\", \n",
        "            padding=\"max_length\", \n",
        "            max_length=64\n",
        "        )\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            # The model directly computes logits_per_image which represents similarity\n",
        "            logits = outputs.logits_per_image\n",
        "            # Convert to probabilities\n",
        "            probs = torch.sigmoid(logits)\n",
        "            \n",
        "            # Store the similarity scores for this image\n",
        "            similarity_matrix[i] = probs[0].detach().numpy()\n",
        "    \n",
        "    return similarity_matrix\n",
        "\n",
        "# Compute similarity using the model's native capabilities\n",
        "print(\"Computing image-text similarity using the model's built-in functionality...\")\n",
        "similarity_matrix = compute_image_text_similarity(images, texts, model, processor)\n",
        "print(\"Similarity computation complete.\")\n",
        "\n",
        "# Display similarity matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(similarity_matrix, vmin=0, vmax=1, cmap='viridis')\n",
        "plt.colorbar(label='Similarity Score')\n",
        "plt.xticks(np.arange(len(texts)), texts, rotation=45, ha='right')\n",
        "plt.yticks(np.arange(len(images)), [f\"Image {i+1}\" for i in range(len(images))])\n",
        "plt.title('Image-Text Similarity Matrix')\n",
        "\n",
        "# Add text annotations with the score values\n",
        "for i in range(len(images)):\n",
        "    for j in range(len(texts)):\n",
        "        plt.text(j, i, f'{similarity_matrix[i, j]:.2f}', \n",
        "                 ha='center', va='center', \n",
        "                 color='white' if similarity_matrix[i, j] < 0.5 else 'black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "cdc45d77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Connecting Images to Meaning: How Embeddings Enable Cross-Modal Understanding\n",
        "\n",
        "Looking at the similarity matrix above, we can now understand how the embedding vectors we visualized earlier enable the model to connect images with text:\n",
        "\n",
        "#### From Numbers to Matching\n",
        "\n",
        "1. **The bear image (Image 1)** shows highest similarity with \"a wild bear\" text. Looking back at their embedding visualizations, while they don't look identical in the first 10 dimensions, the complete 1152-dimensional pattern contains enough similarity for the model to make this connection.\n",
        "\n",
        "2. **Similar concepts, similar embeddings** - When we see a high similarity score (like between the bear image and bear text), it means their complete embedding vectors are pointing in similar directions in the 1152-dimensional space, even if the individual values aren't identical.\n",
        "\n",
        "3. **Embedding space geometry** - You can think of each embedding as a point in a 1152-dimensional space. Similar concepts (whether images or text) are positioned closer together in this space.\n",
        "\n",
        "#### The Magic of Shared Embedding Space\n",
        "\n",
        "What makes these embeddings so powerful is that both images and text are mapped to the same embedding space. This means:\n",
        "\n",
        "- The bear image and the text \"a wild bear\" produce vectors that point in similar directions\n",
        "- The bedroom image and text about bedrooms create vectors in another region of the space\n",
        "- The stop sign image and text about stop signs cluster in yet another region\n",
        "\n",
        "It's as if the model has created a giant 1152-dimensional map where similar concepts are placed near each other, regardless of whether they come from images or text.\n",
        "\n",
        "#### From Individual Values to Overall Meaning\n",
        "\n",
        "Looking at individual embedding values (like `0.1253` or `-0.0891`) doesn't tell us much on its own. It's the pattern across all dimensions that matters. Each dimension might represent complex features like:\n",
        "\n",
        "- \"Furry texture\" (potentially high in the bear image)\n",
        "- \"Red color\" (potentially high in the stop sign image)\n",
        "- \"Indoor setting\" (potentially high in the bedroom image)\n",
        "- \"Natural environment\" (potentially high in the bear image)\n",
        "\n",
        "But these features aren't explicitly defined - they emerge organically during training as the model learns to map similar concepts to similar embedding regions.\n",
        "\n",
        "This is why image embeddings are so powerful: they transform pixels into semantic representations that can be directly compared with text, enabling applications like image search, classification, and multimodal understanding.\n",
        "\n",
        "## Example 3: Visualizing Embeddings with Clustering\n",
        "\n",
        "Let's use clustering to group our images based on their semantic content. For a more meaningful analysis, we'll use a larger set of images from the COCO dataset and visualize them using UMAP before clustering.\n"
      ],
      "id": "23f5c391"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import additional libraries for enhanced visualization\n",
        "from umap import UMAP\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Define a larger set of sample images from COCO dataset\n",
        "coco_image_urls = [\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000285.jpg\",  # bear\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000632.jpg\",  # train\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000724.jpg\",  # umbrella\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000776.jpg\",  # teddy bear\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000785.jpg\",  # clock\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000802.jpg\",  # train\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000872.jpg\",  # person with umbrella\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000885.jpg\",  # dining table\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000934.jpg\",  # person\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001000.jpg\",  # zebra\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001296.jpg\",  # sheep\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001425.jpg\",  # airplane\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001490.jpg\",  # giraffe\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001503.jpg\",  # bird\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001532.jpg\",  # dog\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001584.jpg\",  # boat\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001675.jpg\",  # person on bike\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001761.jpg\",  # cat\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000001818.jpg\",  # horse\n",
        "    \"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000002153.jpg\",  # car\n",
        "]\n",
        "\n",
        "# Extract embeddings for all images\n",
        "print(\"Extracting embeddings for all images...\")\n",
        "large_embeddings = []\n",
        "large_images = []\n",
        "\n",
        "for i, url in enumerate(tqdm(coco_image_urls)):\n",
        "    try:\n",
        "        embedding, image = get_image_embedding(url, model, processor)\n",
        "        large_embeddings.append(embedding)\n",
        "        large_images.append(image)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {i+1}: {e}\")\n",
        "\n",
        "# Convert to numpy array\n",
        "large_embeddings = np.array(large_embeddings)\n",
        "print(f\"Successfully embedded {len(large_embeddings)} images. Embedding shape: {large_embeddings.shape}\")"
      ],
      "id": "f9c92a3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing High-Dimensional Embeddings with UMAP\n",
        "\n",
        "Uniform Manifold Approximation and Projection (UMAP)[^24] is a dimensionality reduction technique that helps us visualize high-dimensional embeddings in 2D space while preserving their local and global structure. Unlike simpler methods like PCA, UMAP can capture non-linear relationships in the data, making it ideal for visualizing complex embedding spaces.\n"
      ],
      "id": "4ce6ab35"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply UMAP for dimensionality reduction to visualize embeddings in 2D\n",
        "print(\"Applying UMAP dimensionality reduction...\")\n",
        "umap_model = UMAP(n_components=2, n_neighbors=5, min_dist=0.1, metric='cosine', random_state=42)  # Using UMAP algorithm for dimensionality reduction\n",
        "umap_embeddings = umap_model.fit_transform(large_embeddings)\n",
        "\n",
        "# Function to plot images on UMAP projection\n",
        "def plot_images_on_umap(embeddings_2d, images, figsize=(12, 10), image_zoom=0.7):\n",
        "    \"\"\"Plot images on a 2D projection (like UMAP or t-SNE)\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    \n",
        "    # First scatter the points to see the overall distribution\n",
        "    ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5, s=10)\n",
        "    \n",
        "    # Determine the data bounds\n",
        "    x_min, x_max = embeddings_2d[:, 0].min(), embeddings_2d[:, 0].max()\n",
        "    y_min, y_max = embeddings_2d[:, 1].min(), embeddings_2d[:, 1].max()\n",
        "    \n",
        "    # Calculate padding to ensure square aspect ratio\n",
        "    x_range = x_max - x_min\n",
        "    y_range = y_max - y_min\n",
        "    max_range = max(x_range, y_range) * 1.1  # Add 10% padding\n",
        "    \n",
        "    x_mid = (x_min + x_max) / 2\n",
        "    y_mid = (y_min + y_max) / 2\n",
        "    \n",
        "    # Set equal aspect ratio for the plot\n",
        "    ax.set_aspect('equal')\n",
        "    \n",
        "    # Set limits to ensure square aspect ratio\n",
        "    ax.set_xlim(x_mid - max_range/2, x_mid + max_range/2)\n",
        "    ax.set_ylim(y_mid - max_range/2, y_mid + max_range/2)\n",
        "    \n",
        "    # Then plot small versions of each image at its 2D location\n",
        "    for i, (x, y) in enumerate(embeddings_2d):\n",
        "        img = images[i]\n",
        "        # Preserve aspect ratio when resizing\n",
        "        width, height = img.size\n",
        "        # Calculate new dimensions while maintaining aspect ratio\n",
        "        if width > height:\n",
        "            new_width = int(width * image_zoom)\n",
        "            new_height = int(height * (new_width / width))\n",
        "        else:\n",
        "            new_height = int(height * image_zoom)\n",
        "            new_width = int(width * (new_height / height))\n",
        "            \n",
        "        try:\n",
        "            # Use LANCZOS for better quality, fall back to other methods if not available\n",
        "            img = img.resize((new_width, new_height), Image.LANCZOS)\n",
        "        except AttributeError:\n",
        "            # For newer Pillow versions where LANCZOS might be removed\n",
        "            img = img.resize((new_width, new_height), Image.BICUBIC)\n",
        "        \n",
        "        # Convert PIL image to a format matplotlib can use\n",
        "        # Increase the zoom parameter to make images larger\n",
        "        img_box = OffsetImage(img, zoom=0.15)\n",
        "        ab = AnnotationBbox(img_box, (x, y), frameon=True, pad=0.1)\n",
        "        ax.add_artist(ab)\n",
        "    \n",
        "    plt.title(\"UMAP Projection of Image Embeddings\")\n",
        "    plt.tight_layout()\n",
        "    return fig, ax\n",
        "\n",
        "# Visualize the UMAP embedding\n",
        "print(\"Visualizing UMAP projection with images...\")\n",
        "fig, ax = plot_images_on_umap(umap_embeddings, large_images)\n",
        "plt.show()"
      ],
      "id": "4fd3e976",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using K-means Clustering on Embeddings\n",
        "\n",
        "Now that we've visualized our embeddings in 2D space, let's use K-means clustering[^25] to identify groups of semantically similar images. K-means is an unsupervised learning algorithm that groups data points with similar features together based on their Euclidean distance in the embedding space.\n"
      ],
      "id": "c45ef69b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply K-means clustering on the original high-dimensional embeddings\n",
        "n_clusters = 5  # Increase the number of clusters for a more nuanced analysis\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters = kmeans.fit_predict(large_embeddings)\n",
        "\n",
        "# Visualize clustering results on the UMAP projection\n",
        "plt.figure(figsize=(12, 10))\n",
        "scatter = plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], \n",
        "                     c=clusters, cmap='viridis', s=100, alpha=0.8)\n",
        "\n",
        "# Determine the data bounds\n",
        "x_min, x_max = umap_embeddings[:, 0].min(), umap_embeddings[:, 0].max()\n",
        "y_min, y_max = umap_embeddings[:, 1].min(), umap_embeddings[:, 1].max()\n",
        "\n",
        "# Calculate padding to ensure square aspect ratio\n",
        "x_range = x_max - x_min\n",
        "y_range = y_max - y_min\n",
        "max_range = max(x_range, y_range) * 1.1  # Add 10% padding\n",
        "\n",
        "x_mid = (x_min + x_max) / 2\n",
        "y_mid = (y_min + y_max) / 2\n",
        "\n",
        "# Set equal aspect ratio for the plot\n",
        "plt.gca().set_aspect('equal')\n",
        "\n",
        "# Set limits to ensure square aspect ratio\n",
        "plt.xlim(x_mid - max_range/2, x_mid + max_range/2)\n",
        "plt.ylim(y_mid - max_range/2, y_mid + max_range/2)\n",
        "\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.title(f'UMAP Projection with K-means Clustering (k={n_clusters})')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "7243d401",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[^24]: McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform manifold approximation and projection for dimension reduction. *arXiv preprint arXiv:1802.03426*. [arXiv:1802.03426](https://arxiv.org/abs/1802.03426)\n",
        "\n",
        "[^25]: Lloyd, S. (1982). Least squares quantization in PCM. *IEEE Transactions on Information Theory*, 28(2), 129-137. https://doi.org/10.1109/TIT.1982.1056489\n",
        "\n",
        "### Visualizing Images by Cluster\n",
        "\n",
        "Let's visualize the actual images in each cluster to see what semantic groupings the model has identified.\n"
      ],
      "id": "fec0260d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display images by cluster\n",
        "for cluster_id in range(n_clusters):\n",
        "    # Get indices of images in this cluster\n",
        "    cluster_indices = np.where(clusters == cluster_id)[0]\n",
        "    n_images_in_cluster = len(cluster_indices)\n",
        "    \n",
        "    if n_images_in_cluster > 0:\n",
        "        # Calculate grid layout dimensions\n",
        "        grid_cols = min(5, n_images_in_cluster)\n",
        "        grid_rows = (n_images_in_cluster + grid_cols - 1) // grid_cols\n",
        "        \n",
        "        fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(grid_cols * 3, grid_rows * 3))\n",
        "        plt.suptitle(f'Cluster {cluster_id+1}: {n_images_in_cluster} Images')\n",
        "        \n",
        "        # Flatten axes array for easy iteration\n",
        "        if grid_rows == 1 and grid_cols == 1:\n",
        "            axes = np.array([axes])\n",
        "        elif grid_rows == 1 or grid_cols == 1:\n",
        "            axes = axes.flatten()\n",
        "            \n",
        "        # Plot each image in the cluster\n",
        "        for i, idx in enumerate(cluster_indices):\n",
        "            if i < len(axes):\n",
        "                row, col = i // grid_cols, i % grid_cols\n",
        "                if grid_rows == 1 and grid_cols == 1:\n",
        "                    ax = axes[0]\n",
        "                elif grid_rows == 1 or grid_cols == 1:\n",
        "                    ax = axes[i]\n",
        "                else:\n",
        "                    ax = axes[row, col]\n",
        "                    \n",
        "                ax.imshow(large_images[idx], aspect='equal')\n",
        "                ax.set_title(f\"Image {idx+1}\")\n",
        "                ax.axis('off')\n",
        "        \n",
        "        # Hide unused subplots\n",
        "        for i in range(n_images_in_cluster, grid_rows * grid_cols):\n",
        "            row, col = i // grid_cols, i % grid_cols\n",
        "            if grid_rows == 1 and grid_cols == 1:\n",
        "                pass  # No unused subplots in a 1x1 grid\n",
        "            elif grid_rows == 1 or grid_cols == 1:\n",
        "                if i < len(axes):\n",
        "                    axes[i].axis('off')\n",
        "            else:\n",
        "                if row < grid_rows and col < grid_cols:\n",
        "                    axes[row, col].axis('off')\n",
        "                \n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust for the suptitle\n",
        "        plt.show()"
      ],
      "id": "31553e4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis of Semantic Clustering\n",
        "\n",
        "The clusters formed above demonstrate how SigLIP 2's embeddings group images based on semantic content rather than just visual similarity. This type of semantic clustering is valuable for:\n",
        "\n",
        "1. **Content organization**: Automatically categorizing large collections of images\n",
        "2. **Recommendation systems**: Finding semantically related content\n",
        "3. **Anomaly detection**: Identifying images that don't fit expected semantic patterns\n",
        "4. **Dataset exploration**: Understanding the distribution of semantic concepts\n",
        "\n",
        "The UMAP visualization provides insight into how the high-dimensional embedding space is organized, while K-means clustering identifies discrete groups within this space. Together, they offer a powerful way to explore and understand the semantic relationships captured by SigLIP 2's image embeddings.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "In this notebook, we've explored the concept of image embeddings and specifically delved into SigLIP 2, Google's advanced multilingual vision-language encoder. We've seen how image embeddings work, the technical evolution from CLIP to SigLIP to SigLIP 2, and the key capabilities that make SigLIP 2 stand out.\n",
        "\n",
        "Through practical examples, we've demonstrated:\n",
        "\n",
        "1. How to perform zero-shot image classification\n",
        "2. How to compute image-text similarity\n",
        "3. How to visualize and cluster embeddings\n",
        "4. How to extract image embeddings for downstream tasks\n",
        "5. How to compute image-to-image similarity\n",
        "6. How to build a simple image search engine\n",
        "\n",
        "Image embeddings like those produced by SigLIP 2 are foundational to modern computer vision applications, enabling efficient search, classification, and multimodal understanding. As models continue to evolve, we can expect even more powerful and versatile embeddings that further bridge the gap between vision and language understanding.\n",
        "\n",
        "The flexible architecture and variant options make SigLIP 2 adaptable to a wide range of applications, from resource-constrained edge devices to high-performance systems requiring maximum accuracy. By understanding these tradeoffs, you can select the most appropriate SigLIP 2 variant for your specific use case, whether you prioritize efficiency, accuracy, or specialized capabilities like document understanding.\n",
        "\n",
        "The multilingual capabilities and enhanced training methodology of SigLIP 2 make it particularly valuable for building more inclusive and accurate AI systems that can understand visual content across different languages and cultures.\n",
        "\n",
        "## Conclusion: The Power and Versatility of Image Embeddings\n",
        "\n",
        "In this notebook, we've explored the concept of image embeddings with a focus on SigLIP 2, Google's advanced multilingual vision-language encoder. We've seen how these sophisticated representations go far beyond simple vector spaces, incorporating advanced mechanisms that significantly enhance their utility.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Advanced Similarity Computation**: SigLIP 2 doesn't just rely on simple cosine similarity between embeddings. It incorporates:\n",
        "   - MAP head pooling for better representation aggregation\n",
        "   - Temperature scaling to control similarity sharpness\n",
        "   - Bias terms to adjust for training imbalances\n",
        "   - Sigmoid activation to convert similarities to probabilities\n",
        "\n",
        "2. **Powerful Applications**: These sophisticated embeddings enable a wide range of applications:\n",
        "   - Visualization and exploration through clustering\n",
        "   - Unsupervised grouping based on semantic content\n",
        "   - Cross-modal understanding between images and text\n",
        "   - Semantic search engines with high precision\n",
        "   - Fine-grained recognition of subtle differences and similarities\n",
        "\n",
        "3. **Proper Usage**: As we've demonstrated, to get the most out of SigLIP 2, it's crucial to use the model's built-in similarity calculation mechanisms rather than trying to manually compute cosine similarity on raw embeddings.\n",
        "\n",
        "The quality of SigLIP 2's embeddings makes these applications more accurate and robust than ever before. Its multilingual capabilities and improved semantic understanding make it particularly valuable for diverse global applications.\n",
        "\n",
        "As image embedding models continue to evolve, we can expect even more powerful capabilities that further bridge the gap between visual content and natural language understanding. These embeddings form the foundation of modern computer vision systems and are becoming increasingly important in multimodal AI applications that combine vision, language, and other modalities.\n",
        "\n",
        "Whether you're building a visual search engine, a content recommendation system, or a multimodal understanding application, image embeddings like those produced by SigLIP 2 provide a solid foundation for bringing semantic understanding to your visual data—just be sure to leverage their full capabilities by using the model's built-in similarity mechanisms!\n",
        "\n",
        "### Important Note on Processing Image-Text Pairs\n",
        "\n",
        "An important detail when working with vision-language models like SigLIP is understanding how to properly compute similarity between images and text.\n",
        "\n",
        "#### The Proper Way: Process Image-Text Pairs Together\n",
        "\n",
        "While it's possible to extract image and text embeddings separately (as we did in some examples for educational purposes), the proper way to compute image-text similarity is to use the model's native capability to process image-text pairs together:\n",
        "\n",
        "```python\n",
        "# The right way to compute image-text similarity with vision-language models\n",
        "inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits_per_image  # Direct similarity scores\n",
        "probabilities = torch.sigmoid(logits)  # Convert to probabilities\n",
        "```\n",
        "\n",
        "#### Why This Matters\n",
        "\n",
        "Vision-language models like SigLIP are specifically trained to compute similarity between image-text pairs in a particular way. When we extract embeddings separately and then compute similarity using dot products, we're not fully leveraging the model's capabilities.\n",
        "\n",
        "The model's native `logits_per_image` output includes any internal transformations, normalization, or calibration that the model has learned during training. This leads to more accurate similarity scores compared to taking embeddings separately and computing similarity manually[^22].\n",
        "\n",
        "#### When to Use Direct Embeddings\n",
        "\n",
        "There are still valid use cases for extracting embeddings directly:\n",
        "\n",
        "1. **Image-to-image similarity**: When comparing within the same modality\n",
        "2. **Building search indices**: For efficient retrieval systems\n",
        "3. **Transfer learning**: Using the embeddings as input features for downstream tasks\n",
        "\n",
        "However, for direct image-text similarity comparisons, always prefer the model's built-in methods for processing the pairs together[^23].\n",
        "\n",
        "[^22]: Hugging Face. (2024). Zero-shot Image Classification with Transformers. https://huggingface.co/docs/transformers/en/tasks/zero_shot_image_classification\n",
        "\n",
        "[^23]: Pinecone. (2024). Zero-shot Image Classification with CLIP. https://www.pinecone.io/learn/series/image-search/zero-shot-image-classification-clip/\n",
        "\n",
        "## References\n",
        "\n",
        "1. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G. (2021). Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (pp. 8748-8763). PMLR. [arXiv:2103.00020](https://arxiv.org/abs/2103.00020)\n",
        "\n",
        "2. Zhai, X., Wang, X., Mustafa, B., Steiner, A., Keysers, D., Kolesnikov, A., & Beyer, L. (2023). Sigmoid loss for language image pre-training. In Proceedings of the 40th International Conference on Machine Learning (pp. 40844-40858). PMLR. [arXiv:2303.15343](https://arxiv.org/abs/2303.15343)\n",
        "\n",
        "3. Beyer, L., Dehghani, M., et al. (2024). SigLIP 2: Next-Generation Multilingual Vision-Language Models. Google Research. [arXiv:2409.01936](https://arxiv.org/abs/2409.01936)\n",
        "\n",
        "4. Google Research. (2024). SigLIP 2: Multilingual vision-language models with self-supervised learning. GitHub. [Repository](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/README_siglip2.md)\n",
        "\n",
        "5. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ... & Rush, A. M. (2020). Transformers: State-of-the-art natural language processing. *In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations* (pp. 38-45). [ACL Anthology](https://aclanthology.org/2020.emnlp-demos.6/)\n",
        "\n",
        "6. McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform manifold approximation and projection for dimension reduction. *arXiv preprint arXiv:1802.03426*. [arXiv:1802.03426](https://arxiv.org/abs/1802.03426)\n",
        "\n",
        "7. Google. (2024). SigLIP 2 SO400M Patch14-384 Model. Hugging Face. [Model Card](https://huggingface.co/google/siglip2-so400m-patch14-384)\n",
        "\n",
        "8. Hugging Face. (2024). Zero-Shot Image Classification with Transformers. [Documentation](https://huggingface.co/docs/transformers/en/tasks/zero_shot_image_classification)\n",
        "\n",
        "9. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. *In Advances in Neural Information Processing Systems* (pp. 5998-6008). [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "10. Bengio, Y., Courville, A., & Vincent, P. (2013). Representation learning: A review and new perspectives. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 35(8), 1798-1828. [IEEE](https://doi.org/10.1109/TPAMI.2013.50)\n",
        "\n",
        "11. Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. *Advances in Neural Information Processing Systems*, 25. [NeurIPS](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
        "\n",
        "12. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. *In International Conference on Learning Representations*. [arXiv:2010.11929](https://arxiv.org/abs/2010.11929)\n",
        "\n",
        "13. Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020). A simple framework for contrastive learning of visual representations. *International Conference on Machine Learning*, 1597-1607. [arXiv:2002.05709](https://arxiv.org/abs/2002.05709)\n",
        "\n",
        "14. Johnson, J., Douze, M., & Jégou, H. (2019). Billion-scale similarity search with GPUs. *IEEE Transactions on Big Data*, 7(3), 535-547. [IEEE](https://doi.org/10.1109/TBDATA.2019.2921572)\n",
        "\n",
        "15. Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*. [arXiv:1503.02531](https://arxiv.org/abs/1503.02531)\n"
      ],
      "id": "3b9d88ff"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}