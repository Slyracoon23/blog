[
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": " Research notes",
    "section": "",
    "text": "Training Data Influence Analysis and Estimation A Survey\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nNOTES: How to Prepare a Vision Dataset\n\n\n\n\n\n\nDataset Preparation\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nNOTES: What is Prompt Engineering?\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nNOTES: Testing Observable JS\n\n\n\n\n\n\nJavaScript\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nExtending the Context Window of LLMs\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html",
    "title": "Building ChatGPT from Scratch",
    "section": "",
    "text": "Building your own ChatGPT-like assistant is now more accessible than ever. With the release of powerful open-source models and efficient fine-tuning techniques, developers can create customized AI assistants tailored to specific use cases. In this comprehensive guide, we‚Äôll walk through the entire process of building a ChatGPT-like system from scratch.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html#understanding-chatgpt-architecture",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html#understanding-chatgpt-architecture",
    "title": "Building ChatGPT from Scratch",
    "section": "ü§ñ Understanding ChatGPT Architecture",
    "text": "ü§ñ Understanding ChatGPT Architecture\nAt its core, ChatGPT consists of several key components:\n\nBase Language Model: A large language model (LLM) trained on vast amounts of text data\nInstruction Fine-tuning: Training to follow instructions and generate helpful responses\nAlignment: Ensuring the model‚Äôs outputs align with human preferences and values\nDeployment Infrastructure: Systems to serve the model efficiently to users\n\n\nThe most critical aspect of building a ChatGPT-like system is the alignment process. Traditional approaches involve a multi-stage pipeline: first Supervised Fine-Tuning (SFT) to adapt the model to follow instructions, followed by preference alignment methods like Reinforcement Learning with Human Feedback (RLHF) or Direct Preference Optimization (DPO).\nHowever, newer techniques like ORPO (Odds Ratio Preference Optimization) now allow us to combine these stages, making the process more efficient.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-1-selecting-a-base-model",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-1-selecting-a-base-model",
    "title": "Building ChatGPT from Scratch",
    "section": "üíª Step 1: Selecting a Base Model",
    "text": "üíª Step 1: Selecting a Base Model\nFor our ChatGPT clone, we‚Äôll use Llama 3 8B, the latest open-weight model from Meta. This model offers an excellent balance of performance and resource requirements, making it ideal for custom development.\nLlama 3 was trained on approximately 15 trillion tokens (compared to 2T tokens for Llama 2) and features an 8,192 token context window. The model uses a new tokenizer with a 128K-token vocabulary, which reduces the number of tokens required to encode text by about 15%.\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n)\n\n# Model\nbase_model = \"meta-llama/Meta-Llama-3-8B\"\n\n# Configure quantization for efficient loading\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-2-preparing-training-data",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-2-preparing-training-data",
    "title": "Building ChatGPT from Scratch",
    "section": "üìö Step 2: Preparing Training Data",
    "text": "üìö Step 2: Preparing Training Data\nHigh-quality training data is crucial for building an effective assistant. We need two types of datasets:\n\nInstruction Dataset: Examples of prompts and helpful responses\nPreference Dataset: Pairs of responses where one is preferred over the other\n\nFor our project, we‚Äôll create a custom dataset combining several high-quality sources:\nfrom datasets import load_dataset\n\n# Load and prepare dataset\ndataset = load_dataset(\"mlabonne/chatgpt-training-mix\")\ndataset = dataset.shuffle(seed=42)\n\n# Format data for chat template\ndef format_chat_template(row):\n    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n    return row\n\ndataset = dataset.map(format_chat_template)\ndataset = dataset.train_test_split(test_size=0.05)",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-3-fine-tuning-with-orpo",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-3-fine-tuning-with-orpo",
    "title": "Building ChatGPT from Scratch",
    "section": "üîÑ Step 3: Fine-tuning with ORPO",
    "text": "üîÑ Step 3: Fine-tuning with ORPO\nNow we‚Äôll fine-tune our model using ORPO, which combines instruction tuning and preference alignment into a single process. This approach is more efficient than traditional methods and produces better results.\nfrom peft import LoraConfig, prepare_model_for_kbit_training\nfrom trl import ORPOConfig, ORPOTrainer, setup_chat_format\n\n# Prepare model for chat format\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = prepare_model_for_kbit_training(model)\n\n# Configure LoRA for parameter-efficient fine-tuning\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\n\n# Configure ORPO training\norpo_args = ORPOConfig(\n    learning_rate=5e-6,\n    beta=0.1,\n    lr_scheduler_type=\"linear\",\n    max_length=2048,\n    max_prompt_length=512,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_8bit\",\n    num_train_epochs=3,\n    output_dir=\"./chatgpt-model/\",\n)\n\n# Initialize trainer and start training\ntrainer = ORPOTrainer(\n    model=model,\n    args=orpo_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n)\ntrainer.train()\ntrainer.save_model(\"./chatgpt-model\")",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-4-evaluation-and-iteration",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-4-evaluation-and-iteration",
    "title": "Building ChatGPT from Scratch",
    "section": "üîç Step 4: Evaluation and Iteration",
    "text": "üîç Step 4: Evaluation and Iteration\nAfter training, we need to evaluate our model to ensure it meets our quality standards. We‚Äôll use a combination of automated benchmarks and human evaluation:\nfrom transformers import pipeline\n\n# Load the fine-tuned model\nmodel_path = \"./chatgpt-model\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Create a chat pipeline\nchat_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_length=1024,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n)\n\n# Test with sample prompts\ntest_prompts = [\n    \"Explain quantum computing in simple terms\",\n    \"Write a short poem about artificial intelligence\",\n    \"How can I improve my programming skills?\"\n]\n\nfor prompt in test_prompts:\n    formatted_prompt = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False)\n    response = chat_pipeline(formatted_prompt)\n    print(f\"Prompt: {prompt}\\nResponse: {response[0]['generated_text']}\\n\")\nBased on evaluation results, we may need to iterate on our training data or fine-tuning approach to improve performance.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-5-deployment",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-5-deployment",
    "title": "Building ChatGPT from Scratch",
    "section": "üöÄ Step 5: Deployment",
    "text": "üöÄ Step 5: Deployment\nFinally, we‚Äôll deploy our ChatGPT clone as a web service that users can interact with:\nimport gradio as gr\nfrom transformers import pipeline\n\n# Load model and create pipeline\nmodel_path = \"./chatgpt-model\"\nchat_pipeline = pipeline(\n    \"text-generation\",\n    model=model_path,\n    tokenizer=model_path,\n    max_length=1024,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    device_map=\"auto\"\n)\n\n# Chat history management\ndef format_history(history):\n    formatted_history = []\n    for human, assistant in history:\n        formatted_history.append({\"role\": \"user\", \"content\": human})\n        if assistant:\n            formatted_history.append({\"role\": \"assistant\", \"content\": assistant})\n    return formatted_history\n\n# Response generation function\ndef generate_response(message, history):\n    formatted_history = format_history(history)\n    formatted_history.append({\"role\": \"user\", \"content\": message})\n    \n    prompt = tokenizer.apply_chat_template(formatted_history, tokenize=False)\n    response = chat_pipeline(prompt)[0][\"generated_text\"]\n    \n    # Extract just the assistant's response\n    assistant_response = response.split(\"assistant\\n\")[-1].strip()\n    return assistant_response\n\n# Create Gradio interface\ndemo = gr.ChatInterface(\n    generate_response,\n    title=\"My ChatGPT Clone\",\n    description=\"Ask me anything!\",\n    theme=\"soft\"\n)\n\n# Launch the web interface\ndemo.launch(share=True)",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html#conclusion",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html#conclusion",
    "title": "Building ChatGPT from Scratch",
    "section": "Conclusion",
    "text": "Conclusion\nBuilding your own ChatGPT-like assistant is a complex but rewarding process. By following the steps outlined in this guide, you can create a customized AI assistant tailored to your specific needs. The key components include selecting a powerful base model, preparing high-quality training data, fine-tuning with modern techniques like ORPO, rigorous evaluation, and deployment as a user-friendly service.\nAs open-source models continue to improve, the gap between custom-built assistants and commercial offerings like ChatGPT is narrowing. This democratization of AI technology enables developers to create specialized assistants for various domains without relying on closed API services.\nI hope this guide helps you on your journey to building your own AI assistant. If you have any questions or want to share your creations, feel free to reach out to me on Twitter @maximelabonne.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html#references",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html#references",
    "title": "Building ChatGPT from Scratch",
    "section": "References",
    "text": "References\n\nJ. Hong, N. Lee, and J. Thorne, ORPO: Monolithic Preference Optimization without Reference Model. 2024.\nL. von Werra et al., TRL: Transformer Reinforcement Learning. GitHub, 2020. [Online]. Available: https://github.com/huggingface/trl\nAI at Meta, Introducing Meta Llama 3, 2024.\nAnthropic, Constitutional AI: Harmlessness from AI Feedback, 2022.\nOpenAI, Training language models to follow instructions with human feedback, 2022.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Earl Potters",
    "section": "",
    "text": "Earl Potters is an AI Engineer specializing in intelligent agents and large language models, based in San Francisco. Co-founded and built production-ready AI solutions at Missio AI and won numerous hackathons, including a $100K grand prize in Blockchain Journalism at Metabuild 2022 hackthon ‚Äì Refound Journalism.\nAn active blogger, Earl writes about AI, large language models, and autonomous agents. He has created numerous open-source projects including an evaluation automation tool, prompt-spec.com, and other contributions to the AI community. His GitHub showcases repositories spanning robotics, AI, and blockchain development.\nConnect with him on X and LinkedIn.\n\nCredits:\n\nEmojis used in figures are designed by OpenMoji, the open-source emoji and icon project. License: CC BY-SA 4.0.\nVector icons are provided by Streamline (https://streamlinehq.com). License: CC BY-SA 4.0."
  },
  {
    "objectID": "notes/Large Language Models/extending_context.html",
    "href": "notes/Large Language Models/extending_context.html",
    "title": "Extending the Context Window of LLMs",
    "section": "",
    "text": "üìù Article: https://kaiokendev.github.io/context"
  },
  {
    "objectID": "notes/Large Language Models/extending_context.html#problem",
    "href": "notes/Large Language Models/extending_context.html#problem",
    "title": "Extending the Context Window of LLMs",
    "section": "Problem",
    "text": "Problem\nProblem: it is hard to extend the sequence length of a model.\n\nAnil et al.¬†(2022): the length extrapolation fails in part because of ‚Äúdistracting tokens‚Äù in the input during the PARITY task.\nChi et al.¬†(2022): bias terms in positional encoding (like in ALiBi) replicate the effect of windowed attention by decaying token inter-dependency on long-range receptive fields (the tokens only focus on the tokens closest to them).\nTao et al.¬†(2023) observe that, in long sequences, rear position embeddings are updates much fewer times than front position embeddings. They add random padding to the front patch of the sequence.\nLiu et al.¬†(2023): attention in long sequences starts to drift as we move to later positions and only attends to the most recent tokens."
  },
  {
    "objectID": "notes/Large Language Models/extending_context.html#silver-linings",
    "href": "notes/Large Language Models/extending_context.html#silver-linings",
    "title": "Extending the Context Window of LLMs",
    "section": "Silver Linings",
    "text": "Silver Linings\nThe attention mechanism seems destabilized in the case of long sequences due to an imbalance of attended tokens (either skewed to the front or the back).\nSeveral solutions have been proposed:\n\nFew-shot chain-of-thought reasoning and marker tokens\nLength generalization/extrapolation can be learned ability to a certain extent (improves performance but not a silver bullet)\nLLaMa 7B has been trained for retrieval over a 32K token window by introducing landmark tokens combined with a windowed-attention (blockwise computation)."
  },
  {
    "objectID": "notes/Large Language Models/extending_context.html#potential-solutions",
    "href": "notes/Large Language Models/extending_context.html#potential-solutions",
    "title": "Extending the Context Window of LLMs",
    "section": "Potential Solutions",
    "text": "Potential Solutions\n\nChange the attention calculation: log(n) scaling (does help), relacing the softmax with ReLU in the attention equation (does not converge), etc.\nRandom Positional Encoding\nShifted Positional Encodings: shifting the tokens progressively along the desired length during the encoding step (failure)."
  },
  {
    "objectID": "notes/Large Language Models/extending_context.html#final-solution",
    "href": "notes/Large Language Models/extending_context.html#final-solution",
    "title": "Extending the Context Window of LLMs",
    "section": "Final Solution",
    "text": "Final Solution\nTransformers do not learn how to gauge position based on the relative distance or the rotational factors, but memorize the tokens and their positional scaling factors.\n\nRotary positional embedding to loop the positions around after crossing the max context length (e.g., 2048): position_ids = position_ids % 2048\nBlock repeated positions: repeating the chosen frequency for a block of positions, so [1, 2, 3, 4, 5, ‚Ä¶, L] becomes [1, 1, 1, 1, 2, 2, 2, 2, 3, ‚Ä¶, L]. This is achieved by changing the frequency update: t *= 1/4.\n\nIn other words, several tokens (4 in this example) are assigned to the same position. This (surprising) scheme can quadruple the context length with minimal performance degradation (~2%). More information about it in this paper from Meta: https://arxiv.org/abs/2306.15595"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " Articles",
    "section": "",
    "text": "What is rrweb?\n\n\nA practical guide to understanding rrweb, a JavaScript library for recording and replaying web sessions\n\n\n\nWeb Recording\n\n\nData Conversion\n\n\n\n\n\n\nMar 14, 2025\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding ChatGPT from Scratch\n\n\nA step-by-step guide to creating your own AI assistant\n\n\n\nLarge Language Models\n\n\n\n\n\n\nFeb 19, 2025\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is an AI Agent?\n\n\nUnderstanding the core concepts, architecture, and applications of autonomous AI systems\n\n\n\nAI Agents\n\n\n\n\n\n\nApr 25, 2024\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/Agents/data_influence.html",
    "href": "notes/Agents/data_influence.html",
    "title": "Training Data Influence Analysis and Estimation A Survey",
    "section": "",
    "text": "üìù Paper: https://arxiv.org/pdf/2212.04612.pdf\nSurvey of methods to calculate the influence of training samples."
  },
  {
    "objectID": "notes/Agents/data_influence.html#pointwise-training-data-influence",
    "href": "notes/Agents/data_influence.html#pointwise-training-data-influence",
    "title": "Training Data Influence Analysis and Estimation A Survey",
    "section": "Pointwise Training Data Influence",
    "text": "Pointwise Training Data Influence\nQuantifies how a single training instance affects the model‚Äôs prediction on a single test instance according to some quality measure (e.g., test loss). Œ∏^‚àó := \\text{arg min} \\frac{1}{|D|} \\sum_{(x_i, y_i) \\in D} (y_i ‚àí Œ∏^\\top x_i)^2 Early pointwise influence analysis shows that a single outlier can completely shift the parameters of a least-squares regression. Thus, this model is completely non-robust. Different models have been proposed to increase the breakdown point, including changing the average function with a median function.\nModern methods can be categorized into two classes:\n\nRetraining-based methods: measure the training data‚Äôs influence by repeatedly retraining a model f using different subsets of training set D.\nGradient-based influence estimators: estimate influence via the alignment of training and test instance gradients, either throughout or at the end of training.\n\n\nAlternative perspectives on influence\nThe concept of influence is not clearly standardized:\n\nGroup influence: think batches of training data\nJoint influence: consider multiple test instances collectively\nMemorization: defined as the self-influence I(z_i, z_i)\nCook‚Äôs distance: measures the effect of training instances on the model parameters themselves I_{Cook}(z_i) := \\theta^{(T)} - \\theta^{(T)}_{D^{\\backslash i}}\nExpected influence: average influence across different instantiations and retrainings of a model class\nInfluence ranking orders training instances from most positively influential to most negatively influential"
  },
  {
    "objectID": "notes/Agents/data_influence.html#retraining-based-influence-analysis",
    "href": "notes/Agents/data_influence.html#retraining-based-influence-analysis",
    "title": "Training Data Influence Analysis and Estimation A Survey",
    "section": "Retraining-Based Influence Analysis",
    "text": "Retraining-Based Influence Analysis\nMeasures influence by training a model with and without some instance. Influence is then defined as the difference in these two models‚Äô behavior.\n\nLeave-On-Out Influence\nLeave-one-out (LOO) is the simplest influence measure described in this work. LOO is also the oldest, dating back to Cook and Weisberg [CW82] who term it case deletion diagnostics. I_{LOO}(z_i, z_{te}) := L(z_{te}; Œ∏^{(T)}_{D^{\\backslash i}}) ‚àí L(z_{te}; Œ∏^{(T)} ), Measuring the entire training set‚Äôs LOO influence requires training (n + 1) models.\n\n\nDownsampling\nMitigates leave-one-out influence‚Äôs two primary weaknesses: (1) computational complexity dependent on n and (2) instability due to stochastic training variation.\nRelies on an ensemble of K submodels, each trained on a subset D^k or the full training set D.\n\nIntuitively, it corresponds to z_{te}‚Äôs average risk when z_i is not used in submodel training.\nBy holding out multiple instances simultaneously and then averaging, each Downsampling submodel provides insight into the influence of all training instances. This allows Downsampling to require (far) fewer retrainings than LOO.\n\n\nShapley Value\nIntuitively, SV is the weighted change in z_{te}‚Äôs risk when z_i is added to a random training subset.\nIt can be viewed as generalizing the leave-one-out influence, where rather than considering only the full training set D, Shapley value averages the LOO influence across all possible subsets of D.\nThe main problem is that SV is computationally intractable for non-trivial datasets, which led to numerous speed-ups in the literature:\n\nTruncated Monte Carlo Shapley (TMC-Shapley): relies on randomized subset sampling from training set D.\nGradient Shapley (G-Shapley): even faster SV estimator that assumes models are trained in just one gradient step (at the expense of lower accuracy).\nk-NN-SV and **k-NN Shapley"
  },
  {
    "objectID": "notes/Agents/data_influence.html#gradient-based-influence-estimation",
    "href": "notes/Agents/data_influence.html#gradient-based-influence-estimation",
    "title": "Training Data Influence Analysis and Estimation A Survey",
    "section": "Gradient-Based Influence Estimation",
    "text": "Gradient-Based Influence Estimation\nIn models trained using gradient descent, the influence of training instances can be assessed through training gradients.\nThere are two types of gradient-based methods:\n\nStatic methods estimate the effect of retraining by examining gradients with respect to final model parameters, but this approach typically requires stronger assumptions due to the limited insight a single set of parameters can provide into the optimization landscape.\nDynamic methods analyze model parameters throughout training, which while being more computationally demanding, allows for fewer assumptions.\n\nHowever, both share a common limitation: they can potentially overlook highly influential training instances.\n\nStatic Estimators\nThere are two main static estimators: influence functions (more general) and representer point (more scalable).\n\nInfluence Functions\nAnalyze how a model changes when the weight of a training instance is slightly perturbed: \\theta^{(T)}_{+ \\epsilon_i} = \\arg \\min_{\\theta} \\frac{1}{n} \\sum_{z \\in D} L(z; \\theta) + \\epsilon_i L(z_i; \\theta). Assuming the model and loss function are twice-differentiable and strictly convex, Cook and Weisberg demonstrated that an infinitesimal perturbation‚Äôs impact could be calculated using a first-order Taylor expansion: \\frac{d\\theta^{(T)}_{+\\epsilon_i}}{d\\epsilon_i} \\bigg|_{\\epsilon_i=0} = - (H^{(T)}_\\theta)^{-1} \\nabla_\\theta L(z_i; \\theta^{(T)}), where the empirical risk Hessian H^{(T)}_\\theta := \\frac{1}{n} \\sum_{z \\in D} \\nabla^2_\\theta L(z; \\theta^{(T)}) is assumed to be positive definite.\nKoh and Liang extend this result to consider the effect of this infinitesimal perturbation on z_{te}‚Äôs risk, whereby applying the chain rule, we get:\n\\begin{align*}\n\\frac{dL(z_{te}; \\theta^{(T)})}{d\\epsilon_i} \\bigg|_{\\epsilon_i=0}\n\n&= \\frac{dL(z_{te}; \\theta^{(T)})}{d\\theta^{(T)}_{+\\epsilon_i}}^\\top {\\frac{d\\theta^{(T)}_{+\\epsilon_i}}{d\\epsilon_i}} \\bigg|_{\\epsilon_i=0} \\\\\n\n&= - \\nabla_\\theta L(z_{te}; \\theta^{(T)})^\\top (H^{(T)}_\\theta)^{-1} \\nabla_\\theta L(z_i; \\theta^{(T)}).\n\\end{align*}\nRemoving training instance z_i from D is equivalent to \\epsilon_i = -\\frac{1}{n}, resulting in the pointwise influence functions estimator \\hat{I}_{IF}(z_i, z_{te}) := \\frac{1}{n} \\nabla_\\theta L(z_{te}; \\theta^{(T)})^\\top (H^{(T)}_\\theta)^{-1} \\nabla_\\theta L(z_i; \\theta^{(T)}) Intuitively, it represents the influence functions‚Äô estimate of the leave-one-out influence of z_i on z_{te}.\n\n\nRepresenter Point Methods\nRepresenter-based methods rely on kernels, which are functions that measure the similarity between two vectors. They decompose the predictions of specific model classes into the individual contributions (i.e., influence) of each training instance.\n\n\n\nDynamic Estimators\n\nTracIn ‚Äì Tracing Gradient Descent\n\n\nHyDRA ‚Äì Hypergradient Data Relevance Analysis"
  },
  {
    "objectID": "notes/obserable JS/interactive-data-visualization-with-obserable-js.html",
    "href": "notes/obserable JS/interactive-data-visualization-with-obserable-js.html",
    "title": "NOTES: Testing Observable JS",
    "section": "",
    "text": "Observable JS is a powerful way to create interactive data visualizations within Quarto documents. Let‚Äôs explore a simple example that demonstrates reactive data binding and visualization.\n\ndata = [\n  {category: \"A\", value: 28},\n  {category: \"B\", value: 55},\n  {category: \"C\", value: 43},\n  {category: \"D\", value: 91},\n  {category: \"E\", value: 81},\n  {category: \"F\", value: 53},\n  {category: \"G\", value: 19},\n  {category: \"H\", value: 87}\n]\n\n// Create an input slider\nviewof multiplier = Inputs.range(\n  [0.1, 3], \n  {value: 1, step: 0.1, label: \"Data multiplier:\"}\n)\n\n// Apply the multiplier reactively\nscaled_data = data.map(d =&gt; ({\n  category: d.category,\n  value: d.value * multiplier\n}))\n\n// Create a bar chart using Plot\nPlot.plot({\n  marginLeft: 60,\n  y: {label: \"Value\"},\n  marks: [\n    Plot.barY(scaled_data, {x: \"category\", y: \"value\", fill: \"steelblue\", tip: true}),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can enhance our visualization by adding more interactive controls:\n\nviewof color = Inputs.select(\n  [\"steelblue\", \"orange\", \"green\", \"purple\", \"red\"],\n  {value: \"steelblue\", label: \"Bar color:\"}\n)\n\n// Create a bar chart with user-selected color\nPlot.plot({\n  marginLeft: 60,\n  y: {label: \"Value\"},\n  marks: [\n    Plot.barY(scaled_data, {x: \"category\", y: \"value\", fill: color, tip: true}),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservable JS also makes it easy to display your data in multiple formats:\n\n// Display the data in a table\nInputs.table(scaled_data)\n\n\n\n\n\n\n\n\n\nThe power of Observable JS lies in its reactive runtime. When you change the slider value, all calculations that depend on multiplier are automatically recomputed, and any visualizations that depend on those calculations are redrawn.\nThis reactivity makes it easy to create interactive dashboards without writing complex event handling code."
  },
  {
    "objectID": "notes/obserable JS/interactive-data-visualization-with-obserable-js.html#interactive-data-visualization-with-observable-js",
    "href": "notes/obserable JS/interactive-data-visualization-with-obserable-js.html#interactive-data-visualization-with-observable-js",
    "title": "NOTES: Testing Observable JS",
    "section": "",
    "text": "Observable JS is a powerful way to create interactive data visualizations within Quarto documents. Let‚Äôs explore a simple example that demonstrates reactive data binding and visualization.\n\ndata = [\n  {category: \"A\", value: 28},\n  {category: \"B\", value: 55},\n  {category: \"C\", value: 43},\n  {category: \"D\", value: 91},\n  {category: \"E\", value: 81},\n  {category: \"F\", value: 53},\n  {category: \"G\", value: 19},\n  {category: \"H\", value: 87}\n]\n\n// Create an input slider\nviewof multiplier = Inputs.range(\n  [0.1, 3], \n  {value: 1, step: 0.1, label: \"Data multiplier:\"}\n)\n\n// Apply the multiplier reactively\nscaled_data = data.map(d =&gt; ({\n  category: d.category,\n  value: d.value * multiplier\n}))\n\n// Create a bar chart using Plot\nPlot.plot({\n  marginLeft: 60,\n  y: {label: \"Value\"},\n  marks: [\n    Plot.barY(scaled_data, {x: \"category\", y: \"value\", fill: \"steelblue\", tip: true}),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can enhance our visualization by adding more interactive controls:\n\nviewof color = Inputs.select(\n  [\"steelblue\", \"orange\", \"green\", \"purple\", \"red\"],\n  {value: \"steelblue\", label: \"Bar color:\"}\n)\n\n// Create a bar chart with user-selected color\nPlot.plot({\n  marginLeft: 60,\n  y: {label: \"Value\"},\n  marks: [\n    Plot.barY(scaled_data, {x: \"category\", y: \"value\", fill: color, tip: true}),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservable JS also makes it easy to display your data in multiple formats:\n\n// Display the data in a table\nInputs.table(scaled_data)\n\n\n\n\n\n\n\n\n\nThe power of Observable JS lies in its reactive runtime. When you change the slider value, all calculations that depend on multiplier are automatically recomputed, and any visualizations that depend on those calculations are redrawn.\nThis reactivity makes it easy to create interactive dashboards without writing complex event handling code."
  },
  {
    "objectID": "notes/obserable JS/interactive-data-visualization-with-obserable-js.html#next-steps",
    "href": "notes/obserable JS/interactive-data-visualization-with-obserable-js.html#next-steps",
    "title": "NOTES: Testing Observable JS",
    "section": "Next Steps",
    "text": "Next Steps\nTo create more complex visualizations with Observable JS, you can:\n\nUse data from external sources with FileAttachment() or fetch APIs\nImplement more complex interactions between multiple visualizations\nIncorporate advanced D3.js visualizations\nUse Observable‚Äôs built-in libraries for specialized visualizations\n\nCheck out the Observable documentation for more examples and tutorials."
  },
  {
    "objectID": "posts/2025-03-14_what_is_rrweb.html",
    "href": "posts/2025-03-14_what_is_rrweb.html",
    "title": "What is rrweb?",
    "section": "",
    "text": "I have used PostHog for a while now. They have a bunch of features like engagement funnels and user tracking. However, one of their features that particularly was of interest to me was their session replays. The session replays look like full recordings of people‚Äôs browsers as they browse your website. Can you imagine my surprise when I learned that they were not capturing your screen!? So how do they do it then? How does it look exactly like how you actually ‚Äúrecord‚Äù your screen?\nIn this article I will go over the open-source framework of rrweb and how it on a conceptual level records our screens. Next we will create some scripts that will allow us to turn those sessions into actual videos, images, and individual HTML snapshots.\nLet‚Äôs Begin",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üåê **Web Technologies**",
      "What is rrweb?"
    ]
  },
  {
    "objectID": "posts/2025-03-14_what_is_rrweb.html#what-is-rrweb",
    "href": "posts/2025-03-14_what_is_rrweb.html#what-is-rrweb",
    "title": "What is rrweb?",
    "section": "What is rrweb?",
    "text": "What is rrweb?\nrrweb is an open-source JavaScript library that allows you to record and replay web sessions with high fidelity. The name ‚Äúrrweb‚Äù stands for ‚Äúrecord and replay the web.‚Äù With over 17,000 GitHub stars, it‚Äôs a popular tool used by many companies including PostHog, LogRocket, FullStory, and Hotjar for their session replay features.\nUnlike traditional screen recording tools that capture pixel data, rrweb works by recording the DOM (Document Object Model) and user interactions. This approach creates lightweight, high-fidelity recordings that can be replayed with perfect visual accuracy.\n\nHow rrweb Works\nAt a high level, rrweb operates through three main components:\n\nDOM Snapshots: rrweb takes an initial snapshot of the page‚Äôs DOM structure\nEvent Recording: It records all DOM mutations and user interactions as they happen\nReplay: It reconstructs the session by applying the recorded events to the initial snapshot\n\n\n\n\nrrweb architecture diagram showing the recording and replay process\n\n\nLet‚Äôs dive deeper into the technical implementation of how rrweb captures these events:\n\n\n\n\n\n\n\n\nCategory\nElement/Interaction\nImplementation\n\n\n\n\nDOM Structure\nHTML Elements\nAll DOM elements in the page via snapshot() function\n\n\n\nText Content\nText within elements via Mutation observer\n\n\n\nAttributes\nElement attributes and properties via Mutation observer\n\n\n\nDOM Structure Changes\nElements being added or removed via Mutation observer\n\n\nUser Interactions\nMouse Movements\nCursor position tracking via Mouse/touch event listeners\n\n\n\nMouse Clicks\nLeft/right clicks on elements via Mouse interaction observer\n\n\n\nTouch Events\nTouch interactions on mobile devices via Touch event listeners\n\n\n\nScrolling\nVertical/horizontal scrolling via Scroll observer\n\n\n\nInput Values\nText entered in form fields via Input observer\n\n\n\nFocus/Blur\nElement focus and blur events via Mouse interaction observer\n\n\n\nSelection\nText selection ranges via Selection observer\n\n\n\nCheckbox/Radio Changes\nState changes of form controls via Input observer\n\n\nVisual Elements\nCSS Styles\nInline and external CSS via StyleSheet rule observer\n\n\n\nCSS Changes\nDynamic style modifications via StyleDeclaration observer\n\n\n\nCanvas 2D\nCanvas drawing operations via Canvas 2D observer\n\n\n\nWebGL Content\nWebGL canvas operations via WebGL observer\n\n\n\nFonts\nCustom font loading via Font observer\n\n\nMedia\nVideo Controls\nPlay, pause, seek, volume via Media interaction observer\n\n\n\nAudio Controls\nPlay, pause, seek, volume via Media interaction observer\n\n\nViewport\nWindow Resize\nBrowser window size changes via Viewport resize observer\n\n\n\nPage Navigation\nURL changes via Meta event recording\n\n\nAdvanced Elements\nShadow DOM\nElements in shadow DOM via Shadow DOM manager\n\n\n\nCustom Elements\nWeb component registration and behavior via Custom element observer\n\n\n\niframes\nContent inside same-origin iframes via iframe manager\n\n\n\nCross-Origin iframes\nContent inside cross-origin iframes via Cross-origin iframe manager\n\n\n\nAdopted Stylesheets\nProgrammatically created stylesheets via Adopted stylesheet observer\n\n\nPage State\nScroll Position\nPage and element scroll positions via Scroll observer\n\n\n\nElement Dimensions\nSize and position of elements captured during DOM changes\n\n\n\nVisibility\nElement visibility changes via Attribute mutation tracking\n\n\nCustom Data\nDeveloper Events\nCustom events defined by developers via Custom event API\n\n\n\nPlugin Data\nData from custom plugins via Plugin architecture\n\n\n\nThis comprehensive architecture allows rrweb to capture virtually every aspect of a web application, ensuring high-fidelity replays with minimal overhead. Each event is precisely timestamped and organized to maintain the exact sequence of user interactions and visual changes.\n\n\n\n\n\n\nNote\n\n\n\nThis architecture captures virtually every aspect of a web application, ensuring high-fidelity replays with minimal overhead. Each event is precisely timestamped and organized to maintain the exact sequence of user interactions and visual changes.\n\n\n\n\nUnderstanding RRWeb‚Äôs Data Serialization Process\nAll of this sophisticated capturing is made possible through rrweb‚Äôs powerful data serialization system. Let‚Äôs peek under the hood to understand how rrweb converts complex browser events into storable JSON formats.\nWhen rrweb records a session, it creates a sequence of serialized events. Each event is a JSON object with a specific structure:\n{\n  type: EventType, // Numeric identifier for the event type\n  data: {/* Event-specific data */},\n  timestamp: 1615482345678 // Unix timestamp when the event occurred\n  sessionId: \"1234567890\" // Unique identifier for the session\n}\n\nRRWeb Event Type Numerical Values\nTo make the serialized data more compact, rrweb uses numerical values instead of strings to identify different types of events. Here‚Äôs what these numbers represent:\n// Main event types\n{\n  DomContentLoaded: 0,\n  Load: 1,\n  FullSnapshot: 2,\n  IncrementalSnapshot: 3,\n  Meta: 4,\n  Custom: 5,\n  Plugin: 6\n}\n\n// Incremental snapshot sources (used when type = 3)\n{\n  Mutation: 0,           // DOM changes\n  MouseMove: 1,          // Mouse movement\n  MouseInteraction: 2,   // Mouse clicks, focus, blur, etc.\n  Scroll: 3,             // Scrolling\n  ViewportResize: 4,     // Window resizing\n  Input: 5,              // Input field changes\n  TouchMove: 6,          // Touch screen movement\n  MediaInteraction: 7,   // Video/audio player interactions\n  StyleSheetRule: 8,     // CSS rule changes\n  CanvasMutation: 9,     // Canvas drawing operations\n  Font: 10,              // Font loading\n  Log: 11,               // Console logs\n  Drag: 12,              // Drag and drop\n  StyleDeclaration: 13,  // Inline style changes\n  Selection: 14,         // Text selection\n  AdoptedStyleSheet: 15, // Constructed stylesheets\n  CustomElement: 16      // Web Components\n}\n\n// Mouse interaction types (used when source = 2)\n{\n  MouseUp: 0,\n  MouseDown: 1,\n  Click: 2,\n  ContextMenu: 3,\n  DblClick: 4,\n  Focus: 5,\n  Blur: 6,\n  TouchStart: 7,\n  TouchEnd: 9,\n  TouchCancel: 10\n}\nThese numerical identifiers appear throughout the serialized events and are crucial for correctly interpreting the recording data during replay.\nLet‚Äôs examine how different aspects of a web session are encoded:\n\n\nDOM Structure Serialization\nThe initial DOM snapshot is one of the most complex parts of the recording:\n{\n  type: 2, // FullSnapshot event\n  data: {\n    node: {\n      type: 1, // Element node\n      tagName: \"html\",\n      attributes: {/* HTML attributes */},\n      childNodes: [/* Recursive tree of DOM nodes */]\n    },\n    initialOffset: {\n      left: 0,\n      top: 0\n    }\n  },\n  timestamp: 1615482345678,\n  sessionId: \"1234567890\"\n}\nEach DOM node receives a unique ID, which is then referenced in subsequent events rather than repeating the entire node information. This ‚Äúmirror system‚Äù is key to keeping data sizes manageable.\n\n\nUser Interactions\nMouse movements, clicks, and other user interactions are captured as incremental events:\n{\n  type: 3, // IncrementalSnapshot event\n  data: {\n    source: 1, // MouseMove event source\n    positions: [\n      {x: 100, y: 200, id: 42, timeOffset: 123} // Mouse position\n    ]\n  },\n  timestamp: 1615482345678,\n  sessionId: \"1234567890\"\n}\nFor high-frequency events like mouse movements, rrweb employs sampling techniques to reduce data size while maintaining visual fidelity.\n\n\nDOM Changes\nAs users interact with the page, rrweb records only the changes to the DOM rather than full snapshots:\n{\n  type: 3, // IncrementalSnapshot event\n  data: {\n    source: 0, // Mutation event\n    adds: [/* Elements added to the DOM */],\n    removes: [/* Elements removed from the DOM */],\n    texts: [/* Text content changes */],\n    attributes: [/* Attribute modifications */]\n  },\n  timestamp: 1615482345678,\n  sessionId: \"1234567890\"\n}\nThis incremental update approach drastically reduces data size compared to capturing full DOM snapshots repeatedly.\n\n\nAdvanced Features\nrrweb also handles complex browser features like Canvas operations, WebGL content, CSS changes, and Shadow DOM:\n{\n  type: 3, // IncrementalSnapshot event\n  data: {\n    source: 7, // CanvasMutation\n    id: 45, // Canvas element ID\n    commands: [\n      {\n        property: \"fillStyle\",\n        args: [\"#ff0000\"],\n        setter: true\n      },\n      {\n        property: \"fillRect\",\n        args: [0, 0, 100, 100]\n      }\n    ]\n  },\n  timestamp: 1615482345678,\n  sessionId: \"1234567890\"\n}\nThe serialization process follows a consistent pattern:\n\nBrowser events trigger rrweb observer callbacks\nThese callbacks format the data into standardized event objects\nEvents are timestamped and wrapped as eventWithTime objects\nThe data is serialized to a JSON-compatible format\nOptional compression may be applied\nFinally, the data is emitted through the provided callback\n\nThis elegant serialization system is what enables rrweb to capture the complete essence of a web session with remarkably small data sizes, typically just kilobytes per minute of recording.\n\n\n\nUnderstanding RRWeb‚Äôs Deserialization Process\nAfter recording and storing these events, rrweb needs to transform them back into a visual experience. Let‚Äôs examine how the deserialization and replay process works.\n\nHow RRWeb Deserializes and Replays Events\nThe replay process involves several sophisticated steps:\n\n1. Initialization and Setup\nWhen creating a Replayer instance, the following happens:\nconst replayer = new Replayer(events, options);\n\nAn iframe is created to serve as an isolated environment for the replay\nA ‚Äúmirror‚Äù system is initialized to map serialized node IDs to actual DOM nodes\nEvents are sorted chronologically by timestamp\nTimers are prepared to handle the playback timing\n\n\n\n2. Initial DOM Reconstruction\nThe first critical step is rebuilding the DOM from the initial snapshot:\n// Conceptual code of what happens internally\nfunction rebuildFullSnapshot(event) {\n  // Create DOM nodes from the serialized snapshot\n  const rootNode = createFromSerializedNode(event.data.node);\n  \n  // Insert into the iframe document\n  iframeDocument.documentElement.replaceWith(rootNode);\n  \n  // Restore initial scroll position\n  iframeWindow.scrollTo(event.data.initialOffset);\n}\nThis process recursively builds actual DOM elements from the serialized node tree, preserving all attributes, text content, and parent-child relationships.\n\n\n3. Incremental Event Application\nOnce the DOM is established, the replayer processes each incremental event based on its type:\n\nDOM Mutations: Adds, removes, or modifies elements in the DOM\nMouse Movements: Updates cursor position and hover states\nInputs: Changes form field values\nScrolling: Adjusts scroll positions\nCanvas Operations: Reapplies drawing commands to canvas elements\n\nFor example, a mouse movement event is processed like this:\n// Simplified internal processing\nfunction applyMouseMove(event) {\n  const { positions } = event.data;\n  \n  positions.forEach(position =&gt; {\n    // Move the mouse cursor visual element\n    mouseCursor.style.left = `${position.x}px`;\n    mouseCursor.style.top = `${position.y}px`;\n    \n    // Update hover state if needed\n    if (position.id) {\n      const targetElement = mirror.getNode(position.id);\n      if (targetElement) {\n        // Simulate hover effects\n        updateElementHoverState(targetElement);\n      }\n    }\n  });\n}\n\n\n4. Timing and Playback Control\nA sophisticated timing system ensures events are replayed with the correct timing relationships:\n// Simplified timer mechanism\nfunction scheduleEvents(events) {\n  const baseTime = events[0].timestamp;\n  \n  events.forEach(event =&gt; {\n    const delay = event.timestamp - baseTime;\n    setTimeout(() =&gt; applyEvent(event), delay * playbackSpeed);\n  });\n}\nThis allows for features like: - Variable playback speed (1x, 2x, 4x) - Pausing at specific timestamps - Jumping to particular points in the recording\n\n\n5. Special Case Handling\nSeveral types of content require special handling:\n\nImages: Recreated from encoded data or loaded from URLs\nCanvas: Drawing commands are reapplied to the canvas context\nStylesheets: CSS rules are reinserted in the correct order\nIframes: Content is rebuilt within nested browsing contexts\nInput Masking: Sensitive data might be masked during replay\n\n\n\n6. Optimization Techniques\nFor performance, especially during fast-forwarding, the replayer uses several optimizations:\n\nVirtual DOM: Can apply events to a lightweight virtual representation first\nBatched Updates: Groups DOM operations for better performance\nLazy Loading: Defers loading of non-essential resources\nEvent Sampling: May skip redundant events during high-speed playback",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üåê **Web Technologies**",
      "What is rrweb?"
    ]
  },
  {
    "objectID": "posts/2025-03-14_what_is_rrweb.html#implementing-rrweb-in-your-project",
    "href": "posts/2025-03-14_what_is_rrweb.html#implementing-rrweb-in-your-project",
    "title": "What is rrweb?",
    "section": "Implementing rrweb in Your Project",
    "text": "Implementing rrweb in Your Project\nNow that we understand how rrweb works, how it serializes data, and how it replays sessions, let‚Äôs implement it in a real project. We‚Äôll cover:\n\nRecording sessions\nSaving the recordings\nReplaying recordings\nConverting recordings to videos and images\n\n\nBasic Recording Implementation\nFirst, let‚Äôs set up a basic recording mechanism. Here‚Äôs the HTML code for a simple recording component:\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n  &lt;title&gt;rrweb Recording Example&lt;/title&gt;\n  &lt;style&gt;\n    .recording {\n      background-color: #f44336;\n      color: white;\n    }\n  &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;rrweb Recording Example&lt;/h1&gt;\n  \n  &lt;button id=\"recordButton\"&gt;Start Recording&lt;/button&gt;\n  &lt;div id=\"status\"&gt;Ready to record&lt;/div&gt;\n  \n  &lt;!-- Load rrweb from CDN --&gt;\n  &lt;script src=\"https://cdn.jsdelivr.net/npm/rrweb@latest/dist/rrweb.min.js\"&gt;&lt;/script&gt;\n  \n  &lt;script&gt;\n    // Global variables\n    let events = [];\n    let stopFn = null;\n    let isRecording = false;\n    \n    // DOM Elements\n    const recordButton = document.getElementById('recordButton');\n    const statusElement = document.getElementById('status');\n    \n    // Function to toggle recording state\n    function toggleRecording() {\n      if (!isRecording) {\n        // Start recording\n        events = []; // Clear previous events\n        \n        // Start rrweb recording\n        stopFn = rrweb.record({\n          emit(event) {\n            events.push(event);\n          },\n        });\n        \n        isRecording = true;\n      } else {\n        // Stop recording\n        if (stopFn) {\n          stopFn();\n          stopFn = null;\n        }\n        \n        // Store in localStorage\n        localStorage.setItem('rrweb-events', JSON.stringify(events));\n        \n        isRecording = false;\n      }\n    }\n    \n    // Event listeners\n    recordButton.addEventListener('click', toggleRecording);\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nTry it out yourself:\n\n\nThe recorded events are stored as a series of JSON objects that describe everything from mouse movements to DOM changes. A typical event might look something like this:\n{\n  type: 3, // Event type (3 represents a mouse move)\n  data: {\n    source: 0, // Source of the event\n    positions: [{x: 100, y: 200, id: 1, timeOffset: 123}] // Mouse position\n  },\n  timestamp: 1615482345678 // When the event occurred\n}\n\n\nReplaying Sessions\nTo replay a recorded session, you can use a basic replayer like this:\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n  &lt;title&gt;rrweb Replay Example&lt;/title&gt;\n  &lt;style&gt;\n    #replayContainer {\n      width: 100%;\n      height: 400px;\n      border: 1px solid #ccc;\n      margin-top: 20px;\n    }\n  &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;rrweb Replay Example&lt;/h1&gt;\n  \n  &lt;div&gt;\n    &lt;button id=\"playButton\"&gt;Play&lt;/button&gt;\n    &lt;button id=\"pauseButton\"&gt;Pause&lt;/button&gt;\n    &lt;button id=\"loadFromStorageButton\"&gt;Load from Storage&lt;/button&gt;\n  &lt;/div&gt;\n  \n  &lt;div id=\"replayContainer\"&gt;&lt;/div&gt;\n  \n  &lt;!-- Load rrweb from CDN --&gt;\n  &lt;script src=\"https://cdn.jsdelivr.net/npm/rrweb@latest/dist/rrweb.min.js\"&gt;&lt;/script&gt;\n  \n  &lt;script&gt;\n    // DOM Elements\n    const playButton = document.getElementById('playButton');\n    const pauseButton = document.getElementById('pauseButton');\n    const loadButton = document.getElementById('loadFromStorageButton');\n    const replayContainer = document.getElementById('replayContainer');\n    \n    // Global variables\n    let replayer = null;\n    let events = [];\n    \n    // Load from localStorage\n    function loadFromStorage() {\n      const storedEvents = localStorage.getItem('rrweb-events');\n      if (storedEvents) {\n        events = JSON.parse(storedEvents);\n        \n        // Create replayer\n        replayer = new rrweb.Replayer(events, {\n          root: replayContainer,\n          speed: 1,\n          showMouseIndicator: true,\n        });\n      }\n    }\n    \n    // Event listeners\n    playButton.addEventListener('click', () =&gt; replayer && replayer.play());\n    pauseButton.addEventListener('click', () =&gt; replayer && replayer.pause());\n    loadButton.addEventListener('click', loadFromStorage);\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nSee it in action:\n\n\nFor a more feature-rich player with built-in controls, you can use the rrweb-player:\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n  &lt;title&gt;rrweb Player with Controls&lt;/title&gt;\n  &lt;!-- Load rrweb player CSS --&gt;\n  &lt;link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/rrweb-player@latest/dist/style.css\"&gt;\n  &lt;style&gt;\n    #playerContainer {\n      width: 100%;\n      margin-top: 20px;\n    }\n  &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;rrweb Player with Controls&lt;/h1&gt;\n  \n  &lt;button id=\"loadFromStorageButton\"&gt;Load from Storage&lt;/button&gt;\n  &lt;div id=\"playerContainer\"&gt;&lt;/div&gt;\n  \n  &lt;!-- Load rrweb and rrweb-player from CDN --&gt;\n  &lt;script src=\"https://cdn.jsdelivr.net/npm/rrweb@latest/dist/rrweb.min.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"https://cdn.jsdelivr.net/npm/rrweb-player@latest/dist/index.js\"&gt;&lt;/script&gt;\n  \n  &lt;script&gt;\n    // DOM Elements\n    const loadButton = document.getElementById('loadFromStorageButton');\n    const playerContainer = document.getElementById('playerContainer');\n    \n    // Load from localStorage\n    function loadFromStorage() {\n      const storedEvents = localStorage.getItem('rrweb-events');\n      if (storedEvents) {\n        const events = JSON.parse(storedEvents);\n        \n        // Create player\n        new rrwebPlayer({\n          target: playerContainer,\n          props: {\n            events,\n            width: playerContainer.clientWidth,\n            height: 600,\n            showController: true,\n            autoPlay: false,\n            speedOption: [1, 2, 4]\n          }\n        });\n      }\n    }\n    \n    // Event listeners\n    loadButton.addEventListener('click', loadFromStorage);\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nSee the enhanced player in action:\n\n\n\n\nReal-World Applications\nrrweb is particularly valuable for:\n\nDebugging: Developers can see exactly what users were doing when errors occurred\nUX Research: Product teams can observe how real users interact with their websites\nCustomer Support: Support teams can see what customers are experiencing without screen sharing\nAnalytics: Understanding user behavior through visual session replays",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üåê **Web Technologies**",
      "What is rrweb?"
    ]
  },
  {
    "objectID": "posts/2025-03-14_what_is_rrweb.html#conclusion",
    "href": "posts/2025-03-14_what_is_rrweb.html#conclusion",
    "title": "What is rrweb?",
    "section": "Conclusion",
    "text": "Conclusion\nrrweb provides a powerful way to capture detailed web sessions without traditional screen recording. By integrating it with standard HTML and JavaScript, we can create interactive visualizations and analyses of user sessions.\nWhether you‚Äôre debugging customer issues, conducting UX research, or analyzing user behavior at scale, rrweb offers a sophisticated solution for web session recording and replay.\nIn the final section, we‚Äôll look at performance considerations and best practices for implementing rrweb in production environments.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üåê **Web Technologies**",
      "What is rrweb?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html",
    "href": "posts/2025-03-03_what_is_an_agent.html",
    "title": "What is an AI Agent?",
    "section": "",
    "text": "In recent years, AI agents have emerged as one of the most exciting developments in artificial intelligence. But what exactly is an AI agent? In this comprehensive guide, we‚Äôll explore the definition, components, and applications of AI agents, and why they represent a significant step forward in the evolution of AI systems.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html#defining-ai-agents",
    "href": "posts/2025-03-03_what_is_an_agent.html#defining-ai-agents",
    "title": "What is an AI Agent?",
    "section": "ü§ñ Defining AI Agents",
    "text": "ü§ñ Defining AI Agents\nAn AI agent is an autonomous system that can perceive its environment, make decisions, and take actions to achieve specific goals. Unlike traditional AI systems that perform isolated tasks, agents operate continuously in dynamic environments, learning and adapting as they interact with the world around them.\nThe key characteristics that define an AI agent include:\n\nAutonomy: Agents operate without direct human intervention\nPerception: They can sense and interpret their environment\nDecision-making: They can evaluate options and choose actions\nAction: They can execute decisions that affect their environment\nLearning: They can improve performance through experience\nGoal-orientation: They work toward specific objectives\n\n\nThis perception-decision-action loop forms the foundation of agent behavior, creating systems that can respond dynamically to changing conditions.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html#core-components-of-ai-agents",
    "href": "posts/2025-03-03_what_is_an_agent.html#core-components-of-ai-agents",
    "title": "What is an AI Agent?",
    "section": "üß© Core Components of AI Agents",
    "text": "üß© Core Components of AI Agents\nModern AI agents typically consist of several key components working together:\n\n1. Perception System\nThe perception system serves as the agent‚Äôs ‚Äúsenses,‚Äù allowing it to gather information about its environment. This might include:\n\nNatural language understanding for processing text\nComputer vision for interpreting images and video\nAudio processing for understanding speech and sounds\nSensor data interpretation for physical agents (robots)\n\n# Example of a simple perception system\ndef perceive_environment(agent, environment):\n    # Process text input\n    if environment.has_text():\n        text = environment.get_text()\n        agent.memory.add(text_processor.process(text))\n    \n    # Process visual input\n    if environment.has_image():\n        image = environment.get_image()\n        agent.memory.add(vision_processor.process(image))\n    \n    # Return the updated state\n    return agent.current_state\n\n\n2. Memory and Knowledge Base\nAgents need both short-term and long-term memory to function effectively:\n\nWorking memory: Holds current context and recent interactions\nLong-term memory: Stores knowledge, experiences, and learned patterns\nEpisodic memory: Records sequences of events and interactions\nSemantic memory: Organizes conceptual knowledge and relationships\n\nModern agent architectures often use vector databases, knowledge graphs, or hybrid approaches to manage this information efficiently.\n\n\n3. Reasoning Engine\nThe reasoning engine is the ‚Äúbrain‚Äù of the agent, responsible for:\n\nPlanning sequences of actions\nMaking decisions based on available information\nSolving problems through logical reasoning\nHandling uncertainty and probabilistic reasoning\n\nLarge language models (LLMs) have become popular reasoning engines due to their ability to perform complex reasoning tasks through techniques like chain-of-thought prompting.\n\n\n4. Action System\nThe action system translates decisions into concrete operations:\n\nAPI calls to external services\nText generation for communication\nControl signals for physical actuators (in robots)\nDatabase queries or modifications\n\n# Example of a simple action system\ndef execute_action(agent, action, environment):\n    if action.type == \"API_CALL\":\n        response = api_handler.call(\n            action.endpoint, \n            action.parameters\n        )\n        agent.memory.add(response)\n        \n    elif action.type == \"GENERATE_TEXT\":\n        text = agent.llm.generate(action.prompt)\n        environment.display(text)\n        agent.memory.add(text)\n    \n    # Return the result of the action\n    return action.result\n\n\n5. Learning Mechanism\nAgents improve over time through various learning approaches:\n\nSupervised learning from human feedback\nReinforcement learning from environmental rewards\nImitation learning from demonstrations\nSelf-supervised learning from exploration",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html#the-agent-loop-how-ai-agents-work",
    "href": "posts/2025-03-03_what_is_an_agent.html#the-agent-loop-how-ai-agents-work",
    "title": "What is an AI Agent?",
    "section": "üîÑ The Agent Loop: How AI Agents Work",
    "text": "üîÑ The Agent Loop: How AI Agents Work\nThe operation of an AI agent follows a continuous cycle:\n\nObserve: The agent gathers information through its perception systems\nOrient: It updates its internal state and understanding of the situation\nDecide: It evaluates possible actions and selects the most promising one\nAct: It executes the chosen action\nLearn: It observes the results and updates its knowledge and strategies\n\nThis loop, inspired by the OODA (Observe, Orient, Decide, Act) framework from military strategy, allows agents to continuously adapt to changing circumstances.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html#types-of-ai-agents",
    "href": "posts/2025-03-03_what_is_an_agent.html#types-of-ai-agents",
    "title": "What is an AI Agent?",
    "section": "üõ†Ô∏è Types of AI Agents",
    "text": "üõ†Ô∏è Types of AI Agents\nAI agents come in various forms, each designed for specific purposes:\n\nSimple Reflex Agents\nThese agents select actions based solely on current perceptions, using condition-action rules:\ndef simple_reflex_agent(perception):\n    if \"error\" in perception:\n        return \"troubleshoot_error\"\n    elif \"question\" in perception:\n        return \"answer_question\"\n    else:\n        return \"default_action\"\n\n\nModel-Based Agents\nThese agents maintain an internal model of the world to make better decisions:\ndef model_based_agent(perception, world_model):\n    # Update the world model with new perception\n    world_model.update(perception)\n    \n    # Predict outcomes of possible actions\n    possible_actions = [\"action1\", \"action2\", \"action3\"]\n    best_action = None\n    best_utility = -float('inf')\n    \n    for action in possible_actions:\n        predicted_state = world_model.predict(action)\n        utility = evaluate_utility(predicted_state)\n        \n        if utility &gt; best_utility:\n            best_utility = utility\n            best_action = action\n            \n    return best_action\n\n\nGoal-Based Agents\nThese agents select actions to achieve specific goals:\ndef goal_based_agent(perception, world_model, goal):\n    # Update the world model\n    world_model.update(perception)\n    \n    # Plan a sequence of actions to reach the goal\n    action_sequence = planner.find_path(\n        current_state=world_model.current_state,\n        goal_state=goal\n    )\n    \n    # Return the first action in the sequence\n    return action_sequence[0]\n\n\nUtility-Based Agents\nThese agents maximize a utility function that represents preferences:\ndef utility_based_agent(perception, world_model, utility_function):\n    # Update the world model\n    world_model.update(perception)\n    \n    # Evaluate all possible actions\n    possible_actions = world_model.get_possible_actions()\n    best_action = None\n    best_utility = -float('inf')\n    \n    for action in possible_actions:\n        for outcome, probability in world_model.predict_outcomes(action):\n            expected_utility = probability * utility_function(outcome)\n            if expected_utility &gt; best_utility:\n                best_utility = expected_utility\n                best_action = action\n                \n    return best_action\n\n\nLearning Agents\nThese agents improve their performance through experience:\ndef learning_agent(perception, world_model, policy, learning_rate):\n    # Update the world model\n    world_model.update(perception)\n    \n    # Choose action based on current policy\n    action = policy.select_action(world_model.current_state)\n    \n    # Execute action and observe result\n    next_state, reward = world_model.simulate(action)\n    \n    # Update policy based on observed reward\n    policy.update(\n        state=world_model.current_state,\n        action=action,\n        reward=reward,\n        next_state=next_state,\n        learning_rate=learning_rate\n    )\n    \n    return action",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html#applications-of-ai-agents",
    "href": "posts/2025-03-03_what_is_an_agent.html#applications-of-ai-agents",
    "title": "What is an AI Agent?",
    "section": "üåê Applications of AI Agents",
    "text": "üåê Applications of AI Agents\nAI agents are being deployed across numerous domains:\n\nPersonal Assistants\nAgents like ChatGPT, Claude, and Gemini help users with tasks ranging from answering questions to scheduling appointments and managing information.\n\n\nBusiness Automation\nAgents can automate complex business processes like: - Customer service and support - Data analysis and reporting - Supply chain optimization - Marketing campaign management\n\n\nResearch and Discovery\nAgents accelerate scientific research by: - Generating and testing hypotheses - Analyzing research papers - Designing experiments - Synthesizing findings across disciplines\n\n\nSoftware Development\nCoding agents assist developers by: - Writing and debugging code - Explaining complex systems - Generating documentation - Testing software\n\n\nHealthcare\nMedical agents support healthcare providers by: - Analyzing patient data - Suggesting diagnoses - Monitoring treatment plans - Providing patient education",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html#the-future-of-ai-agents",
    "href": "posts/2025-03-03_what_is_an_agent.html#the-future-of-ai-agents",
    "title": "What is an AI Agent?",
    "section": "üîÆ The Future of AI Agents",
    "text": "üîÆ The Future of AI Agents\nAs AI technology continues to advance, we can expect several key developments in agent technology:\n\nMulti-Agent Systems\nFuture applications will involve multiple specialized agents working together, each with distinct roles and capabilities. These collaborative systems will be able to tackle more complex problems than any single agent could handle alone.\n\n\nEmbodied Agents\nAs robotics technology improves, we‚Äôll see more agents that can interact with the physical world, combining perception, reasoning, and physical manipulation.\n\n\nPersonalized Agents\nAgents will become increasingly personalized, learning user preferences and adapting to individual needs over time, creating more natural and effective human-AI collaboration.\n\n\nEthical Considerations\nThe development of increasingly autonomous agents raises important ethical questions:\n\nTransparency: How can we ensure agents‚Äô decision-making processes are understandable?\nAccountability: Who is responsible when an agent makes a mistake?\nPrivacy: How should agents handle sensitive personal information?\nAutonomy: What limits should be placed on agent capabilities?",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html#conclusion",
    "href": "posts/2025-03-03_what_is_an_agent.html#conclusion",
    "title": "What is an AI Agent?",
    "section": "Conclusion",
    "text": "Conclusion\nAI agents represent a significant evolution in artificial intelligence, moving beyond static algorithms to create systems that can perceive, decide, act, and learn in dynamic environments. By combining advanced perception, reasoning, memory, and action capabilities, these systems can tackle increasingly complex tasks with growing autonomy.\nAs agent technology continues to mature, we can expect to see these systems playing increasingly important roles across industries and in our daily lives. Understanding the fundamental concepts behind AI agents is essential for anyone looking to harness their potential or contribute to their development.\nWhether you‚Äôre a developer, researcher, business leader, or simply curious about the future of AI, the field of agent-based systems offers exciting possibilities and challenges that will shape the next generation of intelligent technology.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html#references",
    "href": "posts/2025-03-03_what_is_an_agent.html#references",
    "title": "What is an AI Agent?",
    "section": "References",
    "text": "References\n\nRussell, S. J., & Norvig, P. (2021). Artificial Intelligence: A Modern Approach (4th ed.). Pearson.\nWooldridge, M. (2020). An Introduction to MultiAgent Systems (2nd ed.). Wiley.\nSutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press.\nGao, J., Galley, M., & Li, L. (2019). Neural Approaches to Conversational AI. Foundations and Trends in Information Retrieval.\nPark, D. H., et al.¬†(2023). Generative Agents: Interactive Simulacra of Human Behavior. arXiv preprint arXiv:2304.03442.\nWeng, L. (2023). LLM Powered Autonomous Agents. Lil‚ÄôLog. https://lilianweng.github.io/posts/2023-06-23-agent/",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  }
]