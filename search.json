[
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": " Research notes",
    "section": "",
    "text": "Training Data Influence Analysis and Estimation A Survey\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nExtending the Context Window of LLMs\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html",
    "title": "Building ChatGPT from Scratch",
    "section": "",
    "text": "Building your own ChatGPT-like assistant is now more accessible than ever. With the release of powerful open-source models and efficient fine-tuning techniques, developers can create customized AI assistants tailored to specific use cases. In this comprehensive guide, we‚Äôll walk through the entire process of building a ChatGPT-like system from scratch.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html#understanding-chatgpt-architecture",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html#understanding-chatgpt-architecture",
    "title": "Building ChatGPT from Scratch",
    "section": "ü§ñ Understanding ChatGPT Architecture",
    "text": "ü§ñ Understanding ChatGPT Architecture\nAt its core, ChatGPT consists of several key components:\n\nBase Language Model: A large language model (LLM) trained on vast amounts of text data\nInstruction Fine-tuning: Training to follow instructions and generate helpful responses\nAlignment: Ensuring the model‚Äôs outputs align with human preferences and values\nDeployment Infrastructure: Systems to serve the model efficiently to users\n\n\nThe most critical aspect of building a ChatGPT-like system is the alignment process. Traditional approaches involve a multi-stage pipeline: first Supervised Fine-Tuning (SFT) to adapt the model to follow instructions, followed by preference alignment methods like Reinforcement Learning with Human Feedback (RLHF) or Direct Preference Optimization (DPO).\nHowever, newer techniques like ORPO (Odds Ratio Preference Optimization) now allow us to combine these stages, making the process more efficient.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-1-selecting-a-base-model",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-1-selecting-a-base-model",
    "title": "Building ChatGPT from Scratch",
    "section": "üíª Step 1: Selecting a Base Model",
    "text": "üíª Step 1: Selecting a Base Model\nFor our ChatGPT clone, we‚Äôll use Llama 3 8B, the latest open-weight model from Meta. This model offers an excellent balance of performance and resource requirements, making it ideal for custom development.\nLlama 3 was trained on approximately 15 trillion tokens (compared to 2T tokens for Llama 2) and features an 8,192 token context window. The model uses a new tokenizer with a 128K-token vocabulary, which reduces the number of tokens required to encode text by about 15%.\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n)\n\n# Model\nbase_model = \"meta-llama/Meta-Llama-3-8B\"\n\n# Configure quantization for efficient loading\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-2-preparing-training-data",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-2-preparing-training-data",
    "title": "Building ChatGPT from Scratch",
    "section": "üìö Step 2: Preparing Training Data",
    "text": "üìö Step 2: Preparing Training Data\nHigh-quality training data is crucial for building an effective assistant. We need two types of datasets:\n\nInstruction Dataset: Examples of prompts and helpful responses\nPreference Dataset: Pairs of responses where one is preferred over the other\n\nFor our project, we‚Äôll create a custom dataset combining several high-quality sources:\nfrom datasets import load_dataset\n\n# Load and prepare dataset\ndataset = load_dataset(\"mlabonne/chatgpt-training-mix\")\ndataset = dataset.shuffle(seed=42)\n\n# Format data for chat template\ndef format_chat_template(row):\n    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n    return row\n\ndataset = dataset.map(format_chat_template)\ndataset = dataset.train_test_split(test_size=0.05)",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-3-fine-tuning-with-orpo",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-3-fine-tuning-with-orpo",
    "title": "Building ChatGPT from Scratch",
    "section": "üîÑ Step 3: Fine-tuning with ORPO",
    "text": "üîÑ Step 3: Fine-tuning with ORPO\nNow we‚Äôll fine-tune our model using ORPO, which combines instruction tuning and preference alignment into a single process. This approach is more efficient than traditional methods and produces better results.\nfrom peft import LoraConfig, prepare_model_for_kbit_training\nfrom trl import ORPOConfig, ORPOTrainer, setup_chat_format\n\n# Prepare model for chat format\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = prepare_model_for_kbit_training(model)\n\n# Configure LoRA for parameter-efficient fine-tuning\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\n\n# Configure ORPO training\norpo_args = ORPOConfig(\n    learning_rate=5e-6,\n    beta=0.1,\n    lr_scheduler_type=\"linear\",\n    max_length=2048,\n    max_prompt_length=512,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_8bit\",\n    num_train_epochs=3,\n    output_dir=\"./chatgpt-model/\",\n)\n\n# Initialize trainer and start training\ntrainer = ORPOTrainer(\n    model=model,\n    args=orpo_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n)\ntrainer.train()\ntrainer.save_model(\"./chatgpt-model\")",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-4-evaluation-and-iteration",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-4-evaluation-and-iteration",
    "title": "Building ChatGPT from Scratch",
    "section": "üîç Step 4: Evaluation and Iteration",
    "text": "üîç Step 4: Evaluation and Iteration\nAfter training, we need to evaluate our model to ensure it meets our quality standards. We‚Äôll use a combination of automated benchmarks and human evaluation:\nfrom transformers import pipeline\n\n# Load the fine-tuned model\nmodel_path = \"./chatgpt-model\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Create a chat pipeline\nchat_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_length=1024,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n)\n\n# Test with sample prompts\ntest_prompts = [\n    \"Explain quantum computing in simple terms\",\n    \"Write a short poem about artificial intelligence\",\n    \"How can I improve my programming skills?\"\n]\n\nfor prompt in test_prompts:\n    formatted_prompt = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False)\n    response = chat_pipeline(formatted_prompt)\n    print(f\"Prompt: {prompt}\\nResponse: {response[0]['generated_text']}\\n\")\nBased on evaluation results, we may need to iterate on our training data or fine-tuning approach to improve performance.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-5-deployment",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html#step-5-deployment",
    "title": "Building ChatGPT from Scratch",
    "section": "üöÄ Step 5: Deployment",
    "text": "üöÄ Step 5: Deployment\nFinally, we‚Äôll deploy our ChatGPT clone as a web service that users can interact with:\nimport gradio as gr\nfrom transformers import pipeline\n\n# Load model and create pipeline\nmodel_path = \"./chatgpt-model\"\nchat_pipeline = pipeline(\n    \"text-generation\",\n    model=model_path,\n    tokenizer=model_path,\n    max_length=1024,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    device_map=\"auto\"\n)\n\n# Chat history management\ndef format_history(history):\n    formatted_history = []\n    for human, assistant in history:\n        formatted_history.append({\"role\": \"user\", \"content\": human})\n        if assistant:\n            formatted_history.append({\"role\": \"assistant\", \"content\": assistant})\n    return formatted_history\n\n# Response generation function\ndef generate_response(message, history):\n    formatted_history = format_history(history)\n    formatted_history.append({\"role\": \"user\", \"content\": message})\n    \n    prompt = tokenizer.apply_chat_template(formatted_history, tokenize=False)\n    response = chat_pipeline(prompt)[0][\"generated_text\"]\n    \n    # Extract just the assistant's response\n    assistant_response = response.split(\"assistant\\n\")[-1].strip()\n    return assistant_response\n\n# Create Gradio interface\ndemo = gr.ChatInterface(\n    generate_response,\n    title=\"My ChatGPT Clone\",\n    description=\"Ask me anything!\",\n    theme=\"soft\"\n)\n\n# Launch the web interface\ndemo.launch(share=True)",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html#conclusion",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html#conclusion",
    "title": "Building ChatGPT from Scratch",
    "section": "Conclusion",
    "text": "Conclusion\nBuilding your own ChatGPT-like assistant is a complex but rewarding process. By following the steps outlined in this guide, you can create a customized AI assistant tailored to your specific needs. The key components include selecting a powerful base model, preparing high-quality training data, fine-tuning with modern techniques like ORPO, rigorous evaluation, and deployment as a user-friendly service.\nAs open-source models continue to improve, the gap between custom-built assistants and commercial offerings like ChatGPT is narrowing. This democratization of AI technology enables developers to create specialized assistants for various domains without relying on closed API services.\nI hope this guide helps you on your journey to building your own AI assistant. If you have any questions or want to share your creations, feel free to reach out to me on Twitter @maximelabonne.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "posts/2025-02-19_building_chatgpt_from_scratch.html#references",
    "href": "posts/2025-02-19_building_chatgpt_from_scratch.html#references",
    "title": "Building ChatGPT from Scratch",
    "section": "References",
    "text": "References\n\nJ. Hong, N. Lee, and J. Thorne, ORPO: Monolithic Preference Optimization without Reference Model. 2024.\nL. von Werra et al., TRL: Transformer Reinforcement Learning. GitHub, 2020. [Online]. Available: https://github.com/huggingface/trl\nAI at Meta, Introducing Meta Llama 3, 2024.\nAnthropic, Constitutional AI: Harmlessness from AI Feedback, 2022.\nOpenAI, Training language models to follow instructions with human feedback, 2022.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "üó£Ô∏è **Large Language Models**",
      "Building ChatGPT from Scratch"
    ]
  },
  {
    "objectID": "notes/Agents/data_influence.html",
    "href": "notes/Agents/data_influence.html",
    "title": "Training Data Influence Analysis and Estimation A Survey",
    "section": "",
    "text": "üìù Paper: https://arxiv.org/pdf/2212.04612.pdf\nSurvey of methods to calculate the influence of training samples."
  },
  {
    "objectID": "notes/Agents/data_influence.html#pointwise-training-data-influence",
    "href": "notes/Agents/data_influence.html#pointwise-training-data-influence",
    "title": "Training Data Influence Analysis and Estimation A Survey",
    "section": "Pointwise Training Data Influence",
    "text": "Pointwise Training Data Influence\nQuantifies how a single training instance affects the model‚Äôs prediction on a single test instance according to some quality measure (e.g., test loss). Œ∏^‚àó := \\text{arg min} \\frac{1}{|D|} \\sum_{(x_i, y_i) \\in D} (y_i ‚àí Œ∏^\\top x_i)^2 Early pointwise influence analysis shows that a single outlier can completely shift the parameters of a least-squares regression. Thus, this model is completely non-robust. Different models have been proposed to increase the breakdown point, including changing the average function with a median function.\nModern methods can be categorized into two classes:\n\nRetraining-based methods: measure the training data‚Äôs influence by repeatedly retraining a model f using different subsets of training set D.\nGradient-based influence estimators: estimate influence via the alignment of training and test instance gradients, either throughout or at the end of training.\n\n\nAlternative perspectives on influence\nThe concept of influence is not clearly standardized:\n\nGroup influence: think batches of training data\nJoint influence: consider multiple test instances collectively\nMemorization: defined as the self-influence I(z_i, z_i)\nCook‚Äôs distance: measures the effect of training instances on the model parameters themselves I_{Cook}(z_i) := \\theta^{(T)} - \\theta^{(T)}_{D^{\\backslash i}}\nExpected influence: average influence across different instantiations and retrainings of a model class\nInfluence ranking orders training instances from most positively influential to most negatively influential"
  },
  {
    "objectID": "notes/Agents/data_influence.html#retraining-based-influence-analysis",
    "href": "notes/Agents/data_influence.html#retraining-based-influence-analysis",
    "title": "Training Data Influence Analysis and Estimation A Survey",
    "section": "Retraining-Based Influence Analysis",
    "text": "Retraining-Based Influence Analysis\nMeasures influence by training a model with and without some instance. Influence is then defined as the difference in these two models‚Äô behavior.\n\nLeave-On-Out Influence\nLeave-one-out (LOO) is the simplest influence measure described in this work. LOO is also the oldest, dating back to Cook and Weisberg [CW82] who term it case deletion diagnostics. I_{LOO}(z_i, z_{te}) := L(z_{te}; Œ∏^{(T)}_{D^{\\backslash i}}) ‚àí L(z_{te}; Œ∏^{(T)} ), Measuring the entire training set‚Äôs LOO influence requires training (n + 1) models.\n\n\nDownsampling\nMitigates leave-one-out influence‚Äôs two primary weaknesses: (1) computational complexity dependent on n and (2) instability due to stochastic training variation.\nRelies on an ensemble of K submodels, each trained on a subset D^k or the full training set D.\n\nIntuitively, it corresponds to z_{te}‚Äôs average risk when z_i is not used in submodel training.\nBy holding out multiple instances simultaneously and then averaging, each Downsampling submodel provides insight into the influence of all training instances. This allows Downsampling to require (far) fewer retrainings than LOO.\n\n\nShapley Value\nIntuitively, SV is the weighted change in z_{te}‚Äôs risk when z_i is added to a random training subset.\nIt can be viewed as generalizing the leave-one-out influence, where rather than considering only the full training set D, Shapley value averages the LOO influence across all possible subsets of D.\nThe main problem is that SV is computationally intractable for non-trivial datasets, which led to numerous speed-ups in the literature:\n\nTruncated Monte Carlo Shapley (TMC-Shapley): relies on randomized subset sampling from training set D.\nGradient Shapley (G-Shapley): even faster SV estimator that assumes models are trained in just one gradient step (at the expense of lower accuracy).\nk-NN-SV and **k-NN Shapley"
  },
  {
    "objectID": "notes/Agents/data_influence.html#gradient-based-influence-estimation",
    "href": "notes/Agents/data_influence.html#gradient-based-influence-estimation",
    "title": "Training Data Influence Analysis and Estimation A Survey",
    "section": "Gradient-Based Influence Estimation",
    "text": "Gradient-Based Influence Estimation\nIn models trained using gradient descent, the influence of training instances can be assessed through training gradients.\nThere are two types of gradient-based methods:\n\nStatic methods estimate the effect of retraining by examining gradients with respect to final model parameters, but this approach typically requires stronger assumptions due to the limited insight a single set of parameters can provide into the optimization landscape.\nDynamic methods analyze model parameters throughout training, which while being more computationally demanding, allows for fewer assumptions.\n\nHowever, both share a common limitation: they can potentially overlook highly influential training instances.\n\nStatic Estimators\nThere are two main static estimators: influence functions (more general) and representer point (more scalable).\n\nInfluence Functions\nAnalyze how a model changes when the weight of a training instance is slightly perturbed: \\theta^{(T)}_{+ \\epsilon_i} = \\arg \\min_{\\theta} \\frac{1}{n} \\sum_{z \\in D} L(z; \\theta) + \\epsilon_i L(z_i; \\theta). Assuming the model and loss function are twice-differentiable and strictly convex, Cook and Weisberg demonstrated that an infinitesimal perturbation‚Äôs impact could be calculated using a first-order Taylor expansion: \\frac{d\\theta^{(T)}_{+\\epsilon_i}}{d\\epsilon_i} \\bigg|_{\\epsilon_i=0} = - (H^{(T)}_\\theta)^{-1} \\nabla_\\theta L(z_i; \\theta^{(T)}), where the empirical risk Hessian H^{(T)}_\\theta := \\frac{1}{n} \\sum_{z \\in D} \\nabla^2_\\theta L(z; \\theta^{(T)}) is assumed to be positive definite.\nKoh and Liang extend this result to consider the effect of this infinitesimal perturbation on z_{te}‚Äôs risk, whereby applying the chain rule, we get:\n\\begin{align*}\n\\frac{dL(z_{te}; \\theta^{(T)})}{d\\epsilon_i} \\bigg|_{\\epsilon_i=0}\n\n&= \\frac{dL(z_{te}; \\theta^{(T)})}{d\\theta^{(T)}_{+\\epsilon_i}}^\\top {\\frac{d\\theta^{(T)}_{+\\epsilon_i}}{d\\epsilon_i}} \\bigg|_{\\epsilon_i=0} \\\\\n\n&= - \\nabla_\\theta L(z_{te}; \\theta^{(T)})^\\top (H^{(T)}_\\theta)^{-1} \\nabla_\\theta L(z_i; \\theta^{(T)}).\n\\end{align*}\nRemoving training instance z_i from D is equivalent to \\epsilon_i = -\\frac{1}{n}, resulting in the pointwise influence functions estimator \\hat{I}_{IF}(z_i, z_{te}) := \\frac{1}{n} \\nabla_\\theta L(z_{te}; \\theta^{(T)})^\\top (H^{(T)}_\\theta)^{-1} \\nabla_\\theta L(z_i; \\theta^{(T)}) Intuitively, it represents the influence functions‚Äô estimate of the leave-one-out influence of z_i on z_{te}.\n\n\nRepresenter Point Methods\nRepresenter-based methods rely on kernels, which are functions that measure the similarity between two vectors. They decompose the predictions of specific model classes into the individual contributions (i.e., influence) of each training instance.\n\n\n\nDynamic Estimators\n\nTracIn ‚Äì Tracing Gradient Descent\n\n\nHyDRA ‚Äì Hypergradient Data Relevance Analysis"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " Articles",
    "section": "",
    "text": "Building ChatGPT from Scratch\n\n\nA step-by-step guide to creating your own AI assistant\n\n\n\nLarge Language Models\n\n\n\n\n\n\nFeb 19, 2025\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is an AI Agent?\n\n\nUnderstanding the core concepts, architecture, and applications of autonomous AI systems\n\n\n\nAI Agents\n\n\n\n\n\n\nApr 25, 2024\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/Large Language Models/extending_context.html",
    "href": "notes/Large Language Models/extending_context.html",
    "title": "Extending the Context Window of LLMs",
    "section": "",
    "text": "üìù Article: https://kaiokendev.github.io/context"
  },
  {
    "objectID": "notes/Large Language Models/extending_context.html#problem",
    "href": "notes/Large Language Models/extending_context.html#problem",
    "title": "Extending the Context Window of LLMs",
    "section": "Problem",
    "text": "Problem\nProblem: it is hard to extend the sequence length of a model.\n\nAnil et al.¬†(2022): the length extrapolation fails in part because of ‚Äúdistracting tokens‚Äù in the input during the PARITY task.\nChi et al.¬†(2022): bias terms in positional encoding (like in ALiBi) replicate the effect of windowed attention by decaying token inter-dependency on long-range receptive fields (the tokens only focus on the tokens closest to them).\nTao et al.¬†(2023) observe that, in long sequences, rear position embeddings are updates much fewer times than front position embeddings. They add random padding to the front patch of the sequence.\nLiu et al.¬†(2023): attention in long sequences starts to drift as we move to later positions and only attends to the most recent tokens."
  },
  {
    "objectID": "notes/Large Language Models/extending_context.html#silver-linings",
    "href": "notes/Large Language Models/extending_context.html#silver-linings",
    "title": "Extending the Context Window of LLMs",
    "section": "Silver Linings",
    "text": "Silver Linings\nThe attention mechanism seems destabilized in the case of long sequences due to an imbalance of attended tokens (either skewed to the front or the back).\nSeveral solutions have been proposed:\n\nFew-shot chain-of-thought reasoning and marker tokens\nLength generalization/extrapolation can be learned ability to a certain extent (improves performance but not a silver bullet)\nLLaMa 7B has been trained for retrieval over a 32K token window by introducing landmark tokens combined with a windowed-attention (blockwise computation)."
  },
  {
    "objectID": "notes/Large Language Models/extending_context.html#potential-solutions",
    "href": "notes/Large Language Models/extending_context.html#potential-solutions",
    "title": "Extending the Context Window of LLMs",
    "section": "Potential Solutions",
    "text": "Potential Solutions\n\nChange the attention calculation: log(n) scaling (does help), relacing the softmax with ReLU in the attention equation (does not converge), etc.\nRandom Positional Encoding\nShifted Positional Encodings: shifting the tokens progressively along the desired length during the encoding step (failure)."
  },
  {
    "objectID": "notes/Large Language Models/extending_context.html#final-solution",
    "href": "notes/Large Language Models/extending_context.html#final-solution",
    "title": "Extending the Context Window of LLMs",
    "section": "Final Solution",
    "text": "Final Solution\nTransformers do not learn how to gauge position based on the relative distance or the rotational factors, but memorize the tokens and their positional scaling factors.\n\nRotary positional embedding to loop the positions around after crossing the max context length (e.g., 2048): position_ids = position_ids % 2048\nBlock repeated positions: repeating the chosen frequency for a block of positions, so [1, 2, 3, 4, 5, ‚Ä¶, L] becomes [1, 1, 1, 1, 2, 2, 2, 2, 3, ‚Ä¶, L]. This is achieved by changing the frequency update: t *= 1/4.\n\nIn other words, several tokens (4 in this example) are assigned to the same position. This (surprising) scheme can quadruple the context length with minimal performance degradation (~2%). More information about it in this paper from Meta: https://arxiv.org/abs/2306.15595"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Earl Potters",
    "section": "",
    "text": "Earl Potters is an AI Engineer specializing in intelligent agents and large language models, based in San Francisco. Co-founded and built production-ready AI solutions at Missio AI and won numerous hackathons, including a $100K grand prize in Blockchain Journalism at Metabuild 2022 hackthon ‚Äì Refound Journalism.\nAn active blogger, Earl writes about AI, large language models, and autonomous agents. He has created numerous open-source projects including an evaluation automation tool, prompt-spec.com, and other contributions to the AI community. His GitHub showcases repositories spanning robotics, AI, and blockchain development.\nConnect with him on X and LinkedIn.\n\nCredits:\n\nEmojis used in figures are designed by OpenMoji, the open-source emoji and icon project. License: CC BY-SA 4.0.\nVector icons are provided by Streamline (https://streamlinehq.com). License: CC BY-SA 4.0."
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html",
    "href": "posts/2025-03-03_what_is_an_agent.html",
    "title": "What is an AI Agent?",
    "section": "",
    "text": "In recent years, AI agents have emerged as one of the most exciting developments in artificial intelligence. But what exactly is an AI agent? In this comprehensive guide, we‚Äôll explore the definition, components, and applications of AI agents, and why they represent a significant step forward in the evolution of AI systems.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html#defining-ai-agents",
    "href": "posts/2025-03-03_what_is_an_agent.html#defining-ai-agents",
    "title": "What is an AI Agent?",
    "section": "ü§ñ Defining AI Agents",
    "text": "ü§ñ Defining AI Agents\nAn AI agent is an autonomous system that can perceive its environment, make decisions, and take actions to achieve specific goals. Unlike traditional AI systems that perform isolated tasks, agents operate continuously in dynamic environments, learning and adapting as they interact with the world around them.\nThe key characteristics that define an AI agent include:\n\nAutonomy: Agents operate without direct human intervention\nPerception: They can sense and interpret their environment\nDecision-making: They can evaluate options and choose actions\nAction: They can execute decisions that affect their environment\nLearning: They can improve performance through experience\nGoal-orientation: They work toward specific objectives\n\n\nThis perception-decision-action loop forms the foundation of agent behavior, creating systems that can respond dynamically to changing conditions.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html#core-components-of-ai-agents",
    "href": "posts/2025-03-03_what_is_an_agent.html#core-components-of-ai-agents",
    "title": "What is an AI Agent?",
    "section": "üß© Core Components of AI Agents",
    "text": "üß© Core Components of AI Agents\nModern AI agents typically consist of several key components working together:\n\n1. Perception System\nThe perception system serves as the agent‚Äôs ‚Äúsenses,‚Äù allowing it to gather information about its environment. This might include:\n\nNatural language understanding for processing text\nComputer vision for interpreting images and video\nAudio processing for understanding speech and sounds\nSensor data interpretation for physical agents (robots)\n\n# Example of a simple perception system\ndef perceive_environment(agent, environment):\n    # Process text input\n    if environment.has_text():\n        text = environment.get_text()\n        agent.memory.add(text_processor.process(text))\n    \n    # Process visual input\n    if environment.has_image():\n        image = environment.get_image()\n        agent.memory.add(vision_processor.process(image))\n    \n    # Return the updated state\n    return agent.current_state\n\n\n2. Memory and Knowledge Base\nAgents need both short-term and long-term memory to function effectively:\n\nWorking memory: Holds current context and recent interactions\nLong-term memory: Stores knowledge, experiences, and learned patterns\nEpisodic memory: Records sequences of events and interactions\nSemantic memory: Organizes conceptual knowledge and relationships\n\nModern agent architectures often use vector databases, knowledge graphs, or hybrid approaches to manage this information efficiently.\n\n\n3. Reasoning Engine\nThe reasoning engine is the ‚Äúbrain‚Äù of the agent, responsible for:\n\nPlanning sequences of actions\nMaking decisions based on available information\nSolving problems through logical reasoning\nHandling uncertainty and probabilistic reasoning\n\nLarge language models (LLMs) have become popular reasoning engines due to their ability to perform complex reasoning tasks through techniques like chain-of-thought prompting.\n\n\n4. Action System\nThe action system translates decisions into concrete operations:\n\nAPI calls to external services\nText generation for communication\nControl signals for physical actuators (in robots)\nDatabase queries or modifications\n\n# Example of a simple action system\ndef execute_action(agent, action, environment):\n    if action.type == \"API_CALL\":\n        response = api_handler.call(\n            action.endpoint, \n            action.parameters\n        )\n        agent.memory.add(response)\n        \n    elif action.type == \"GENERATE_TEXT\":\n        text = agent.llm.generate(action.prompt)\n        environment.display(text)\n        agent.memory.add(text)\n    \n    # Return the result of the action\n    return action.result\n\n\n5. Learning Mechanism\nAgents improve over time through various learning approaches:\n\nSupervised learning from human feedback\nReinforcement learning from environmental rewards\nImitation learning from demonstrations\nSelf-supervised learning from exploration",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html#the-agent-loop-how-ai-agents-work",
    "href": "posts/2025-03-03_what_is_an_agent.html#the-agent-loop-how-ai-agents-work",
    "title": "What is an AI Agent?",
    "section": "üîÑ The Agent Loop: How AI Agents Work",
    "text": "üîÑ The Agent Loop: How AI Agents Work\nThe operation of an AI agent follows a continuous cycle:\n\nObserve: The agent gathers information through its perception systems\nOrient: It updates its internal state and understanding of the situation\nDecide: It evaluates possible actions and selects the most promising one\nAct: It executes the chosen action\nLearn: It observes the results and updates its knowledge and strategies\n\nThis loop, inspired by the OODA (Observe, Orient, Decide, Act) framework from military strategy, allows agents to continuously adapt to changing circumstances.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html#types-of-ai-agents",
    "href": "posts/2025-03-03_what_is_an_agent.html#types-of-ai-agents",
    "title": "What is an AI Agent?",
    "section": "üõ†Ô∏è Types of AI Agents",
    "text": "üõ†Ô∏è Types of AI Agents\nAI agents come in various forms, each designed for specific purposes:\n\nSimple Reflex Agents\nThese agents select actions based solely on current perceptions, using condition-action rules:\ndef simple_reflex_agent(perception):\n    if \"error\" in perception:\n        return \"troubleshoot_error\"\n    elif \"question\" in perception:\n        return \"answer_question\"\n    else:\n        return \"default_action\"\n\n\nModel-Based Agents\nThese agents maintain an internal model of the world to make better decisions:\ndef model_based_agent(perception, world_model):\n    # Update the world model with new perception\n    world_model.update(perception)\n    \n    # Predict outcomes of possible actions\n    possible_actions = [\"action1\", \"action2\", \"action3\"]\n    best_action = None\n    best_utility = -float('inf')\n    \n    for action in possible_actions:\n        predicted_state = world_model.predict(action)\n        utility = evaluate_utility(predicted_state)\n        \n        if utility &gt; best_utility:\n            best_utility = utility\n            best_action = action\n            \n    return best_action\n\n\nGoal-Based Agents\nThese agents select actions to achieve specific goals:\ndef goal_based_agent(perception, world_model, goal):\n    # Update the world model\n    world_model.update(perception)\n    \n    # Plan a sequence of actions to reach the goal\n    action_sequence = planner.find_path(\n        current_state=world_model.current_state,\n        goal_state=goal\n    )\n    \n    # Return the first action in the sequence\n    return action_sequence[0]\n\n\nUtility-Based Agents\nThese agents maximize a utility function that represents preferences:\ndef utility_based_agent(perception, world_model, utility_function):\n    # Update the world model\n    world_model.update(perception)\n    \n    # Evaluate all possible actions\n    possible_actions = world_model.get_possible_actions()\n    best_action = None\n    best_utility = -float('inf')\n    \n    for action in possible_actions:\n        for outcome, probability in world_model.predict_outcomes(action):\n            expected_utility = probability * utility_function(outcome)\n            if expected_utility &gt; best_utility:\n                best_utility = expected_utility\n                best_action = action\n                \n    return best_action\n\n\nLearning Agents\nThese agents improve their performance through experience:\ndef learning_agent(perception, world_model, policy, learning_rate):\n    # Update the world model\n    world_model.update(perception)\n    \n    # Choose action based on current policy\n    action = policy.select_action(world_model.current_state)\n    \n    # Execute action and observe result\n    next_state, reward = world_model.simulate(action)\n    \n    # Update policy based on observed reward\n    policy.update(\n        state=world_model.current_state,\n        action=action,\n        reward=reward,\n        next_state=next_state,\n        learning_rate=learning_rate\n    )\n    \n    return action",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html#applications-of-ai-agents",
    "href": "posts/2025-03-03_what_is_an_agent.html#applications-of-ai-agents",
    "title": "What is an AI Agent?",
    "section": "üåê Applications of AI Agents",
    "text": "üåê Applications of AI Agents\nAI agents are being deployed across numerous domains:\n\nPersonal Assistants\nAgents like ChatGPT, Claude, and Gemini help users with tasks ranging from answering questions to scheduling appointments and managing information.\n\n\nBusiness Automation\nAgents can automate complex business processes like: - Customer service and support - Data analysis and reporting - Supply chain optimization - Marketing campaign management\n\n\nResearch and Discovery\nAgents accelerate scientific research by: - Generating and testing hypotheses - Analyzing research papers - Designing experiments - Synthesizing findings across disciplines\n\n\nSoftware Development\nCoding agents assist developers by: - Writing and debugging code - Explaining complex systems - Generating documentation - Testing software\n\n\nHealthcare\nMedical agents support healthcare providers by: - Analyzing patient data - Suggesting diagnoses - Monitoring treatment plans - Providing patient education",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html#the-future-of-ai-agents",
    "href": "posts/2025-03-03_what_is_an_agent.html#the-future-of-ai-agents",
    "title": "What is an AI Agent?",
    "section": "üîÆ The Future of AI Agents",
    "text": "üîÆ The Future of AI Agents\nAs AI technology continues to advance, we can expect several key developments in agent technology:\n\nMulti-Agent Systems\nFuture applications will involve multiple specialized agents working together, each with distinct roles and capabilities. These collaborative systems will be able to tackle more complex problems than any single agent could handle alone.\n\n\nEmbodied Agents\nAs robotics technology improves, we‚Äôll see more agents that can interact with the physical world, combining perception, reasoning, and physical manipulation.\n\n\nPersonalized Agents\nAgents will become increasingly personalized, learning user preferences and adapting to individual needs over time, creating more natural and effective human-AI collaboration.\n\n\nEthical Considerations\nThe development of increasingly autonomous agents raises important ethical questions:\n\nTransparency: How can we ensure agents‚Äô decision-making processes are understandable?\nAccountability: Who is responsible when an agent makes a mistake?\nPrivacy: How should agents handle sensitive personal information?\nAutonomy: What limits should be placed on agent capabilities?",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html#conclusion",
    "href": "posts/2025-03-03_what_is_an_agent.html#conclusion",
    "title": "What is an AI Agent?",
    "section": "Conclusion",
    "text": "Conclusion\nAI agents represent a significant evolution in artificial intelligence, moving beyond static algorithms to create systems that can perceive, decide, act, and learn in dynamic environments. By combining advanced perception, reasoning, memory, and action capabilities, these systems can tackle increasingly complex tasks with growing autonomy.\nAs agent technology continues to mature, we can expect to see these systems playing increasingly important roles across industries and in our daily lives. Understanding the fundamental concepts behind AI agents is essential for anyone looking to harness their potential or contribute to their development.\nWhether you‚Äôre a developer, researcher, business leader, or simply curious about the future of AI, the field of agent-based systems offers exciting possibilities and challenges that will shape the next generation of intelligent technology.",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  },
  {
    "objectID": "posts/2025-03-03_what_is_an_agent.html#references",
    "href": "posts/2025-03-03_what_is_an_agent.html#references",
    "title": "What is an AI Agent?",
    "section": "References",
    "text": "References\n\nRussell, S. J., & Norvig, P. (2021). Artificial Intelligence: A Modern Approach (4th ed.). Pearson.\nWooldridge, M. (2020). An Introduction to MultiAgent Systems (2nd ed.). Wiley.\nSutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press.\nGao, J., Galley, M., & Li, L. (2019). Neural Approaches to Conversational AI. Foundations and Trends in Information Retrieval.\nPark, D. H., et al.¬†(2023). Generative Agents: Interactive Simulacra of Human Behavior. arXiv preprint arXiv:2304.03442.\nWeng, L. (2023). LLM Powered Autonomous Agents. Lil‚ÄôLog. https://lilianweng.github.io/posts/2023-06-23-agent/",
    "crumbs": [
      "{{< fa address-card >}} About",
      "ü§ñ **AI Agents**",
      "What is an AI Agent?"
    ]
  }
]