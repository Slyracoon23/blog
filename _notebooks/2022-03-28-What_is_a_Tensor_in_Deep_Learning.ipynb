{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# What is a Tensor in Deep Learning?\n",
        "> A definition in simple terms with¬†examples\n",
        "\n",
        "- toc: false \n",
        "- badges: true\n",
        "- comments: true\n",
        "- author: Maxime Labonne\n",
        "- categories: [data science, tensor, numpy, pytorch, tensorflow, deep learning]\n",
        "- image: images/tensor/thumbnail.png\n",
        "- permalink: /what-is-a-tensor/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wjjq6SRRSyR"
      },
      "source": [
        "<center><div id=\"anim_container\"><img alt=\"Thumbnail\" src=\"https://mlabonne.github.io/blog/images/tensor/thumbnail.png\" loading=\"lazy\" id=\"anim_inner\"></div></center>\n",
        "\n",
        "**What is a tensor, exactly?**\n",
        "\n",
        "Most **deep learning practitioners** know about them but **can't pinpoint an exact definition**.\n",
        "\n",
        "TensorFlow, PyTorch: every deep learning framework **relies on the same basic object**: tensors. They're **used to store almost everything** in deep learning: input data, weights, biases, predictions, etc.\n",
        "\n",
        "And yet, their **definition is incredibly fuzzy**: the <a href=\"https://en.wikipedia.org/wiki/Category:Tensors\">Wikipedia category</a> alone has **over 100 pages** related to tensors.\n",
        "\n",
        "In this article, we'll give **a definitive answer** to the following question: what is a tensor in neural networks?\n",
        "\n",
        "## üíª Tensors in computer science\n",
        "\n",
        "So why are there **so many definitions**?\n",
        "\n",
        "It's quite simple: **different fields have different definitions**. Tensors in **mathematics** are not quite the same as tensors in **physics**, which are different from tensors in **computer science**.\n",
        "\n",
        "<center><div id=\"anim_container\"><img alt=\"Data structure vs. Objects\" src=\"https://mlabonne.github.io/blog/images/tensor/datastructure.png\" loading=\"lazy\" id=\"anim_inner\"></div></center>\n",
        "\n",
        "These definitions can be **divided into two categories**: tensors as a data structure or as objects (in an [object-oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming) sense).\n",
        "\n",
        "* **Data structure**: this is the definition we use **in computer science**. Tensors are [**multidimensional arrays**](https://en.wikipedia.org/wiki/Array_data_type#Multi-dimensional_arrays) that store a **specific type** of value.\n",
        "* **Objects**: this is the definition used **in other fields**. In <a href=\"https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)\">mathematics</a> and <a href=\"https://en.wikipedia.org/wiki/Infinitesimal_strain_theory\">physics</a>, tensors are **not just** a data structure: they also have a **list of properties**, like a specific product.\n",
        "\n",
        "This is why you see a lot of people (sometimes quite pedantically) saying \"*tensors are **not** n-dimensional arrays/matrices*\": **they don't talk about data structures**, but **about objects with properties**.\n",
        "\n",
        "Even the **same words** have **different meanings**. For instance, in computer science, a **2D tensor is a matrix** (it's a tensor of rank 2). In linear algebra, a tensor with 2 dimensions means **it only stores two values**. The **rank** also has a completely **different definition**: it is the maximum number of its linearly independent column (or row) vectors.\n",
        "\n",
        "In computer science, we're only interested in a definition **focused on the data structure**. From this point of view, tensors **truly are** a generalization in $n$ dimensions of **matrices**.\n",
        "\n",
        "But we're still **missing an important nuance** when talking about tensors specifically in the context of **deep learning**...\n",
        "\n",
        "## üß† Tensors in deep learning\n",
        "\n",
        "<center><div id=\"anim_container\"><img alt=\"Array vs. Tensor\" src=\"https://mlabonne.github.io/blog/images/tensor/arrayvstensor.png\" loading=\"lazy\" id=\"anim_inner\" width=\"800\"></div>\n",
        "<i>Icons created by Freepik and smashingstocks - <a href=\"https://www.flaticon.com/free-icons/\">Flaticon</a></i></center>\n",
        "\n",
        "So **why are they called \"tensors\" instead of \"multidimensional arrays\"**? Ok, it is shorter, but is it all there is to it? Actually, people make an **implicit assumption when they talk about tensors**.\n",
        "\n",
        "PyTorch's <a href=\"https://pytorch.org/tutorials/beginner/examples_tensor/polynomial_tensor.html#:~:text=PyTorch%3A%20Tensors,-A%20third%20order&text=A%20PyTorch%20Tensor%20is%20basically,used%20for%20arbitrary%20numeric%20computation.\">official documentation</a> gives us a practical answer:\n",
        "\n",
        "> The biggest difference between a **numpy array and a PyTorch Tensor** is that a PyTorch Tensor can run on either **CPU or GPU**.\n",
        "\n",
        "In deep learning, we need **performance** to compute a lot of **matrix multiplications** in a **highly parallel** way. These matrices (and n-dimensional arrays in general) are generally **stored and processed on GPUs** to speed up training and inference times.\n",
        "\n",
        "This is what was **missing** in our previous definition: tensors in deep learning are **not just n-dimensional arrays**, there's also the implicit assumption they can be **run on a GPU**.\n",
        "\n",
        "## ‚öîÔ∏è NumPy vs PyTorch\n",
        "\n",
        "Let's see the difference between **NumPy arrays** and **PyTorch tensors**.\n",
        "\n",
        "<center><div id=\"anim_container\"><img alt=\"Sclar, vector, matrix\" src=\"https://mlabonne.github.io/blog/images/tensor/scalar.png\" loading=\"lazy\" id=\"anim_inner\" width=\"800\"></div></center>\n",
        "\n",
        "These two objects are **very similar**: we can initialize a 1D array and a 1D tensor with **nearly the same syntax**. They also **share a lot of methods** and can be easily **converted into one another**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af6ham967Aax",
        "outputId": "5fd43800-41c6-4d07-816d-f6db922fea67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NumPy Array: [1 2 3]\n",
            "PyTorch Tensor: tensor([1, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "array = np.array([1, 2, 3])\n",
        "print(f'NumPy Array: {array}')\n",
        "\n",
        "tensor = torch.tensor([1, 2, 3])\n",
        "print(f'PyTorch Tensor: {tensor}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXdjYo2FHc6S"
      },
      "source": [
        "Initializing 2D arrays and 2D tensors is **not more complicated**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc6Nos7g-Olq",
        "outputId": "2627ad8d-a75a-4f2e-cca8-783b526de425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NumPy Array:\n",
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "\n",
            "PyTorch Tensor:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        }
      ],
      "source": [
        "x = np.array([[1, 2, 3],\n",
        "              [4, 5, 6]])\n",
        "print(f'NumPy Array:\\n{x}')\n",
        "\n",
        "x = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "print(f'\\nPyTorch Tensor:\\n{x}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwXSpGhGHmGh"
      },
      "source": [
        "We said that the only **difference** between tensors and arrays was the fact that **tensors can be run on GPUs**. So in the end, this distinction is **based on performance**. But is this boost **that important**?\n",
        "\n",
        "Let's **compare the performance** between NumPy arrays and PyTorch tensors on matrix multiplication. In the following example, we randomly initialize **4D arrays/tensors and multiply them**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z448Qo93-45T"
      },
      "outputs": [],
      "source": [
        "# You need to run this code on a computer with a GPU\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# 4D arrays\n",
        "array1 = np.random.rand(100, 100, 100, 100)\n",
        "array2 = np.random.rand(100, 100, 100, 100)\n",
        "\n",
        "# 4D tensors\n",
        "tensor1 = torch.rand(100, 100, 100, 100).to(device)\n",
        "tensor2 = torch.rand(100, 100, 100, 100).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMrv4eTuAdkW",
        "outputId": "17758f7d-19d2-43bb-c4aa-af3ba574e326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 loop, best of 5: 1.32 s per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "np.matmul(array1, array2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OibOHbnFCgsl",
        "outputId": "8b7ee3f5-e069-4784-9119-12de23e4609c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 loops, best of 5: 25.2 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "torch.matmul(tensor1, tensor2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_RhSPzN-PKO"
      },
      "source": [
        "As we can see, PyTorch tensors completed outperformed NumPy arrays: they completed the multiplication **52 times faster**!\n",
        "\n",
        "This is the true power of tensors: they're **blazingly fast**! Performance might vary depending on the **dimensions**, the **implementation**, and the **hardware**, but this speed is the reason why tensors (and not arrays) are **so common** in deep learning.\n",
        "\n",
        "## üìù Conclusion\n",
        "\n",
        "In this article, we wrote a **definition** of tensors based on:\n",
        "\n",
        "1. Their use in **computer science** (data structure);\n",
        "2. More specifically, in **deep learning** (they can run on GPUs).\n",
        "\n",
        "Here's how we can **summarize** it in one sentence:\n",
        "\n",
        "> Tensors are **n-dimensional arrays** with the **implicit assumption** that they can **run on a GPU**.\n",
        "\n",
        "Finally, we saw the **difference of performance** between tensors and arrays, which motivates the **need for tensors in deep learning**.\n",
        "\n",
        "So next time someone tries to explain to you that **tensors are not exactly a generalization of matrices**, you'll know that they're **right in a particular definition** of tensors, but not in the computer science/deep learning one.\n",
        "\n",
        "If you're looking for more **data science** and **machine learning** content in n-dimensions, please **follow me on twitter [@maximelabonne](https://twitter.com/maximelabonne)**. You can find the code used in this article [at this address](https://github.com/mlabonne/how-to-data-science/blob/main/What_is_a_Tensor_in_Deep_Learning.ipynb). üì£"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "What is a Tensor in Deep Learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
