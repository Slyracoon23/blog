{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference-Based Fine-Tuning: DPO vs ORPO\n",
    "\n",
    "This notebook provides templates and examaples for two popular preference-based fine-tuning methods:\n",
    "- **DPO (Direct Preference Optimizatioan)**: Optimizes language models directly on preference data without requiring a reward model\n",
    "- **ORPO (Odds Ratio Preference Optimization)**: A reference-free approach that combines SFT and preference alignment in a single stage\n",
    "\n",
    "## Overview\n",
    "\n",
    "Preference-based fine-tuning methods aim to align language models with human preferences by training on pairs of responses where one is preferred over another. These methods have become crucial for creating more helpful, harmless, and honest AI systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Training Configuration\n",
    "class Config:\n",
    "    # Model settings\n",
    "    model_name = \"microsoft/DialoGPT-medium\"  # Change to your preferred base model\n",
    "    model_max_length = 512\n",
    "    \n",
    "    # LoRA settings\n",
    "    lora_r = 16\n",
    "    lora_alpha = 32\n",
    "    lora_dropout = 0.1\n",
    "    lora_target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "    \n",
    "    # Training settings\n",
    "    learning_rate = 5e-5\n",
    "    batch_size = 4\n",
    "    gradient_accumulation_steps = 4\n",
    "    num_epochs = 3\n",
    "    warmup_steps = 100\n",
    "    logging_steps = 10\n",
    "    save_steps = 500\n",
    "    eval_steps = 500\n",
    "    \n",
    "    # DPO specific\n",
    "    dpo_beta = 0.1  # Temperature parameter for DPO\n",
    "    \n",
    "    # ORPO specific\n",
    "    orpo_alpha = 1.0  # Weight for the SFT loss\n",
    "    orpo_beta = 0.1   # Weight for the preference loss\n",
    "    \n",
    "    # Output directories\n",
    "    output_dir = \"./results\"\n",
    "    logging_dir = \"./logs\"\n",
    "    \n",
    "    # Quantization (for memory efficiency)\n",
    "    use_4bit = True\n",
    "    bnb_4bit_compute_dtype = torch.float16\n",
    "    bnb_4bit_use_double_quant = True\n",
    "    bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install transformers datasets torch accelerate trl peft bitsandbytes wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "from trl import DPOTrainer, ORPOTrainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import logging\n",
    "import wandb\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "Both DPO and ORPO require preference datasets with the following structure:\n",
    "- **prompt**: The input/question\n",
    "- **chosen**: The preferred response\n",
    "- **rejected**: The less preferred response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_preference_dataset() -> Dataset:\n",
    "    \"\"\"\n",
    "    Create a sample preference dataset for demonstration.\n",
    "    In practice, you would load your actual preference data.\n",
    "    \"\"\"\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"prompt\": \"What's the best way to learn programming?\",\n",
    "            \"chosen\": \"Start with the basics like variables and control structures, then practice with small projects. Choose a beginner-friendly language like Python, and work through tutorials while building your own projects.\",\n",
    "            \"rejected\": \"Just memorize all the syntax and you'll be fine. Programming is just about knowing all the commands.\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"How do I stay motivated while learning?\",\n",
    "            \"chosen\": \"Set small, achievable goals and celebrate your progress. Find a community of learners, work on projects you're passionate about, and remember that everyone learns at their own pace.\",\n",
    "            \"rejected\": \"Just force yourself to study 12 hours a day. If you're not exhausted, you're not trying hard enough.\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"What's the most important skill for a programmer?\",\n",
    "            \"chosen\": \"Problem-solving is crucial. Programming is fundamentally about breaking down complex problems into smaller, manageable pieces and finding efficient solutions.\",\n",
    "            \"rejected\": \"Typing speed. The faster you can type, the better programmer you are.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return Dataset.from_list(sample_data)\n",
    "\n",
    "def load_preference_dataset(dataset_name: Optional[str] = None) -> Dataset:\n",
    "    \"\"\"\n",
    "    Load a preference dataset. You can use popular datasets like:\n",
    "    - Anthropic/hh-rlhf\n",
    "    - Intel/orca_dpo_pairs\n",
    "    - argilla/ultrafeedback-binarized-preferences\n",
    "    \"\"\"\n",
    "    if dataset_name:\n",
    "        # Load from HuggingFace Hub\n",
    "        dataset = load_dataset(dataset_name, split=\"train\")\n",
    "        return dataset\n",
    "    else:\n",
    "        # Use sample data for demonstration\n",
    "        return create_sample_preference_dataset()\n",
    "\n",
    "def preprocess_dataset(dataset: Dataset, tokenizer) -> Dataset:\n",
    "    \"\"\"Preprocess the dataset for training\"\"\"\n",
    "    def tokenize_function(examples):\n",
    "        # Tokenize prompts, chosen, and rejected responses\n",
    "        prompts = [f\"Human: {prompt}\\nAssistant: \" for prompt in examples[\"prompt\"]]\n",
    "        \n",
    "        model_inputs = {}\n",
    "        model_inputs[\"prompt\"] = prompts\n",
    "        model_inputs[\"chosen\"] = [f\"{prompt}{chosen}\" for prompt, chosen in zip(prompts, examples[\"chosen\"])]\n",
    "        model_inputs[\"rejected\"] = [f\"{prompt}{rejected}\" for prompt, rejected in zip(prompts, examples[\"rejected\"])]\n",
    "        \n",
    "        return model_inputs\n",
    "    \n",
    "    return dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load and preprocess data\n",
    "print(\"Loading preference dataset...\")\n",
    "train_dataset = load_preference_dataset()\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "print(\"Sample entry:\")\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Model Loading and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
