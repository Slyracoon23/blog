{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"Chapter 3: Generative Adversarial Networks (GANs)\"\n",
        "description: \"A comprehensive guide to understanding and implementing GANs using PyTorch\"\n",
        "date: \"2025-01-02\"\n",
        "categories: [generative-ai, pytorch, gans, deep-learning]\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Chapter 3: Generative Adversarial Networks (GANs)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Generative Adversarial Networks (GANs) are one of the most exciting developments in generative AI. Introduced by Ian Goodfellow in 2014, GANs use an adversarial training process where two neural networks compete against each other:\n",
        "\n",
        "- **Generator (G)**: Creates fake data that tries to fool the discriminator\n",
        "- **Discriminator (D)**: Distinguishes between real and fake data\n",
        "\n",
        "This adversarial process leads to increasingly realistic generated samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3.1 GAN Theory and Architecture\n",
        "\n",
        "### The GAN Game Theory\n",
        "\n",
        "GANs can be understood as a minimax game between two players:\n",
        "\n",
        "$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$$\n",
        "\n",
        "Where:\n",
        "- $G$ is the generator that maps noise $z$ to fake data\n",
        "- $D$ is the discriminator that outputs the probability that input is real\n",
        "- $p_{data}$ is the real data distribution\n",
        "- $p_z$ is the noise distribution (usually Gaussian)\n",
        "\n",
        "### Training Process\n",
        "\n",
        "1. **Train Discriminator**: Maximize ability to distinguish real from fake\n",
        "2. **Train Generator**: Minimize discriminator's ability to detect fakes\n",
        "3. **Alternate**: Between generator and discriminator training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the Generator Network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz=100, ngf=64, nc=1):\n",
        "        \"\"\"\n",
        "        nz: size of latent vector\n",
        "        ngf: generator feature map size\n",
        "        nc: number of output channels\n",
        "        \"\"\"\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size: (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size: (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size: (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size: (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size: (nc) x 64 x 64\n",
        "        )\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the Discriminator Network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, nc=1, ndf=64):\n",
        "        \"\"\"\n",
        "        nc: number of input channels\n",
        "        ndf: discriminator feature map size\n",
        "        \"\"\"\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size: (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size: (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size: (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size: (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "            # output size: 1 x 1 x 1\n",
        "        )\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1, 1).squeeze(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data loading and preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(64),\n",
        "    transforms.CenterCrop(64),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Create data loader\n",
        "batch_size = 128\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"Number of batches: {len(dataloader)}\")\n",
        "\n",
        "# Visualize some real images\n",
        "def show_images(images, title=\"Images\"):\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i < len(images):\n",
        "            # Denormalize for visualization\n",
        "            img = images[i].squeeze() * 0.5 + 0.5\n",
        "            ax.imshow(img, cmap='gray')\n",
        "            ax.axis('off')\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Show some real images\n",
        "real_batch = next(iter(dataloader))\n",
        "show_images(real_batch[0][:8], \"Real MNIST Images\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3.2 Training Setup and Hyperparameters\n",
        "\n",
        "Key training considerations for GANs:\n",
        "\n",
        "1. **Learning Rates**: Often different for G and D\n",
        "2. **Loss Functions**: Binary cross-entropy for standard GAN\n",
        "3. **Initialization**: Proper weight initialization is crucial\n",
        "4. **Training Balance**: Ensuring neither network dominates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize networks\n",
        "nz = 100  # Size of latent vector\n",
        "ngf = 64  # Generator feature map size\n",
        "ndf = 64  # Discriminator feature map size\n",
        "nc = 1    # Number of channels (grayscale)\n",
        "\n",
        "# Create networks\n",
        "netG = Generator(nz, ngf, nc).to(device)\n",
        "netD = Discriminator(nc, ndf).to(device)\n",
        "\n",
        "# Weight initialization function\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "# Apply weight initialization\n",
        "netG.apply(weights_init)\n",
        "netD.apply(weights_init)\n",
        "\n",
        "print(\"Generator Architecture:\")\n",
        "print(netG)\n",
        "print(\"\\nDiscriminator Architecture:\")\n",
        "print(netD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training setup\n",
        "criterion = nn.BCELoss()\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "\n",
        "# Optimizers\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "# Labels\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# Create batch of latent vectors for visualization\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 25\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        real_batch = data[0].to(device)\n",
        "        b_size = real_batch.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        \n",
        "        output = netD(real_batch)\n",
        "        errD_real = criterion(output, label)\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        output = netD(fake.detach())\n",
        "        errD_fake = criterion(output, label)\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        output = netD(fake)\n",
        "        errG = criterion(output, label)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print(f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}] '\n",
        "                  f'Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f} '\n",
        "                  f'D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f} / {D_G_z2:.4f}')\n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "\n",
        "    # Check how the generator is doing by saving G's output on fixed_noise\n",
        "    if (epoch % 5 == 0) or (epoch == num_epochs-1):\n",
        "        with torch.no_grad():\n",
        "            fake = netG(fixed_noise).detach().cpu()\n",
        "            show_images(fake[:8], f\"Generated Images - Epoch {epoch}\")\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses, label=\"G\")\n",
        "plt.plot(D_losses, label=\"D\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Generate new samples\n",
        "with torch.no_grad():\n",
        "    noise = torch.randn(16, nz, 1, 1, device=device)\n",
        "    generated_images = netG(noise).cpu()\n",
        "    show_images(generated_images, \"Final Generated Samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3.3 Common GAN Problems and Solutions\n",
        "\n",
        "### Mode Collapse\n",
        "- **Problem**: Generator produces limited variety of samples\n",
        "- **Solutions**: Unrolled GANs, Feature matching, Minibatch discrimination\n",
        "\n",
        "### Training Instability\n",
        "- **Problem**: Loss oscillations, non-convergence\n",
        "- **Solutions**: Proper learning rates, spectral normalization, gradient penalties\n",
        "\n",
        "### Vanishing Gradients\n",
        "- **Problem**: Generator receives no useful gradient signal\n",
        "- **Solutions**: Alternative loss functions (LSGAN, WGAN), careful architecture design\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3.4 GAN Variants and Improvements\n",
        "\n",
        "### DCGAN (Deep Convolutional GAN)\n",
        "- Uses convolutional layers throughout\n",
        "- Batch normalization and proper activation functions\n",
        "- Architecture guidelines for stable training\n",
        "\n",
        "### WGAN (Wasserstein GAN)\n",
        "- Different loss function based on Earth Mover's distance\n",
        "- More stable training\n",
        "- Meaningful loss curves\n",
        "\n",
        "### StyleGAN\n",
        "- Progressive growing\n",
        "- Style-based generator architecture\n",
        "- High-quality image generation\n",
        "\n",
        "### Conditional GANs\n",
        "- Add label information to both G and D\n",
        "- Control over generated content\n",
        "- Applications in image-to-image translation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Conditional GAN for MNIST\n",
        "class ConditionalGenerator(nn.Module):\n",
        "    def __init__(self, nz=100, num_classes=10, ngf=64, nc=1):\n",
        "        super(ConditionalGenerator, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # Embedding for class labels\n",
        "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "        \n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(nz + num_classes, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, noise, labels):\n",
        "        # Embed labels and concatenate with noise\n",
        "        label_embedding = self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
        "        gen_input = torch.cat((noise, label_embedding), 1)\n",
        "        return self.main(gen_input)\n",
        "\n",
        "# Example usage of conditional generator\n",
        "cond_gen = ConditionalGenerator().to(device)\n",
        "print(\"Conditional Generator created!\")\n",
        "print(f\"Input size: noise({nz}) + labels({10}) = {nz + 10}\")\n",
        "\n",
        "# Generate samples for specific digits\n",
        "with torch.no_grad():\n",
        "    # Create noise\n",
        "    test_noise = torch.randn(10, nz, 1, 1, device=device)\n",
        "    # Create labels (digits 0-9)\n",
        "    test_labels = torch.arange(0, 10, device=device)\n",
        "    \n",
        "    # Note: This would need to be trained first\n",
        "    print(f\"Generated conditional samples for digits: {test_labels.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3.5 Evaluation Metrics for GANs\n",
        "\n",
        "### Inception Score (IS)\n",
        "- Measures quality and diversity of generated images\n",
        "- Uses pre-trained Inception network\n",
        "- Higher scores indicate better quality\n",
        "\n",
        "### FrÃ©chet Inception Distance (FID)\n",
        "- Compares feature distributions of real and generated images\n",
        "- Lower scores indicate better quality\n",
        "- More robust than IS\n",
        "\n",
        "### Precision and Recall\n",
        "- Precision: Quality of generated samples\n",
        "- Recall: Diversity/coverage of the data distribution\n",
        "- Helps understand mode collapse\n",
        "\n",
        "### Human Evaluation\n",
        "- Still the gold standard for many applications\n",
        "- Subjective but reliable\n",
        "- Important for applications like art generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this chapter, we covered:\n",
        "\n",
        "1. **GAN Theory**: The adversarial training framework and game theory\n",
        "2. **Implementation**: Complete PyTorch implementation of a DCGAN\n",
        "3. **Training**: Practical considerations and common challenges\n",
        "4. **Variants**: Overview of important GAN improvements\n",
        "5. **Evaluation**: Methods to assess GAN performance\n",
        "\n",
        "### Key Takeaways:\n",
        "- GANs use adversarial training between generator and discriminator\n",
        "- Proper architecture and training techniques are crucial for stability\n",
        "- Many variants exist to address specific problems and applications\n",
        "- Evaluation remains challenging but multiple metrics are available\n",
        "\n",
        "### Next Steps:\n",
        "- Experiment with different architectures\n",
        "- Try conditional GANs for controlled generation\n",
        "- Explore advanced variants like StyleGAN or BigGAN\n",
        "- Apply GANs to your specific domain/dataset\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
