{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Transformer Architecture: From Theory to Implementation\n",
        "## A Complete Course in Understanding and Building Transformers from Scratch\n",
        "\n",
        "**Course Overview:**\n",
        "- Week 1 (May 24-25): The Transformer Architecture\n",
        "- Progressive learning with theory, implementation, and exercises\n",
        "- Built with PyTorch and practical examples\n",
        "\n",
        "**Learning Objectives:**\n",
        "By the end of this course, you will:\n",
        "1. Understand the core concepts behind the Transformer architecture\n",
        "2. Implement each component from scratch using PyTorch\n",
        "3. Build a complete working Transformer model\n",
        "4. Understand how attention mechanisms revolutionized NLP\n",
        "\n",
        "## üìö Day 1: May 24 - Architecture Basics\n",
        "### Section 1: Introduction to Transformers\n"
      ],
      "id": "932b513f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import seaborn as sns\n",
        "from typing import Optional, Tuple\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device detection with MPS support for Mac\n",
        "def get_device():\n",
        "    \"\"\"Get the best available device (CUDA > MPS > CPU)\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    else:\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "device = get_device()\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(f\"Device available: {device}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Device performance monitoring\n",
        "def monitor_device_performance():\n",
        "    \"\"\"Monitor device performance and memory usage\"\"\"\n",
        "    print(\"\\nüñ•Ô∏è Device Performance Monitoring\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    if device.type == \"mps\":\n",
        "        print(\"‚úÖ Using Apple Metal Performance Shaders (MPS)\")\n",
        "        print(\"- Optimized for Apple Silicon and discrete GPUs on Mac\")\n",
        "        print(\"- Significantly faster than CPU for large models\")\n",
        "        \n",
        "        # Create a test tensor to check memory allocation\n",
        "        test_tensor = torch.randn(1000, 1000, device=device)\n",
        "        print(f\"- Successfully allocated tensor on MPS: {test_tensor.shape}\")\n",
        "        del test_tensor\n",
        "        \n",
        "    elif device.type == \"cuda\":\n",
        "        print(\"‚úÖ Using NVIDIA CUDA\")\n",
        "        print(f\"- GPU: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"- Memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "        print(f\"- Memory cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Using CPU\")\n",
        "        print(\"- Consider using a GPU for better performance with larger models\")\n",
        "    \n",
        "    return device\n",
        "\n",
        "# Monitor performance\n",
        "current_device = monitor_device_performance()"
      ],
      "id": "72d37e44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Exercise 1.1: Understanding the Problem\n",
        "**Before Transformers, NLP models had limitations:**\n",
        "- RNNs processed sequences sequentially (slow, hard to parallelize)\n",
        "- LSTMs helped with long sequences but still sequential\n",
        "- CNNs could parallelize but struggled with long-range dependencies\n",
        "\n",
        "**The Transformer solution:**\n",
        "- Process entire sequences in parallel\n",
        "- Use attention to capture long-range dependencies\n",
        "- \"Attention is all you need\" - no recurrence or convolution needed\n"
      ],
      "id": "a6656858"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 1.1: Let's see why sequential processing is slow\n",
        "def simulate_rnn_processing():\n",
        "    \"\"\"Simulate how RNNs process sequences sequentially\"\"\"\n",
        "    sequence_length = 10\n",
        "    hidden_size = 4\n",
        "    \n",
        "    print(\"RNN Sequential Processing:\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Simulate processing each token one by one\n",
        "    hidden_state = torch.zeros(hidden_size)\n",
        "    for i in range(sequence_length):\n",
        "        # Each step depends on the previous hidden state\n",
        "        print(f\"Step {i+1}: Processing token {i+1}, depends on step {i}\")\n",
        "        # Simulated computation\n",
        "        hidden_state = torch.tanh(hidden_state + torch.randn(hidden_size))\n",
        "    \n",
        "    print(\"\\nTransformer Parallel Processing:\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"All tokens processed simultaneously using attention!\")\n",
        "    \n",
        "    return hidden_state\n",
        "\n",
        "result = simulate_rnn_processing()"
      ],
      "id": "01da7536",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 2: The Overall Architecture\n",
        "\n",
        "The Transformer consists of:\n",
        "1. **Input Embeddings** + **Positional Encoding**\n",
        "2. **Encoder Stack** (6 layers)\n",
        "   - Multi-Head Self-Attention\n",
        "   - Feed-Forward Network\n",
        "   - Residual connections + Layer Normalization\n",
        "3. **Decoder Stack** (6 layers)\n",
        "   - Masked Multi-Head Self-Attention\n",
        "   - Encoder-Decoder Attention\n",
        "   - Feed-Forward Network\n",
        "   - Residual connections + Layer Normalization\n",
        "4. **Output Layer**\n"
      ],
      "id": "9fab20c6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Let's use the Excalidraw visualization instead of creating our own\n",
        "def show_transformer_architecture():\n",
        "    \"\"\"Display the transformer architecture using the Excalidraw diagram\"\"\"\n",
        "    target_file = \"transformer_architecture_excalidraw.excalidraw.png\"\n",
        "    \n",
        "    try:\n",
        "        from IPython.display import Image, display\n",
        "        print(\"üé® Transformer Architecture Visualization\")\n",
        "        print(\"=\" * 50)\n",
        "        display(Image(target_file))\n",
        "    except ImportError:\n",
        "        # Fallback for non-Jupyter environments\n",
        "        import matplotlib.pyplot as plt\n",
        "        import matplotlib.image as mpimg\n",
        "        \n",
        "        try:\n",
        "            img = mpimg.imread(target_file)\n",
        "            plt.figure(figsize=(16, 12))\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')\n",
        "            plt.title('Transformer Architecture: \"Attention Is All You Need\"', \n",
        "                     fontsize=16, weight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"‚ö†Ô∏è Could not find {target_file}\")\n",
        "            print(\"Please ensure the image file is in the same directory as this script.\")\n",
        "            print(\"\\nThe Transformer consists of:\")\n",
        "            print(\"1. Input Embeddings + Positional Encoding\")\n",
        "            print(\"2. Encoder Stack (6 layers)\")\n",
        "            print(\"   - Multi-Head Self-Attention\")\n",
        "            print(\"   - Feed-Forward Network\")\n",
        "            print(\"   - Residual connections + Layer Normalization\")\n",
        "            print(\"3. Decoder Stack (6 layers)\")\n",
        "            print(\"   - Masked Multi-Head Self-Attention\")\n",
        "            print(\"   - Encoder-Decoder Attention\")\n",
        "            print(\"   - Feed-Forward Network\")\n",
        "            print(\"   - Residual connections + Layer Normalization\")\n",
        "            print(\"4. Output Layer (Linear + Softmax)\")\n",
        "\n",
        "# Also create a detailed attention mechanism visualization\n",
        "def plot_attention_mechanism():\n",
        "    \"\"\"Visualize the attention mechanism in detail\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # 1. Query, Key, Value concept\n",
        "    ax1 = axes[0, 0]\n",
        "    \n",
        "    # Create sample matrices\n",
        "    seq_len = 4\n",
        "    d_model = 6\n",
        "    \n",
        "    # Input matrix\n",
        "    input_matrix = np.random.randn(seq_len, d_model)\n",
        "    \n",
        "    # Q, K, V matrices (simplified)\n",
        "    Q = input_matrix @ np.random.randn(d_model, d_model)\n",
        "    K = input_matrix @ np.random.randn(d_model, d_model) \n",
        "    V = input_matrix @ np.random.randn(d_model, d_model)\n",
        "    \n",
        "    # Show the transformation\n",
        "    ax1.text(0.5, 0.9, 'Input ‚Üí Query, Key, Value', ha='center', transform=ax1.transAxes,\n",
        "            fontsize=14, weight='bold')\n",
        "    \n",
        "    positions = [0.1, 0.35, 0.6, 0.85]\n",
        "    labels = ['Input\\nX', 'Query\\nQ=XWq', 'Key\\nK=XWk', 'Value\\nV=XWv']\n",
        "    matrices = [input_matrix, Q, K, V]\n",
        "    colors = ['lightblue', 'lightcoral', 'lightgreen', 'lightyellow']\n",
        "    \n",
        "    for i, (pos, label, matrix, color) in enumerate(zip(positions, labels, matrices, colors)):\n",
        "        im = ax1.imshow(matrix, cmap='RdBu_r', aspect='auto', \n",
        "                       extent=[pos, pos+0.15, 0.1, 0.7])\n",
        "        ax1.text(pos+0.075, 0.05, label, ha='center', fontsize=10, weight='bold')\n",
        "        \n",
        "        if i < 3:  # Draw arrows\n",
        "            ax1.annotate('', xy=(positions[i+1]-0.02, 0.4), xytext=(pos+0.17, 0.4),\n",
        "                        arrowprops=dict(arrowstyle='->', lw=2))\n",
        "    \n",
        "    ax1.set_xlim(0, 1)\n",
        "    ax1.set_ylim(0, 1)\n",
        "    ax1.axis('off')\n",
        "    \n",
        "    # 2. Attention score calculation\n",
        "    ax2 = axes[0, 1]\n",
        "    \n",
        "    # Simulate attention scores\n",
        "    scores = np.random.rand(seq_len, seq_len)\n",
        "    scores = scores / scores.sum(axis=1, keepdims=True)  # Normalize\n",
        "    \n",
        "    im2 = ax2.imshow(scores, cmap='Blues', aspect='equal')\n",
        "    ax2.set_title('Attention Weights\\nAttention(Q,K,V) = softmax(QK^T/‚àöd_k)V', \n",
        "                 fontsize=12, weight='bold')\n",
        "    \n",
        "    # Add labels\n",
        "    ax2.set_xticks(range(seq_len))\n",
        "    ax2.set_yticks(range(seq_len))\n",
        "    ax2.set_xticklabels([f'K{i}' for i in range(seq_len)])\n",
        "    ax2.set_yticklabels([f'Q{i}' for i in range(seq_len)])\n",
        "    \n",
        "    # Add values as text\n",
        "    for i in range(seq_len):\n",
        "        for j in range(seq_len):\n",
        "            ax2.text(j, i, f'{scores[i,j]:.2f}', ha='center', va='center',\n",
        "                    color='white' if scores[i,j] > 0.5 else 'black')\n",
        "    \n",
        "    plt.colorbar(im2, ax=ax2, shrink=0.8)\n",
        "    \n",
        "    # 3. Multi-head attention\n",
        "    ax3 = axes[1, 0]\n",
        "    \n",
        "    num_heads = 4\n",
        "    head_colors = ['Reds', 'Blues', 'Greens', 'Purples']\n",
        "    \n",
        "    for head in range(num_heads):\n",
        "        # Generate different attention patterns for each head\n",
        "        if head == 0:  # Diagonal pattern\n",
        "            pattern = np.eye(seq_len) + 0.1 * np.random.rand(seq_len, seq_len)\n",
        "        elif head == 1:  # Previous token pattern\n",
        "            pattern = np.tril(np.ones((seq_len, seq_len))) + 0.1 * np.random.rand(seq_len, seq_len)\n",
        "        elif head == 2:  # Uniform pattern\n",
        "            pattern = np.ones((seq_len, seq_len)) + 0.2 * np.random.rand(seq_len, seq_len)\n",
        "        else:  # Local pattern\n",
        "            pattern = np.zeros((seq_len, seq_len))\n",
        "            for i in range(seq_len):\n",
        "                for j in range(max(0, i-1), min(seq_len, i+2)):\n",
        "                    pattern[i, j] = 1\n",
        "            pattern += 0.1 * np.random.rand(seq_len, seq_len)\n",
        "        \n",
        "        pattern = pattern / pattern.sum(axis=1, keepdims=True)\n",
        "        \n",
        "        # Create subplot\n",
        "        start_x = 0.02 + head * 0.24\n",
        "        start_y = 0.1\n",
        "        width = height = 0.2\n",
        "        \n",
        "        # Create inset axes\n",
        "        inset = ax3.inset_axes([start_x, start_y, width, height])\n",
        "        im = inset.imshow(pattern, cmap=head_colors[head], aspect='equal')\n",
        "        inset.set_title(f'Head {head+1}', fontsize=10, weight='bold')\n",
        "        inset.set_xticks([])\n",
        "        inset.set_yticks([])\n",
        "    \n",
        "    ax3.text(0.5, 0.9, 'Multi-Head Attention: Different Heads Learn Different Patterns', \n",
        "            ha='center', transform=ax3.transAxes, fontsize=14, weight='bold')\n",
        "    ax3.text(0.5, 0.05, 'Each head captures different types of relationships', \n",
        "            ha='center', transform=ax3.transAxes, fontsize=12, style='italic')\n",
        "    ax3.axis('off')\n",
        "    \n",
        "    # 4. Self-attention vs Cross-attention\n",
        "    ax4 = axes[1, 1]\n",
        "    \n",
        "    # Self-attention\n",
        "    self_att = np.random.rand(seq_len, seq_len)\n",
        "    self_att = self_att / self_att.sum(axis=1, keepdims=True)\n",
        "    \n",
        "    # Cross-attention (different dimensions)\n",
        "    cross_att = np.random.rand(seq_len, seq_len + 2)  # Different sequence lengths\n",
        "    cross_att = cross_att / cross_att.sum(axis=1, keepdims=True)\n",
        "    \n",
        "    # Plot both\n",
        "    im_self = ax4.imshow(self_att, cmap='Blues', aspect='equal', \n",
        "                        extent=[0, seq_len, seq_len, 0])\n",
        "    im_cross = ax4.imshow(cross_att, cmap='Reds', aspect='equal', alpha=0.7,\n",
        "                         extent=[seq_len+1, seq_len*2+3, seq_len, 0])\n",
        "    \n",
        "    ax4.set_title('Self-Attention vs Cross-Attention', fontsize=12, weight='bold')\n",
        "    ax4.text(seq_len/2, -0.5, 'Self-Attention\\n(Decoder‚ÜíDecoder)', \n",
        "            ha='center', fontsize=10, weight='bold', color='blue')\n",
        "    ax4.text(seq_len*1.5+2, -0.5, 'Cross-Attention\\n(Decoder‚ÜíEncoder)', \n",
        "            ha='center', fontsize=10, weight='bold', color='red')\n",
        "    \n",
        "    ax4.set_xlim(-0.5, seq_len*2+3.5)\n",
        "    ax4.set_ylim(-1, seq_len+0.5)\n",
        "    ax4.set_xticks([])\n",
        "    ax4.set_yticks([])\n",
        "    \n",
        "    plt.suptitle('Understanding Attention Mechanisms in Transformers', \n",
        "                fontsize=16, weight='bold', y=0.95)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create the new visualizations\n",
        "print(\"üé® Creating Enhanced Transformer Architecture Visualization...\")\n",
        "show_transformer_architecture()\n",
        "\n",
        "print(\"\\nüîç Creating Detailed Attention Mechanism Visualization...\")\n",
        "plot_attention_mechanism()"
      ],
      "id": "b7cb41f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Exercise 2.1: Understanding Key Concepts\n",
        "**Fill in the blanks and run the code to test your understanding:**\n"
      ],
      "id": "bae93079"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 2.1: Key Transformer Concepts\n",
        "def transformer_quiz():\n",
        "    \"\"\"Interactive quiz about Transformer concepts\"\"\"\n",
        "    \n",
        "    print(\"üß† Transformer Architecture Quiz\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Question 1\n",
        "    print(\"1. The Transformer uses _______ to capture relationships between words\")\n",
        "    answer1 = \"attention\"  # Fill this in\n",
        "    print(f\"Your answer: {answer1}\")\n",
        "    print(\"‚úì Correct! Attention mechanisms allow the model to focus on relevant parts of the input.\\n\")\n",
        "    \n",
        "    # Question 2  \n",
        "    print(\"2. Unlike RNNs, Transformers can process sequences in _______ \")\n",
        "    answer2 = \"parallel\"  # Fill this in\n",
        "    print(f\"Your answer: {answer2}\")\n",
        "    print(\"‚úì Correct! This makes training much faster.\\n\")\n",
        "    \n",
        "    # Question 3\n",
        "    print(\"3. The three key matrices in attention are Query, Key, and _______\")\n",
        "    answer3 = \"Value\"  # Fill this in\n",
        "    print(f\"Your answer: {answer3}\")\n",
        "    print(\"‚úì Correct! Q, K, V are the foundation of attention.\\n\")\n",
        "    \n",
        "    return True\n",
        "\n",
        "quiz_result = transformer_quiz()"
      ],
      "id": "70d64815",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 3: The Self-Attention Layer\n",
        "\n",
        "**Self-attention allows each word to attend to all other words in the sequence.**\n",
        "\n",
        "**The process:**\n",
        "1. Transform input into Query (Q), Key (K), Value (V) matrices\n",
        "2. Calculate attention weights: `Attention(Q,K,V) = softmax(QK^T/‚àöd_k)V`\n",
        "3. Each position can attend to all positions in the input\n"
      ],
      "id": "a79c50e4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class SingleHeadAttention(nn.Module):\n",
        "    \"\"\"Implementation of single-head self-attention\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, d_k: int):\n",
        "        super().__init__()\n",
        "        self.d_k = d_k\n",
        "        \n",
        "        # Linear transformations for Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, d_k, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_k, bias=False) \n",
        "        self.W_v = nn.Linear(d_model, d_k, bias=False)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "            mask: Optional mask tensor\n",
        "        Returns:\n",
        "            Attention output of shape (batch_size, seq_len, d_k)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "        \n",
        "        # Step 1: Create Q, K, V matrices\n",
        "        Q = self.W_q(x)  # (batch_size, seq_len, d_k)\n",
        "        K = self.W_k(x)  # (batch_size, seq_len, d_k)\n",
        "        V = self.W_v(x)  # (batch_size, seq_len, d_k)\n",
        "        \n",
        "        # Step 2: Calculate attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        # scores shape: (batch_size, seq_len, seq_len)\n",
        "        \n",
        "        # Step 3: Apply mask if provided (for decoder)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        \n",
        "        # Step 4: Apply softmax to get attention weights\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        # Step 5: Apply attention weights to values\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        return output, attention_weights"
      ],
      "id": "bbac255c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Exercise 3.1: Understanding Self-Attention\n"
      ],
      "id": "65e29427"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 3.1: Let's see self-attention in action!\n",
        "def demonstrate_self_attention():\n",
        "    \"\"\"Demonstrate how self-attention works with a simple example\"\"\"\n",
        "    \n",
        "    # Create a simple example\n",
        "    vocab_size = 10\n",
        "    seq_len = 4\n",
        "    d_model = 8\n",
        "    d_k = 6\n",
        "    \n",
        "    # Create sample input (let's say it represents \"The cat sat\")\n",
        "    torch.manual_seed(42)\n",
        "    x = torch.randn(1, seq_len, d_model).to(device)  # batch_size=1\n",
        "    \n",
        "    # Initialize attention layer and move to device\n",
        "    attention = SingleHeadAttention(d_model, d_k).to(device)\n",
        "    \n",
        "    # Forward pass\n",
        "    output, weights = attention(x)\n",
        "    \n",
        "    print(\"üîç Self-Attention Demonstration\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Attention weights shape: {weights.shape}\")\n",
        "    print(f\"Device: {x.device}\")\n",
        "    \n",
        "    # Visualize attention weights (move to CPU for visualization)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(weights[0].detach().cpu().numpy(), \n",
        "                annot=True, \n",
        "                fmt='.3f',\n",
        "                xticklabels=[f'Pos {i}' for i in range(seq_len)],\n",
        "                yticklabels=[f'Pos {i}' for i in range(seq_len)],\n",
        "                cmap='Blues')\n",
        "    plt.title('Self-Attention Weights\\n(How much each position attends to every position)')\n",
        "    plt.xlabel('Key Positions')\n",
        "    plt.ylabel('Query Positions') \n",
        "    plt.show()\n",
        "    \n",
        "    # Verify attention weights sum to 1\n",
        "    print(f\"\\nAttention weights sum (should be ~1.0): {weights.sum(dim=-1)}\")\n",
        "    \n",
        "    return output, weights\n",
        "\n",
        "attention_output, attention_weights = demonstrate_self_attention()"
      ],
      "id": "bade846d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Exercise 3.2: Build Your Own Attention\n"
      ],
      "id": "3a06ccd1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 3.2: Complete the missing parts of this attention function\n",
        "def manual_attention_calculation(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Calculate attention manually to understand the process\n",
        "    Complete the missing parts marked with # TODO\n",
        "    \"\"\"\n",
        "    print(\"üõ†Ô∏è Manual Attention Calculation\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # TODO: Calculate the scaling factor\n",
        "    d_k = Q.shape[-1]\n",
        "    scale = 1.0 / math.sqrt(d_k)  # Fill this in\n",
        "    \n",
        "    # TODO: Calculate attention scores (Q @ K^T)\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1))  # Fill this in\n",
        "    \n",
        "    # TODO: Apply scaling\n",
        "    scores = scores * scale  # Fill this in\n",
        "    \n",
        "    # Apply mask if provided\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    \n",
        "    # TODO: Apply softmax\n",
        "    attention_weights = F.softmax(scores, dim=-1)  # Fill this in\n",
        "    \n",
        "    # TODO: Apply weights to values\n",
        "    output = torch.matmul(attention_weights, V)  # Fill this in\n",
        "    \n",
        "    print(f\"‚úì Scaling factor: {scale:.4f}\")\n",
        "    print(f\"‚úì Scores shape: {scores.shape}\")\n",
        "    print(f\"‚úì Attention weights shape: {attention_weights.shape}\")\n",
        "    print(f\"‚úì Output shape: {output.shape}\")\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "# Test your implementation\n",
        "test_Q = torch.randn(1, 4, 6).to(device)\n",
        "test_K = torch.randn(1, 4, 6).to(device)\n",
        "test_V = torch.randn(1, 4, 6).to(device)\n",
        "\n",
        "manual_output, manual_weights = manual_attention_calculation(test_Q, test_K, test_V)"
      ],
      "id": "a18e4087",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 4: The Multi-Head Attention Layer\n",
        "\n",
        "**Multi-head attention allows the model to attend to information from different representation subspaces:**\n",
        "- Split the input into multiple \"heads\"\n",
        "- Each head learns different types of relationships\n",
        "- Concatenate and project the results\n"
      ],
      "id": "f38bd7c9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Attention implementation\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        # Linear layers for Q, K, V for all heads\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        \"\"\"Calculate scaled dot-product attention\"\"\"\n",
        "        d_k = Q.shape[-1]\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        \n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "            \n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        return output, attention_weights\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size, query_seq_len, d_model = query.shape\n",
        "        _, key_seq_len, _ = key.shape\n",
        "        _, value_seq_len, _ = value.shape\n",
        "        \n",
        "        # Step 1: Linear transformations and split into heads\n",
        "        Q = self.W_q(query).view(batch_size, query_seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(key).view(batch_size, key_seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(value).view(batch_size, value_seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        # Step 2: Apply attention to each head\n",
        "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        \n",
        "        # Step 3: Concatenate heads\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, query_seq_len, d_model)\n",
        "        \n",
        "        # Step 4: Final linear projection\n",
        "        output = self.W_o(attention_output)\n",
        "        \n",
        "        return output, attention_weights"
      ],
      "id": "b76383ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Exercise 4.1: Multi-Head Attention in Action\n"
      ],
      "id": "4f28708f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 4.1: Compare single-head vs multi-head attention\n",
        "def compare_attention_heads():\n",
        "    \"\"\"Compare single-head and multi-head attention\"\"\"\n",
        "    \n",
        "    # Setup\n",
        "    batch_size, seq_len, d_model = 1, 6, 12\n",
        "    num_heads = 3\n",
        "    \n",
        "    # Create input and move to device\n",
        "    x = torch.randn(batch_size, seq_len, d_model).to(device)\n",
        "    \n",
        "    # Single-head attention\n",
        "    single_head = SingleHeadAttention(d_model, d_model).to(device)\n",
        "    single_output, single_weights = single_head(x)\n",
        "    \n",
        "    # Multi-head attention  \n",
        "    multi_head = MultiHeadAttention(d_model, num_heads).to(device)\n",
        "    multi_output, multi_weights = multi_head(x, x, x)\n",
        "    \n",
        "    print(\"üîÑ Single-Head vs Multi-Head Attention\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Single-head output: {single_output.shape}\")\n",
        "    print(f\"Multi-head output: {multi_output.shape}\")\n",
        "    print(f\"Single-head weights: {single_weights.shape}\")\n",
        "    print(f\"Multi-head weights: {multi_weights.shape}\")\n",
        "    print(f\"Device: {x.device}\")\n",
        "    \n",
        "    # Visualize multiple heads (move to CPU for visualization)\n",
        "    fig, axes = plt.subplots(1, num_heads, figsize=(15, 4))\n",
        "    for head in range(num_heads):\n",
        "        sns.heatmap(multi_weights[0, head].detach().cpu().numpy(),\n",
        "                   annot=True, fmt='.2f', ax=axes[head],\n",
        "                   xticklabels=[f'K{i}' for i in range(seq_len)],\n",
        "                   yticklabels=[f'Q{i}' for i in range(seq_len)],\n",
        "                   cmap='viridis')\n",
        "        axes[head].set_title(f'Head {head + 1}')\n",
        "    \n",
        "    plt.suptitle('Multi-Head Attention: Each Head Learns Different Patterns')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return single_output, multi_output\n",
        "\n",
        "single_out, multi_out = compare_attention_heads()"
      ],
      "id": "cebc36ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Exercise 4.2: Understanding Different Attention Patterns\n"
      ],
      "id": "01fa78e9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 4.2: Create different types of attention patterns\n",
        "def create_attention_patterns():\n",
        "    \"\"\"Create and visualize different attention patterns\"\"\"\n",
        "    \n",
        "    seq_len = 8\n",
        "    patterns = {}\n",
        "    \n",
        "    # 1. Identity pattern (attend to self)\n",
        "    identity = torch.eye(seq_len)\n",
        "    patterns['Identity (Self-Attention)'] = identity\n",
        "    \n",
        "    # 2. Uniform pattern (attend to all equally)\n",
        "    uniform = torch.ones(seq_len, seq_len) / seq_len\n",
        "    patterns['Uniform (Global Attention)'] = uniform\n",
        "    \n",
        "    # 3. Causal pattern (only attend to previous tokens)\n",
        "    causal = torch.tril(torch.ones(seq_len, seq_len))\n",
        "    causal = causal / causal.sum(dim=-1, keepdim=True)\n",
        "    patterns['Causal (Decoder Attention)'] = causal\n",
        "    \n",
        "    # 4. Local pattern (attend to nearby tokens)\n",
        "    local = torch.zeros(seq_len, seq_len)\n",
        "    for i in range(seq_len):\n",
        "        start = max(0, i-1)\n",
        "        end = min(seq_len, i+2)\n",
        "        local[i, start:end] = 1\n",
        "    local = local / local.sum(dim=-1, keepdim=True)\n",
        "    patterns['Local (Window Attention)'] = local\n",
        "    \n",
        "    # Visualize all patterns\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, (name, pattern) in enumerate(patterns.items()):\n",
        "        sns.heatmap(pattern.numpy(), annot=True, fmt='.2f', \n",
        "                   ax=axes[idx], cmap='Blues',\n",
        "                   xticklabels=[f'K{i}' for i in range(seq_len)],\n",
        "                   yticklabels=[f'Q{i}' for i in range(seq_len)])\n",
        "        axes[idx].set_title(name)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return patterns\n",
        "\n",
        "attention_patterns = create_attention_patterns()"
      ],
      "id": "6455317b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 5: Position Embedding\n",
        "\n",
        "**Since Transformers have no inherent notion of sequence order, we need to inject positional information:**\n",
        "- Absolute positional encoding (original Transformer)\n",
        "- Relative positional encoding (more recent variants)\n",
        "- Learned vs Fixed positional encodings\n"
      ],
      "id": "d491fe77"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Sinusoidal positional encoding from 'Attention is All You Need'\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, max_seq_len: int = 5000):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
        "        \n",
        "        # Calculate div_term for sinusoidal pattern\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
        "                           -(math.log(10000.0) / d_model))\n",
        "        \n",
        "        # Apply sine to even indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Apply cosine to odd indices  \n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        # Add batch dimension and register as buffer\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Add positional encoding to input embeddings\"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "        return x + self.pe[:, :seq_len]"
      ],
      "id": "4ed9e1e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Exercise 5.1: Visualizing Positional Encodings\n"
      ],
      "id": "ba0d221e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 5.1: Understand how positional encodings work\n",
        "def visualize_positional_encoding():\n",
        "    \"\"\"Visualize positional encoding patterns\"\"\"\n",
        "    \n",
        "    d_model = 16\n",
        "    max_seq_len = 50\n",
        "    \n",
        "    # Create positional encoding\n",
        "    pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "    \n",
        "    # Get the encoding matrix\n",
        "    pe_matrix = pos_encoding.pe[0, :max_seq_len, :].numpy()\n",
        "    \n",
        "    # Plot the positional encoding\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    \n",
        "    # Heatmap of all dimensions\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.heatmap(pe_matrix.T, cmap='RdBu_r', center=0, \n",
        "                xticklabels=range(0, max_seq_len, 5),\n",
        "                yticklabels=range(0, d_model, 2))\n",
        "    plt.title('Positional Encoding Matrix\\n(Rows=Dimensions, Cols=Positions)')\n",
        "    plt.xlabel('Position')\n",
        "    plt.ylabel('Encoding Dimension')\n",
        "    \n",
        "    # Plot specific dimensions over positions\n",
        "    plt.subplot(2, 2, 2)\n",
        "    positions = range(max_seq_len)\n",
        "    for dim in [0, 1, 4, 5]:\n",
        "        plt.plot(positions, pe_matrix[:, dim], label=f'Dim {dim}')\n",
        "    plt.title('Positional Encoding Values')\n",
        "    plt.xlabel('Position')\n",
        "    plt.ylabel('Encoding Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Show sine/cosine patterns\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(positions[:20], pe_matrix[:20, 0], 'o-', label='Dim 0 (sin)')\n",
        "    plt.plot(positions[:20], pe_matrix[:20, 1], 's-', label='Dim 1 (cos)')\n",
        "    plt.title('Sine/Cosine Pattern (First 20 positions)')\n",
        "    plt.xlabel('Position')\n",
        "    plt.ylabel('Encoding Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Compare learned vs fixed\n",
        "    plt.subplot(2, 2, 4)\n",
        "    # Simulate learned positional embedding (random for comparison)\n",
        "    learned_pe = torch.randn(max_seq_len, d_model).numpy()\n",
        "    plt.plot(positions[:20], pe_matrix[:20, 0], label='Fixed (Sinusoidal)')\n",
        "    plt.plot(positions[:20], learned_pe[:20, 0], label='Learned (Random)')\n",
        "    plt.title('Fixed vs Learned Positional Encoding')\n",
        "    plt.xlabel('Position')\n",
        "    plt.ylabel('Encoding Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return pe_matrix\n",
        "\n",
        "pe_matrix = visualize_positional_encoding()"
      ],
      "id": "67cfa3d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Exercise 5.2: Position Encoding Effects\n"
      ],
      "id": "b5ff3b63"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 5.2: See how positional encoding affects word embeddings\n",
        "def demonstrate_position_effects():\n",
        "    \"\"\"Show how positional encoding affects the same word in different positions\"\"\"\n",
        "    \n",
        "    # Simulate word embeddings\n",
        "    vocab_size = 1000\n",
        "    d_model = 8\n",
        "    seq_len = 10\n",
        "    \n",
        "    # Create embedding layer and move to device\n",
        "    embedding = nn.Embedding(vocab_size, d_model).to(device)\n",
        "    pos_encoding = PositionalEncoding(d_model).to(device)\n",
        "    \n",
        "    # Same word \"cat\" (token id = 42) in different positions\n",
        "    word_id = 42\n",
        "    positions = [0, 2, 5, 8]\n",
        "    \n",
        "    print(\"üê± How Position Affects Word Meaning\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    results = {}\n",
        "    for pos in positions:\n",
        "        # Create sequence with \"cat\" at position pos\n",
        "        input_ids = torch.zeros(1, seq_len, dtype=torch.long).to(device)\n",
        "        input_ids[0, pos] = word_id\n",
        "        \n",
        "        # Get word embedding\n",
        "        word_emb = embedding(input_ids)\n",
        "        \n",
        "        # Add positional encoding\n",
        "        word_with_pos = pos_encoding(word_emb)\n",
        "        \n",
        "        # Extract the embedding for our word\n",
        "        cat_embedding = word_emb[0, pos]\n",
        "        cat_with_position = word_with_pos[0, pos]\n",
        "        \n",
        "        results[pos] = {\n",
        "            'original': cat_embedding,\n",
        "            'with_position': cat_with_position\n",
        "        }\n",
        "        \n",
        "        print(f\"Position {pos}:\")\n",
        "        print(f\"  Original embedding: {cat_embedding[:4].detach().cpu().numpy()}\")\n",
        "        print(f\"  With position:      {cat_with_position[:4].detach().cpu().numpy()}\")\n",
        "        print()\n",
        "    \n",
        "    # Calculate similarities between the same word at different positions\n",
        "    print(\"Similarity between 'cat' at different positions:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for i, pos1 in enumerate(positions):\n",
        "        for pos2 in positions[i+1:]:\n",
        "            emb1 = results[pos1]['with_position']\n",
        "            emb2 = results[pos2]['with_position']\n",
        "            \n",
        "            # Cosine similarity\n",
        "            similarity = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0))\n",
        "            print(f\"Position {pos1} vs Position {pos2}: {similarity.item():.3f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "position_results = demonstrate_position_effects()"
      ],
      "id": "07dfbcfc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Day 2: May 25 - Implementation Day\n",
        "### Section 6: The Encoder\n"
      ],
      "id": "476563c8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Position-wise Feed-Forward Network\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # FFN(x) = max(0, xW1 + b1)W2 + b2\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"Single Transformer Encoder Layer\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        \n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model) \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        # Multi-head attention with residual connection\n",
        "        attn_output, _ = self.self_attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        \n",
        "        # Feed-forward with residual connection  \n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        \n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"Stack of Transformer Encoder Layers\"\"\"\n",
        "    \n",
        "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout) \n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x"
      ],
      "id": "6e3d4bca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Exercise 6.1: Build and Test the Encoder\n"
      ],
      "id": "d208456e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 6.1: Let's build a complete encoder and see how it transforms inputs\n",
        "def test_encoder():\n",
        "    \"\"\"Test the Transformer encoder with sample data\"\"\"\n",
        "    \n",
        "    # Hyperparameters\n",
        "    batch_size = 2\n",
        "    seq_len = 10\n",
        "    d_model = 16\n",
        "    num_heads = 4\n",
        "    num_layers = 3\n",
        "    d_ff = 64\n",
        "    vocab_size = 100\n",
        "    \n",
        "    # Create sample input\n",
        "    torch.manual_seed(42)\n",
        "    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
        "    \n",
        "    # Create embeddings and move to device\n",
        "    embedding = nn.Embedding(vocab_size, d_model).to(device)\n",
        "    pos_encoding = PositionalEncoding(d_model).to(device)\n",
        "    \n",
        "    # Get embeddings with position\n",
        "    x = embedding(input_ids)\n",
        "    x = pos_encoding(x)\n",
        "    \n",
        "    print(\"üèóÔ∏è Transformer Encoder Test\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Device: {x.device}\")\n",
        "    \n",
        "    # Create encoder and move to device\n",
        "    encoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff).to(device)\n",
        "    \n",
        "    # Forward pass\n",
        "    output = encoder(x)\n",
        "    \n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Number of parameters: {sum(p.numel() for p in encoder.parameters()):,}\")\n",
        "    \n",
        "    # Analyze how representations change through layers\n",
        "    layer_outputs = []\n",
        "    temp_x = x\n",
        "    \n",
        "    for i, layer in enumerate(encoder.layers):\n",
        "        temp_x = layer(temp_x)\n",
        "        layer_outputs.append(temp_x.clone())\n",
        "        \n",
        "        # Calculate average attention pattern (simplified analysis)\n",
        "        mean_values = temp_x.mean(dim=[0, 1])\n",
        "        print(f\"Layer {i+1} - Mean activation: {mean_values.mean().item():.4f}, \"\n",
        "              f\"Std: {mean_values.std().item():.4f}\")\n",
        "    \n",
        "    return output, layer_outputs\n",
        "\n",
        "encoder_output, layer_outputs = test_encoder()"
      ],
      "id": "f8e17fc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Exercise 6.2: Analyze Information Flow\n"
      ],
      "id": "0b09527b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 6.2: Visualize how information flows through encoder layers\n",
        "def analyze_encoder_information_flow():\n",
        "    \"\"\"Analyze how information changes through encoder layers\"\"\"\n",
        "    \n",
        "    # Use outputs from previous exercise\n",
        "    if 'layer_outputs' not in locals():\n",
        "        _, layer_outputs = test_encoder()\n",
        "    \n",
        "    num_layers = len(layer_outputs)\n",
        "    \n",
        "    # Calculate layer-wise statistics\n",
        "    stats = {\n",
        "        'mean_activation': [],\n",
        "        'std_activation': [],\n",
        "        'mean_magnitude': []\n",
        "    }\n",
        "    \n",
        "    for layer_output in layer_outputs:\n",
        "        stats['mean_activation'].append(layer_output.mean().item())\n",
        "        stats['std_activation'].append(layer_output.std().item())\n",
        "        stats['mean_magnitude'].append(layer_output.abs().mean().item())\n",
        "    \n",
        "    # Plot the statistics\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "    \n",
        "    layers = range(1, num_layers + 1)\n",
        "    \n",
        "    # Mean activation\n",
        "    axes[0].plot(layers, stats['mean_activation'], 'o-')\n",
        "    axes[0].set_title('Mean Activation per Layer')\n",
        "    axes[0].set_xlabel('Layer')\n",
        "    axes[0].set_ylabel('Mean Activation')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Standard deviation\n",
        "    axes[1].plot(layers, stats['std_activation'], 's-', color='orange')\n",
        "    axes[1].set_title('Activation Std per Layer')\n",
        "    axes[1].set_xlabel('Layer')\n",
        "    axes[1].set_ylabel('Standard Deviation')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Mean magnitude\n",
        "    axes[2].plot(layers, stats['mean_magnitude'], '^-', color='green')\n",
        "    axes[2].set_title('Mean Magnitude per Layer')\n",
        "    axes[2].set_xlabel('Layer')\n",
        "    axes[2].set_ylabel('Mean Magnitude')\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"üìä Information Flow Analysis\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"As information flows through layers:\")\n",
        "    print(\"- Mean activation shows the average 'signal' level\")\n",
        "    print(\"- Standard deviation shows how varied the representations are\")\n",
        "    print(\"- Mean magnitude shows the overall 'strength' of representations\")\n",
        "    \n",
        "    return stats\n",
        "\n",
        "flow_stats = analyze_encoder_information_flow()"
      ],
      "id": "1dedaa93",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 7: The Decoder\n",
        "\n",
        "**The decoder has additional complexity:**\n",
        "- Masked self-attention (can't see future tokens)\n",
        "- Encoder-decoder attention (attends to encoder outputs)\n",
        "- Autoregressive generation during inference\n"
      ],
      "id": "3b9bed25"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_causal_mask(seq_len: int) -> torch.Tensor:\n",
        "    \"\"\"Create causal mask for decoder self-attention\"\"\"\n",
        "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
        "    return mask.unsqueeze(0).unsqueeze(0)  # Add batch and head dimensions\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"Single Transformer Decoder Layer\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Self-attention (masked)\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        \n",
        "        # Encoder-decoder attention\n",
        "        self.encoder_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        \n",
        "        # Feed-forward\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        \n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, \n",
        "                causal_mask: Optional[torch.Tensor] = None,\n",
        "                padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \n",
        "        # 1. Masked self-attention\n",
        "        attn_output, _ = self.self_attention(x, x, x, causal_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        \n",
        "        # 2. Encoder-decoder attention\n",
        "        attn_output, _ = self.encoder_attention(x, encoder_output, encoder_output, padding_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        \n",
        "        # 3. Feed-forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        \n",
        "        return x\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    \"\"\"Stack of Transformer Decoder Layers\"\"\"\n",
        "    \n",
        "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,\n",
        "                causal_mask: Optional[torch.Tensor] = None,\n",
        "                padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, causal_mask, padding_mask)\n",
        "            \n",
        "        return x"
      ],
      "id": "deb95fe9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Exercise 7.1: Understanding Causal Masking\n"
      ],
      "id": "1ec62c14"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 7.1: Understand why we need causal masking\n",
        "def demonstrate_causal_masking():\n",
        "    \"\"\"Show the difference between masked and unmasked attention in decoder\"\"\"\n",
        "    \n",
        "    seq_len = 6\n",
        "    d_model = 8\n",
        "    \n",
        "    # Create sample decoder input (representing partial translation)\n",
        "    x = torch.randn(1, seq_len, d_model).to(device)\n",
        "    \n",
        "    # Create causal mask and move to device\n",
        "    causal_mask = create_causal_mask(seq_len).to(device)\n",
        "    \n",
        "    # Attention without mask (WRONG for decoder)\n",
        "    attention_no_mask = MultiHeadAttention(d_model, 2).to(device)\n",
        "    output_no_mask, weights_no_mask = attention_no_mask(x, x, x)\n",
        "    \n",
        "    # Attention with causal mask (CORRECT for decoder)  \n",
        "    attention_with_mask = MultiHeadAttention(d_model, 2).to(device)\n",
        "    output_with_mask, weights_with_mask = attention_with_mask(x, x, x, causal_mask)\n",
        "    \n",
        "    # Visualize the difference (move to CPU for visualization)\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "    \n",
        "    # Show the mask\n",
        "    axes[0].imshow(causal_mask[0, 0].cpu().numpy(), cmap='Blues')\n",
        "    axes[0].set_title('Causal Mask\\n(1=allowed, 0=blocked)')\n",
        "    axes[0].set_xlabel('Key Position')\n",
        "    axes[0].set_ylabel('Query Position')\n",
        "    \n",
        "    # Attention without mask (head 0)\n",
        "    sns.heatmap(weights_no_mask[0, 0].detach().cpu().numpy(), \n",
        "                annot=True, fmt='.2f', ax=axes[1], cmap='Reds',\n",
        "                xticklabels=[f'K{i}' for i in range(seq_len)],\n",
        "                yticklabels=[f'Q{i}' for i in range(seq_len)])\n",
        "    axes[1].set_title('Without Causal Mask\\n(Can see future!)')\n",
        "    \n",
        "    # Attention with mask (head 0)\n",
        "    sns.heatmap(weights_with_mask[0, 0].detach().cpu().numpy(),\n",
        "                annot=True, fmt='.2f', ax=axes[2], cmap='Blues',\n",
        "                xticklabels=[f'K{i}' for i in range(seq_len)],\n",
        "                yticklabels=[f'Q{i}' for i in range(seq_len)])\n",
        "    axes[2].set_title('With Causal Mask\\n(Cannot see future)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"üîí Causal Masking Demonstration\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"Without causal mask: Decoder can 'cheat' by looking at future tokens\")\n",
        "    print(\"With causal mask: Decoder can only look at previous tokens (autoregressive)\")\n",
        "    print(\"\\nThis is crucial for:\")\n",
        "    print(\"- Training: Ensures model learns to predict from past context only\")\n",
        "    print(\"- Inference: Matches the autoregressive generation process\")\n",
        "    \n",
        "    return weights_no_mask, weights_with_mask\n",
        "\n",
        "no_mask_weights, masked_weights = demonstrate_causal_masking()"
      ],
      "id": "0c2e0c3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Exercise 7.2: Complete Encoder-Decoder\n"
      ],
      "id": "9c18e6fb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 7.2: Build and test complete encoder-decoder\n",
        "def test_encoder_decoder():\n",
        "    \"\"\"Test complete encoder-decoder transformer\"\"\"\n",
        "    \n",
        "    # Hyperparameters\n",
        "    batch_size = 2\n",
        "    src_seq_len = 8  # Source sequence length\n",
        "    tgt_seq_len = 6  # Target sequence length  \n",
        "    d_model = 16\n",
        "    num_heads = 4\n",
        "    num_layers = 2\n",
        "    d_ff = 64\n",
        "    vocab_size = 100\n",
        "    \n",
        "    # Create sample data and move to device\n",
        "    torch.manual_seed(42)\n",
        "    src_tokens = torch.randint(0, vocab_size, (batch_size, src_seq_len)).to(device)\n",
        "    tgt_tokens = torch.randint(0, vocab_size, (batch_size, tgt_seq_len)).to(device)\n",
        "    \n",
        "    # Embeddings and positional encoding - move to device\n",
        "    embedding = nn.Embedding(vocab_size, d_model).to(device)\n",
        "    pos_encoding = PositionalEncoding(d_model).to(device)\n",
        "    \n",
        "    # Encode source\n",
        "    src_emb = pos_encoding(embedding(src_tokens))\n",
        "    encoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff).to(device)\n",
        "    encoder_output = encoder(src_emb)\n",
        "    \n",
        "    # Decode target\n",
        "    tgt_emb = pos_encoding(embedding(tgt_tokens))\n",
        "    decoder = TransformerDecoder(num_layers, d_model, num_heads, d_ff).to(device)\n",
        "    \n",
        "    # Create causal mask for target and move to device\n",
        "    causal_mask = create_causal_mask(tgt_seq_len).to(device)\n",
        "    \n",
        "    decoder_output = decoder(tgt_emb, encoder_output, causal_mask)\n",
        "    \n",
        "    print(\"üîÑ Complete Encoder-Decoder Test\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Source tokens shape: {src_tokens.shape}\")\n",
        "    print(f\"Target tokens shape: {tgt_tokens.shape}\")\n",
        "    print(f\"Encoder output shape: {encoder_output.shape}\")\n",
        "    print(f\"Decoder output shape: {decoder_output.shape}\")\n",
        "    print(f\"Device: {src_tokens.device}\")\n",
        "    \n",
        "    # Calculate number of parameters\n",
        "    total_params = (sum(p.numel() for p in encoder.parameters()) + \n",
        "                   sum(p.numel() for p in decoder.parameters()) +\n",
        "                   sum(p.numel() for p in embedding.parameters()) +\n",
        "                   sum(p.numel() for p in pos_encoding.parameters()))\n",
        "    \n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    \n",
        "    return encoder_output, decoder_output\n",
        "\n",
        "enc_out, dec_out = test_encoder_decoder()"
      ],
      "id": "fe4ee04a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 8: Complete Transformer Implementation\n"
      ],
      "id": "33e4531b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"Complete Transformer Model\"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int, \n",
        "                 d_model: int = 512,\n",
        "                 num_heads: int = 8,\n",
        "                 num_encoder_layers: int = 6,\n",
        "                 num_decoder_layers: int = 6,\n",
        "                 d_ff: int = 2048,\n",
        "                 max_seq_len: int = 5000,\n",
        "                 dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # Embeddings\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        \n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "        \n",
        "        # Encoder and Decoder\n",
        "        self.encoder = TransformerEncoder(num_encoder_layers, d_model, num_heads, d_ff, dropout)\n",
        "        self.decoder = TransformerDecoder(num_decoder_layers, d_model, num_heads, d_ff, dropout)\n",
        "        \n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n",
        "        \n",
        "        # Initialize parameters\n",
        "        self._init_parameters()\n",
        "        \n",
        "    def _init_parameters(self):\n",
        "        \"\"\"Initialize parameters with Xavier uniform\"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "                \n",
        "    def encode(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"Encode source sequence\"\"\"\n",
        "        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)\n",
        "        src_emb = self.pos_encoding(src_emb)\n",
        "        return self.encoder(src_emb, src_mask)\n",
        "    \n",
        "    def decode(self, tgt: torch.Tensor, encoder_output: torch.Tensor,\n",
        "               tgt_mask: Optional[torch.Tensor] = None,\n",
        "               src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"Decode target sequence\"\"\"\n",
        "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.pos_encoding(tgt_emb)\n",
        "        decoder_output = self.decoder(tgt_emb, encoder_output, tgt_mask, src_mask)\n",
        "        return self.output_projection(decoder_output)\n",
        "    \n",
        "    def forward(self, src: torch.Tensor, tgt: torch.Tensor,\n",
        "                src_mask: Optional[torch.Tensor] = None,\n",
        "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"Forward pass through the complete transformer\"\"\"\n",
        "        encoder_output = self.encode(src, src_mask)\n",
        "        output = self.decode(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return output"
      ],
      "id": "5359bf1f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Exercise 8.1: Build Your Complete Transformer\n"
      ],
      "id": "1dffdd17"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 8.1: Create and test the complete transformer\n",
        "def test_complete_transformer():\n",
        "    \"\"\"Test the complete transformer implementation\"\"\"\n",
        "    \n",
        "    # Model hyperparameters\n",
        "    src_vocab_size = 1000\n",
        "    tgt_vocab_size = 800\n",
        "    d_model = 128\n",
        "    num_heads = 8\n",
        "    num_encoder_layers = 3\n",
        "    num_decoder_layers = 3\n",
        "    d_ff = 512\n",
        "    max_seq_len = 100\n",
        "    \n",
        "    # Create model and move to device\n",
        "    transformer = Transformer(\n",
        "        src_vocab_size=src_vocab_size,\n",
        "        tgt_vocab_size=tgt_vocab_size,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        num_encoder_layers=num_encoder_layers,\n",
        "        num_decoder_layers=num_decoder_layers,\n",
        "        d_ff=d_ff,\n",
        "        max_seq_len=max_seq_len\n",
        "    ).to(device)\n",
        "    \n",
        "    # Sample data\n",
        "    batch_size = 4\n",
        "    src_seq_len = 12\n",
        "    tgt_seq_len = 10\n",
        "    \n",
        "    torch.manual_seed(42)\n",
        "    src = torch.randint(0, src_vocab_size, (batch_size, src_seq_len)).to(device)\n",
        "    tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_seq_len)).to(device)\n",
        "    \n",
        "    # Create masks and move to device\n",
        "    tgt_mask = create_causal_mask(tgt_seq_len).to(device)\n",
        "    \n",
        "    # Forward pass\n",
        "    output = transformer(src, tgt, tgt_mask=tgt_mask)\n",
        "    \n",
        "    print(\"üéâ Complete Transformer Test\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Source shape: {src.shape}\")\n",
        "    print(f\"Target shape: {tgt.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Output vocabulary size: {output.shape[-1]}\")\n",
        "    print(f\"Device: {src.device}\")\n",
        "    \n",
        "    # Model size\n",
        "    total_params = sum(p.numel() for p in transformer.parameters())\n",
        "    trainable_params = sum(p.numel() for p in transformer.parameters() if p.requires_grad)\n",
        "    \n",
        "    print(f\"\\nModel Statistics:\")\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Model size (MB): {total_params * 4 / 1024 / 1024:.2f}\")  # 4 bytes per float32\n",
        "    \n",
        "    # Test gradient flow\n",
        "    loss = F.cross_entropy(output.view(-1, tgt_vocab_size), \n",
        "                          torch.randint(0, tgt_vocab_size, (batch_size * tgt_seq_len,)).to(device))\n",
        "    loss.backward()\n",
        "    \n",
        "    # Check gradient statistics\n",
        "    grad_norms = []\n",
        "    for name, param in transformer.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            grad_norm = param.grad.norm().item()\n",
        "            grad_norms.append(grad_norm)\n",
        "    \n",
        "    print(f\"\\nGradient Statistics:\")\n",
        "    print(f\"Average gradient norm: {np.mean(grad_norms):.6f}\")\n",
        "    print(f\"Max gradient norm: {np.max(grad_norms):.6f}\")\n",
        "    print(f\"Min gradient norm: {np.min(grad_norms):.6f}\")\n",
        "    \n",
        "    return transformer, output\n",
        "\n",
        "transformer_model, transformer_output = test_complete_transformer()"
      ],
      "id": "860f20f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Exercise 8.2: Autoregressive Generation\n"
      ],
      "id": "9fb0ffd8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 8.2: Implement autoregressive text generation\n",
        "def autoregressive_generation(model: Transformer, src: torch.Tensor, \n",
        "                            start_token: int, end_token: int, max_length: int = 20):\n",
        "    \"\"\"Generate text autoregressively using the transformer\"\"\"\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Encode source\n",
        "        encoder_output = model.encode(src)\n",
        "        \n",
        "        # Start with start token on the same device as src\n",
        "        generated = torch.tensor([[start_token]], dtype=torch.long, device=src.device)\n",
        "        \n",
        "        for _ in range(max_length):\n",
        "            # Create causal mask for current sequence\n",
        "            seq_len = generated.shape[1]\n",
        "            tgt_mask = create_causal_mask(seq_len).to(src.device)\n",
        "            \n",
        "            # Decode\n",
        "            output = model.decode(generated, encoder_output, tgt_mask)\n",
        "            \n",
        "            # Get next token probabilities\n",
        "            next_token_logits = output[0, -1, :]  # Last position, batch 0\n",
        "            next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
        "            \n",
        "            # Sample next token (you could also use greedy or beam search)\n",
        "            next_token = torch.multinomial(next_token_probs, 1)\n",
        "            \n",
        "            # Add to sequence\n",
        "            generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)\n",
        "            \n",
        "            # Stop if end token generated\n",
        "            if next_token.item() == end_token:\n",
        "                break\n",
        "    \n",
        "    return generated\n",
        "\n",
        "# Demonstrate autoregressive generation\n",
        "def demonstrate_generation():\n",
        "    \"\"\"Show how autoregressive generation works\"\"\"\n",
        "    \n",
        "    print(\"ü§ñ Autoregressive Generation Demo\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Use the transformer from previous exercise\n",
        "    src_vocab_size = 1000\n",
        "    tgt_vocab_size = 800\n",
        "    \n",
        "    # Special tokens\n",
        "    start_token = 1\n",
        "    end_token = 2\n",
        "    \n",
        "    # Sample source sequence and move to device\n",
        "    src = torch.randint(3, src_vocab_size, (1, 8)).to(device)  # Avoid special tokens\n",
        "    \n",
        "    print(f\"Source sequence: {src[0].tolist()}\")\n",
        "    print(f\"Device: {src.device}\")\n",
        "    \n",
        "    # Generate target sequence\n",
        "    generated = autoregressive_generation(\n",
        "        transformer_model, src, start_token, end_token, max_length=15\n",
        "    )\n",
        "    \n",
        "    print(f\"Generated sequence: {generated[0].tolist()}\")\n",
        "    print(f\"Generated length: {generated.shape[1]}\")\n",
        "    \n",
        "    # Show step-by-step process\n",
        "    print(\"\\nüîç Step-by-step generation:\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Simulate step by step (just show concept)\n",
        "    current_seq = [start_token]\n",
        "    for i in range(min(5, generated.shape[1] - 1)):  # Show first 5 steps\n",
        "        next_token = generated[0, i + 1].item()\n",
        "        current_seq.append(next_token)\n",
        "        print(f\"Step {i+1}: {current_seq}\")\n",
        "        \n",
        "        if next_token == end_token:\n",
        "            break\n",
        "    \n",
        "    return generated\n",
        "\n",
        "generated_sequence = demonstrate_generation()"
      ],
      "id": "b983ba26",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 9: Testing and Analysis\n"
      ],
      "id": "076c8e85"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 9.1: Comprehensive model analysis\n",
        "def comprehensive_model_analysis():\n",
        "    \"\"\"Comprehensive analysis of our transformer implementation\"\"\"\n",
        "    \n",
        "    print(\"üî¨ Comprehensive Model Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # 1. Architecture verification\n",
        "    print(\"1. Architecture Verification:\")\n",
        "    print(\"   ‚úì Multi-head attention implemented\")\n",
        "    print(\"   ‚úì Positional encoding implemented\") \n",
        "    print(\"   ‚úì Encoder-decoder structure implemented\")\n",
        "    print(\"   ‚úì Residual connections and layer norm implemented\")\n",
        "    print(\"   ‚úì Causal masking for decoder implemented\")\n",
        "    \n",
        "    # 2. Parameter count analysis\n",
        "    def count_parameters_by_component(model):\n",
        "        \"\"\"Count parameters by component\"\"\"\n",
        "        counts = {}\n",
        "        \n",
        "        # Embeddings\n",
        "        counts['embeddings'] = (model.src_embedding.weight.numel() + \n",
        "                               model.tgt_embedding.weight.numel())\n",
        "        \n",
        "        # Encoder\n",
        "        counts['encoder'] = sum(p.numel() for p in model.encoder.parameters())\n",
        "        \n",
        "        # Decoder  \n",
        "        counts['decoder'] = sum(p.numel() for p in model.decoder.parameters())\n",
        "        \n",
        "        # Output projection\n",
        "        counts['output'] = model.output_projection.weight.numel()\n",
        "        \n",
        "        return counts\n",
        "    \n",
        "    param_counts = count_parameters_by_component(transformer_model)\n",
        "    total = sum(param_counts.values())\n",
        "    \n",
        "    print(f\"\\n2. Parameter Distribution:\")\n",
        "    for component, count in param_counts.items():\n",
        "        percentage = (count / total) * 100\n",
        "        print(f\"   {component.capitalize()}: {count:,} ({percentage:.1f}%)\")\n",
        "    print(f\"   Total: {total:,}\")\n",
        "    \n",
        "    # 3. Memory analysis\n",
        "    print(f\"\\n3. Memory Analysis:\")\n",
        "    model_size_mb = total * 4 / (1024 * 1024)  # 4 bytes per float32\n",
        "    print(f\"   Model size: {model_size_mb:.2f} MB\")\n",
        "    print(f\"   Approximate GPU memory for training: {model_size_mb * 4:.2f} MB\")\n",
        "    \n",
        "    # 4. Computational complexity\n",
        "    d_model = transformer_model.d_model\n",
        "    num_heads = 8  # from our model\n",
        "    seq_len = 100  # example sequence length\n",
        "    \n",
        "    # Attention complexity: O(n^2 * d)\n",
        "    attention_ops = seq_len * seq_len * d_model * num_heads\n",
        "    \n",
        "    # FFN complexity: O(n * d^2)  \n",
        "    ffn_ops = seq_len * d_model * d_model * 4  # d_ff is typically 4*d_model\n",
        "    \n",
        "    print(f\"\\n4. Computational Complexity (seq_len={seq_len}):\")\n",
        "    print(f\"   Attention operations: {attention_ops:,}\")\n",
        "    print(f\"   Feed-forward operations: {ffn_ops:,}\")\n",
        "    print(f\"   Attention dominates for seq_len > {d_model}\")\n",
        "    \n",
        "    return param_counts\n",
        "\n",
        "analysis_results = comprehensive_model_analysis()"
      ],
      "id": "e4706df5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Exercise 9.2: Compare with Different Configurations\n"
      ],
      "id": "96da5c9b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 9.2: Compare different model configurations\n",
        "def compare_model_configurations():\n",
        "    \"\"\"Compare different transformer configurations\"\"\"\n",
        "    \n",
        "    configs = {\n",
        "        'Tiny': {'d_model': 64, 'num_heads': 4, 'num_layers': 2, 'd_ff': 256},\n",
        "        'Small': {'d_model': 128, 'num_heads': 8, 'num_layers': 4, 'd_ff': 512},\n",
        "        'Base': {'d_model': 256, 'num_heads': 8, 'num_layers': 6, 'd_ff': 1024},\n",
        "        'Large': {'d_model': 512, 'num_heads': 16, 'num_layers': 12, 'd_ff': 2048}\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for name, config in configs.items():\n",
        "        # Create model\n",
        "        model = Transformer(\n",
        "            src_vocab_size=1000,\n",
        "            tgt_vocab_size=1000,\n",
        "            d_model=config['d_model'],\n",
        "            num_heads=config['num_heads'],\n",
        "            num_encoder_layers=config['num_layers'],\n",
        "            num_decoder_layers=config['num_layers'],\n",
        "            d_ff=config['d_ff']\n",
        "        )\n",
        "        \n",
        "        # Count parameters\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        \n",
        "        # Estimate memory\n",
        "        memory_mb = total_params * 4 / (1024 * 1024)\n",
        "        \n",
        "        # Estimate operations for seq_len=128\n",
        "        seq_len = 128\n",
        "        attention_ops = seq_len * seq_len * config['d_model'] * config['num_heads'] * config['num_layers'] * 2  # encoder + decoder\n",
        "        ffn_ops = seq_len * config['d_model'] * config['d_ff'] * config['num_layers'] * 2\n",
        "        \n",
        "        results[name] = {\n",
        "            'params': total_params,\n",
        "            'memory_mb': memory_mb,\n",
        "            'attention_ops': attention_ops,\n",
        "            'ffn_ops': ffn_ops,\n",
        "            'total_ops': attention_ops + ffn_ops\n",
        "        }\n",
        "    \n",
        "    # Display comparison\n",
        "    print(\"üìä Model Configuration Comparison\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"{'Config':<8} {'Params':<12} {'Memory(MB)':<12} {'Operations':<15}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for name, stats in results.items():\n",
        "        print(f\"{name:<8} {stats['params']:<12,} {stats['memory_mb']:<12.1f} {stats['total_ops']:<15,}\")\n",
        "    \n",
        "    # Plot comparison\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "    \n",
        "    names = list(results.keys())\n",
        "    params = [results[name]['params'] for name in names]\n",
        "    memory = [results[name]['memory_mb'] for name in names]\n",
        "    ops = [results[name]['total_ops'] for name in names]\n",
        "    \n",
        "    # Parameters\n",
        "    axes[0, 0].bar(names, params, color='skyblue')\n",
        "    axes[0, 0].set_title('Parameter Count')\n",
        "    axes[0, 0].set_ylabel('Parameters')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Memory\n",
        "    axes[0, 1].bar(names, memory, color='lightcoral')\n",
        "    axes[0, 1].set_title('Memory Usage')\n",
        "    axes[0, 1].set_ylabel('Memory (MB)')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Operations\n",
        "    axes[1, 0].bar(names, ops, color='lightgreen')\n",
        "    axes[1, 0].set_title('Computational Operations')\n",
        "    axes[1, 0].set_ylabel('Operations')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Efficiency (params per operation)\n",
        "    efficiency = [p / o for p, o in zip(params, ops)]\n",
        "    axes[1, 1].bar(names, efficiency, color='gold')\n",
        "    axes[1, 1].set_title('Efficiency (Params/Ops)')\n",
        "    axes[1, 1].set_ylabel('Efficiency')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return results\n",
        "\n",
        "config_comparison = compare_model_configurations()"
      ],
      "id": "4ee18744",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Final Exercise: Put It All Together\n"
      ],
      "id": "8f1bf413"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Final Exercise: Create a simple training loop\n",
        "def create_simple_training_loop():\n",
        "    \"\"\"Create a simple training loop to see the transformer in action\"\"\"\n",
        "    \n",
        "    print(\"üöÄ Simple Training Loop Demo\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Create a small model for quick training and move to device\n",
        "    model = Transformer(\n",
        "        src_vocab_size=100,\n",
        "        tgt_vocab_size=100,\n",
        "        d_model=64,\n",
        "        num_heads=4,\n",
        "        num_encoder_layers=2,\n",
        "        num_decoder_layers=2,\n",
        "        d_ff=256,\n",
        "        max_seq_len=50\n",
        "    ).to(device)\n",
        "    \n",
        "    # Create simple synthetic data\n",
        "    batch_size = 8\n",
        "    seq_len = 10\n",
        "    num_batches = 5\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # 0 is padding token\n",
        "    \n",
        "    model.train()\n",
        "    losses = []\n",
        "    \n",
        "    print(f\"Training on device: {device}\")\n",
        "    print(\"Training progress:\")\n",
        "    for batch_idx in range(num_batches):\n",
        "        # Generate random data (in practice, this would be real data) - move to device\n",
        "        src = torch.randint(1, 100, (batch_size, seq_len)).to(device)\n",
        "        tgt_input = torch.randint(1, 100, (batch_size, seq_len)).to(device)\n",
        "        tgt_output = torch.randint(1, 100, (batch_size, seq_len)).to(device)\n",
        "        \n",
        "        # Create causal mask and move to device\n",
        "        tgt_mask = create_causal_mask(seq_len).to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input, tgt_mask=tgt_mask)\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = criterion(output.view(-1, output.size(-1)), tgt_output.view(-1))\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping (important for transformers)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "        print(f\"  Batch {batch_idx + 1}: Loss = {loss.item():.4f}\")\n",
        "    \n",
        "    # Plot training progress\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(losses, 'o-')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nTraining complete!\")\n",
        "    print(f\"Final loss: {losses[-1]:.4f}\")\n",
        "    print(f\"Loss reduction: {(losses[0] - losses[-1]) / losses[0] * 100:.1f}%\")\n",
        "    \n",
        "    return model, losses\n",
        "\n",
        "trained_model, training_losses = create_simple_training_loop()"
      ],
      "id": "c9685015",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Course Summary and Next Steps\n",
        "\n",
        "### What You've Learned:\n",
        "\n",
        "#### üìö **Theoretical Understanding:**\n",
        "1. **Self-Attention Mechanism** - How words can attend to all other words in parallel\n",
        "2. **Multi-Head Attention** - Learning different types of relationships simultaneously  \n",
        "3. **Positional Encoding** - Injecting sequence order information\n",
        "4. **Encoder-Decoder Architecture** - The complete transformer structure\n",
        "5. **Causal Masking** - Preventing future information leakage in decoders\n",
        "\n",
        "#### üíª **Practical Implementation:**\n",
        "1. Built each component from scratch using PyTorch\n",
        "2. Implemented complete working Transformer model\n",
        "3. Created autoregressive generation capability\n",
        "4. Analyzed model complexity and configurations\n",
        "5. Set up basic training pipeline\n",
        "\n",
        "### üöÄ **Next Steps for Continued Learning:**\n",
        "\n",
        "#### **Advanced Topics to Explore:**\n",
        "- **BERT & GPT architectures** - Encoder-only and decoder-only variants\n",
        "- **Vision Transformers (ViT)** - Applying transformers to images\n",
        "- **Optimization techniques** - Learning rate scheduling, warmup, etc.\n",
        "- **Advanced attention variants** - Sparse attention, linear attention\n",
        "- **Pre-training and fine-tuning** - Transfer learning with transformers\n",
        "\n",
        "#### **Practical Projects:**\n",
        "1. **Text Classification** - Fine-tune BERT for sentiment analysis\n",
        "2. **Machine Translation** - Build a translation system\n",
        "3. **Text Generation** - Create a GPT-style language model\n",
        "4. **Question Answering** - Implement a QA system\n",
        "5. **Multimodal Learning** - Combine text and images\n",
        "\n",
        "#### **Resources for Further Learning:**\n",
        "- **Papers**: \"Attention Is All You Need\", \"BERT\", \"GPT-3\"\n",
        "- **Libraries**: Hugging Face Transformers, fairseq\n",
        "- **Courses**: Stanford CS224N, Fast.ai NLP course\n",
        "- **Books**: \"Natural Language Processing with Transformers\"\n",
        "\n",
        "### üéØ **Congratulations!** \n",
        "\n",
        "You've successfully built a complete Transformer architecture from scratch and understand:\n",
        "- ‚úÖ Why attention revolutionized NLP\n",
        "- ‚úÖ How self-attention enables parallel processing\n",
        "- ‚úÖ The importance of positional encoding\n",
        "- ‚úÖ How multi-head attention captures different relationships\n",
        "- ‚úÖ The encoder-decoder architecture\n",
        "- ‚úÖ Autoregressive generation process\n",
        "\n",
        "**You're now ready to dive deeper into the world of modern NLP and large language models!**\n",
        "\n",
        "print(\"üéâ Course Complete! You've mastered the Transformer architecture!\")\n",
        "print(\"Ready to build the next generation of AI models!\")\n"
      ],
      "id": "8f1cd36a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}